{"cells":[{"cell_type":"markdown","metadata":{"id":"Gypmeklj8tAd"},"source":["# L4: Part-of-speech tagging"]},{"cell_type":"markdown","metadata":{"id":"DgaAxp5n8tAf"},"source":["Part-of-speech tagging is the task of labelling the words (tokens) of a sentence with parts-of-speech such as noun, adjective, and verb. In this lab you will implement the simple, autoregressive fixed-window tagger that was presented in Lecture&nbsp;4.2."]},{"cell_type":"markdown","metadata":{"id":"Ik4794o-8tAf"},"source":["## The data set"]},{"cell_type":"markdown","metadata":{"id":"KdIowYBC8tAf"},"source":["The data set for the lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org), a corpus containing more than 16,000 sentences (254,000&nbsp;tokens) annotated with, among other things, parts-of-speech. The Universal Dependencies Project distributes its data in the [CoNLL-U format](https://universaldependencies.org/format.html), but for this lab we have converted the data into a simpler format: words and their part-of-speech tags are separated by tabs, sentences are separated by empty lines. The code in the next cell defines a container class for data with this format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msOHODFL8tAg"},"outputs":[],"source":["class Dataset():\n","\n","    def __init__(self, filename):\n","        self.filename = filename\n","\n","    def __iter__(self):\n","        tmp = []\n","        with open(self.filename, 'rt', encoding='utf-8') as lines:\n","            for line in lines:\n","                line = line.rstrip()\n","                if line:\n","                    tmp.append(tuple(line.split('\\t')))\n","                else:\n","                    yield tmp\n","                    tmp = []"]},{"cell_type":"markdown","metadata":{"id":"rzmR6xOz8tAg"},"source":["We load the training data and the development data for this lab:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaQe-tf88tAg"},"outputs":[],"source":["train_data = Dataset('train.txt')\n","dev_data = Dataset('dev.txt')"]},{"cell_type":"markdown","metadata":{"id":"-YuevrM78tAg"},"source":["Both data sets consist of **tagged sentences**. On the Python side of things, a tagged sentence is represented as a list of string pairs, where the first component of each pair represents a word token and the second component represents the wordâ€™s tag. The possible tags are listed and exemplified in the [Annotation Guidelines](http://universaldependencies.org/u/pos/all.html) of the Universal Dependencies Project. Run the next code cell to see an example of a tagged sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cZZ2ufK8tAh","outputId":"0e115190-9241-4599-da23-b4d644de9345"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('There', 'PRON'), ('has', 'AUX'), ('been', 'VERB'), ('talk', 'NOUN'), ('that', 'SCONJ'), ('the', 'DET'), ('night', 'NOUN'), ('curfew', 'NOUN'), ('might', 'AUX'), ('be', 'AUX'), ('implemented', 'VERB'), ('again', 'ADV'), ('.', 'PUNCT')]\n","[('We', 'PRON'), (\"'ve\", 'AUX'), ('grown', 'VERB'), ('up', 'ADP'), ('.', 'PUNCT')]\n"]}],"source":["print(list(train_data)[42])\n","print(list(dev_data)[42])\n"]},{"cell_type":"markdown","metadata":{"id":"wCUYzZru8tAh"},"source":["## Tagger interface"]},{"cell_type":"markdown","metadata":{"id":"ydAz5pnl8tAh"},"source":["The tagger that you will implement in this lab follows a simple interface with just one method:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjXJ1K9A8tAh"},"outputs":[],"source":["class Tagger(object):\n","\n","    def predict(self, sentence):\n","        raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"Tx6wuc9k8tAh"},"source":["The single method of this interface has the following specification:\n","\n","**predict** (*self*, *sentence*)\n","\n","> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens).\n","\n","One trivial implementation of this interface is a tagger that always predicts the same tag for every word, independently of the input:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyhttaUY8tAi"},"outputs":[],"source":["class ConstantTagger(Tagger):\n","    \n","    def __init__(self, the_tag):\n","        self.the_tag = the_tag\n","    \n","    def predict(self, words):\n","        return [self.the_tag] * len(words)"]},{"cell_type":"markdown","metadata":{"id":"Pn1LmfNz8tAi"},"source":["## Problem 1: Implement an evaluation function"]},{"cell_type":"markdown","metadata":{"id":"MEEVSRb48tAi"},"source":["Your first task is to implement a function that computes the accuracy of a tagger on gold-standard data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0v_cGJyO8tAi"},"outputs":[],"source":["def accuracy(tagger, gold_data):\n","    # TODO: Replace the next line with your own code\n","    tot = 0\n","    correct = 0\n","\n","    for words in gold_data:\n","        tot += len(words)\n","        gold_words = [i[1] for i in words]\n","        pred_words = tagger.predict(words)\n","\n","        correct += sum(1 for x,y in zip(gold_words,pred_words) if x == y)\n","    \n","    return correct / tot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKV7EVGJ9rC4","outputId":"9c41c435-2277-4094-ec84-533081be1958"},"outputs":[{"data":{"text/plain":["0.1668919993637665"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["const_tagger = ConstantTagger('NOUN')\n","\n","acc = accuracy(const_tagger, dev_data)\n","acc"]},{"cell_type":"markdown","metadata":{"id":"HQV4JpET8tAi"},"source":["Your implementation should conform to the following specification:\n","\n","**accuracy** (*tagger*, *gold_data*)\n","\n","> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard)."]},{"cell_type":"markdown","metadata":{"id":"ANGzkK8s8tAi"},"source":["### ðŸ¤ž Test your code\n","\n","Test your code by computing the accuracy on the development set of a trivial tagger that tags each word as a noun. The expected value is 16.69%."]},{"cell_type":"markdown","metadata":{"id":"BEMebMlS8tAi"},"source":["## Problem 2: Implement a baseline"]},{"cell_type":"markdown","metadata":{"id":"adPaMRdw8tAi"},"source":["Before you start working on the tagger as such, we ask you to first implement a simple baseline:\n","\n","> Tag each input word with the most frequent tag for that word in the training data. If an input word does not occur in the training data, tag it with the overall most frequent tag in the training data. Break ties by choosing that tag which comes first in the alphabetical order.\n","\n","To implement the baseline, you need to implement both a class `BaselineTagger` and a function `train_baseline`. A `BaselineTagger` has two fields: a dictionary mapping each word in the training data to the most frequent tag for that word, and a string representing the fallback tag (overall most frequent tag in the training data). Both of these fields are set in the `train_baseline` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btHfDgDR8tAi"},"outputs":[],"source":["class BaselineTagger(Tagger):\n","\n","    def __init__(self):\n","        self.most_frequent = {}\n","        self.fallback = None\n","\n","    def predict(self, words):\n","        # TODO: Replace the next line with your own code\n","        res = []\n","        for word, tag in words:\n","            if word in self.most_frequent:\n","                res.append(self.most_frequent[word])\n","            else:\n","                res.append(self.fallback)\n","        #print(words)\n","        #print(res)\n","        return res\n","\n","\n","def train_baseline(train_data):\n","    # TODO: Replace the next line with your own code\n","\n","    ret = BaselineTagger()\n","\n","    temp = {}\n","    fallback = {}\n","\n","    all_words = set()\n","\n","    for sent in train_data:\n","        for word_and_tag in sent:\n","            if word_and_tag in temp:\n","                temp[word_and_tag] += 1\n","            else:\n","                temp[word_and_tag] = 1\n","            \n","            if word_and_tag[1] in fallback:\n","                fallback[word_and_tag[1]] += 1\n","            else:\n","                fallback[word_and_tag[1]] = 1\n","\n","            all_words.add(word_and_tag[0])\n","\n","\n","    most_frequent = {}\n","    for word in all_words:\n","        best_tag = ''\n","        freq = 0\n","        for word_and_tag, occ in temp.items():\n","            if word == word_and_tag[0]:\n","                if occ > freq:\n","                    best_tag = word_and_tag[1]\n","                    freq = occ\n","        most_frequent[word] = best_tag\n","\n","    \n","    #print(most_frequent)\n","    ret.most_frequent = most_frequent\n","    ret.fallback = max(fallback, key=fallback.get)\n","    return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zhl8pF1D9rC5"},"outputs":[],"source":["baseline_tagger = train_baseline(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-T5KyNwQ9rC5","outputId":"d9ee774f-cdf4-454c-a6ee-09a246efe83a"},"outputs":[{"data":{"text/plain":["0.8564100524892636"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["acc = accuracy(baseline_tagger, dev_data)\n","acc"]},{"cell_type":"markdown","metadata":{"id":"wj3AuS2w8tAj"},"source":["### ðŸ¤ž Test your code\n","\n","Test your implementation by computing the accuracy of the baseline tagger on the development data. The expected value is 85.61%."]},{"cell_type":"markdown","metadata":{"id":"aVfIUDgW8tAj"},"source":["## Problem 3: Create the vocabularies"]},{"cell_type":"markdown","metadata":{"id":"XA3v6aov8tAj"},"source":["As in previous labs, you will need an explicit representation of your vocabulary. Here we actually have two vocabularies: one for the words and one for the tags. Both should be represented as dictionaries that map words/tags to a contiguous range of integers, starting at zero.\n","\n","The next cell contains skeleton code for a function `make_vocabs` that constructs the two vocabularies from gold-standard data. The code cell also defines a name for the â€˜unknown wordâ€™ (`UNK`) and for an additional pseudoword that you will use as a placeholder for undefined values (`PAD`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1jqZ8iv8tAj"},"outputs":[],"source":["PAD = '<pad>'\n","UNK = '<unk>'\n","\n","def make_vocabs(gold_data):\n","    #print(list(gold_data))\n","\n","    words = {}\n","    tags = {}\n","\n","    curr_word_idx = 2\n","    curr_tag_idx = 1\n","\n","    words[PAD] = 0\n","    words[UNK] = 1\n","\n","    tags[PAD] = 0\n","\n","    for sent in list(gold_data):\n","        for word, tag in sent:\n","            if word not in words:\n","                words[word] = curr_word_idx\n","                curr_word_idx += 1\n","            \n","            if tag not in tags:\n","                tags[tag] = curr_tag_idx\n","                curr_tag_idx += 1\n","\n","    return words, tags"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O32MZUo-9rC7","outputId":"896c51f5-491a-463e-b08d-7a8b209bf0c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["19674\n","18\n"]}],"source":["words, tags = make_vocabs(train_data)\n","\n","print(len(words))\n","print(len(tags))"]},{"cell_type":"markdown","metadata":{"id":"qqykh5s58tAj"},"source":["Complete the code according to the following specification:\n","\n","**make_vocabs** (*gold_data*)\n","\n","> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over tagged sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."]},{"cell_type":"markdown","metadata":{"id":"r2vwZ6q88tAj"},"source":["### ðŸ¤ž Test your code\n","\n","Test your implementation by computing the total number of unique words and tags in the training data (including the pseudowords). The expected values are 19,674&nbsp;words and 18&nbsp;tags."]},{"cell_type":"markdown","metadata":{"id":"CsmeqtYH8tAj"},"source":["## Problem 4: Fixed-window tagger"]},{"cell_type":"markdown","metadata":{"id":"_bPMCL_w8tAj"},"source":["Your main task in this lab is to implement a complete, autoregressive part-of-speech tagger based on the fixed-window architecture. This implementation has four parts: the fixed-window model; a tagger that uses the fixed-window model to make predictions; a function that generates training examples for the tagger; and the training function.\n","\n","**âš ï¸ We expect that solving this problem will take you the longest time in this lab.**"]},{"cell_type":"markdown","metadata":{"id":"y-DbRZy_8tAj"},"source":["### Problem 4.1: Implement the fixed-window model"]},{"cell_type":"markdown","metadata":{"id":"uu-23JDx8tAj"},"source":["The architecture of the fixed-window model is presented in Lecture&nbsp;4.2. An input to the network takes the form of a $k$-dimensional vector of word ids and/or tag ids. Each integer $i$ is mapped to an $e_i$-dimensional embedding vector. These vectors are concatenated to form a vector of length $e_1 + \\cdots + e_k$, and sent through a feed-forward network with a single hidden layer and a rectified linear unit (ReLU).\n","\n","#### Default features\n","\n","We ask you to implement a fixed-window model with the following features ($k=4$):\n","\n","0. current word\n","1. previous word\n","2. next word\n","3. tag predicted for the previous word\n","\n","Whenever the value of a feature is undefined, you should use the special value `PAD`.\n","\n","#### Embedding specifications\n","\n","To make your implementation of the fixed-window model useful for a range of different applications (including the parser that you will build in lab&nbsp;5), it should support other feature sets than the default model. To this end, the constructor of your model should accept a list of what we call *embedding specifications*. An embedding specification is a triple $(m, n, e)$ consisting of three integers. Such a triple specifies that the model should include $m$ instances of an embedding from $n$ items to vectors of size $e$. All of the $m$ instances are to share their weights. In this lab, the embeddings will be embeddings for words and tags. For example, to instantiate the default feature model, you would initialise the model with the following specifications:\n","\n","``\n","[(3, num_words, word_dim), (1, num_tags, tag_dim)]\n","``\n","\n","This specifies that the model should use 3 instances of an embedding from *num_words* words to vectors of length *word_dim*, and 1 instance of an embedding from *num_tags* tags to vectors of length *tag_dim*. All 3 instances of the word embedding would share their weights. If you rather wanted to have word embeddings with separate weights, you would initialise the model with the following specifications:\n","\n","``\n","[(1, num_words, word_dim), (1, num_words, word_dim), (1, num_words, word_dim), (1, num_tags, tag_dim)]\n","``\n","\n","We recommend that you initialize the weights of each embedding with values drawn from $\\mathcal{N}(0, 10^{-2})$.\n","\n","#### Hyperparameters\n","\n","The network architecture introduces a number of hyperparameters. The following choices are reasonable defaults:\n","\n","* width of each word embedding: 50\n","* width of each tag embedding: 10\n","* size of the hidden layer: 100"]},{"cell_type":"markdown","metadata":{"id":"ZcblQoeX8tAj"},"source":["The next cell contains skeleton code for the implementation of the fixed-window model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o29ZRHd58tAj"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class FixedWindowModel(nn.Module):\n","\n","    def __init__(self, embedding_specs, hidden_dim, output_dim):\n","        super().__init__()\n","        # TODO: Add your code here\n","        self.embedding_specs = embedding_specs\n","        self.embeddings = nn.ModuleList((nn.Embedding(i[1], i[2])) for i in embedding_specs for j in range(i[0]))  \n","        self.linear1   = nn.Linear(50*3 + 10, hidden_dim)\n","        self.relu      = nn.ReLU()\n","        self.linear2   = nn.Linear(hidden_dim, output_dim)\n","        \n","    # Features is B x k where B == batch_size and k is total embeddings (k = 4, 3 word embs and 1 tag emb)\n","    def forward(self, features):\n","        # TODO: Replace the next line with your own code\n","        #print(self.embeddings)\n","        #print(\"feature size: \", features.size())\n","\n","        rows=features.size()[0]\n","        cols=features.size()[1]\n","        feat_v = torch.empty(features.size()[0], 160)\n","        for row in range(0,rows):\n","            target_embedding = self.embeddings[0].weight[features[row][0]]\n","            for column in range(1,cols):\n","                word_embedding = self.embeddings[column].weight[features[row][column]]\n","                target_embedding = torch.cat((target_embedding, word_embedding),-1)\n","                #print(\"target_embedding size: \", target_embedding.size())\n","            feat_v[row] = target_embedding\n","        \n","        linear1 = self.linear1(feat_v)\n","        rel     = self.relu(linear1)\n","        output  = self.linear2(rel)   \n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkcilVxZ9rC9"},"outputs":[],"source":["test = FixedWindowModel([(3, len(words), 50), (1, len(tags), 10)], 100, 18)\n","\n","#test.forward(\"asd\")"]},{"cell_type":"markdown","metadata":{"id":"Alf0rVmv8tAj"},"source":["Your implementation should meet the following specification:\n","\n","**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n","\n","> A fixed-window model is initialized with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n","\n","**forward** (*self*, *features*)\n","\n","> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding.\n","\n","#### ðŸ’¡ Hint on the implementation\n","\n","You will have to construct embeddings based on the embedding specifications. It is natural to store these embeddings in a list- or dictionary-valued attribute of the `FixedWindowModel` object. However, in order to expose the embeddings to the auto-differentiation magic of PyTorch (so that their weights are updated during training), you must instead store them in an [`nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList) or [`nn.ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict)."]},{"cell_type":"markdown","metadata":{"id":"6BL2VRfj8tAj"},"source":["### Problem 4.2: Implement the tagger"]},{"cell_type":"markdown","metadata":{"id":"9B0m03Gf8tAk"},"source":["The next step is to implement the tagger itself. The tagger will use the simple algorithm that was presented in Lecture&nbsp;4.2: It processes an input sentence from left to right, and at each position, predicts the tag for the current word based on the features extracted from the current feature window."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UbAs5NM8tAk"},"outputs":[],"source":["class FixedWindowTagger(Tagger):\n","\n","    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n","        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n","\n","        self.vocab_words = vocab_words\n","        self.vocab_tags  = vocab_tags\n","        self.model = FixedWindowModel(embedding_specs, hidden_dim, len(vocab_tags))\n","        # TODO: Replace the next line with your own code\n","        pass\n","\n","    def featurize(self, words, i, pred_tags):\n","        # TODO: Replace the next line with your own code\n","      print(words)\n","        # All cases\n","      if len(words) == 1: # If only one word is sent in\n","            return torch.LongTensor([words[i], 0, 0, 0])\n","      elif i == len(words) - 1: #If current word end of list\n","            return torch.LongTensor([words[i], words[i - 1], 0, pred_tags[i - 1]])\n","      elif i == 0: #If current word beginning of list\n","            return torch.LongTensor([words[i], 0, words[i + 1], 0])\n","      else:  # If middle of words\n","            return torch.LongTensor([words[i], words[i - 1], words[i + 1], pred_tags[i - 1]])\n","\n","    def predict(self, words):\n","        # TODO: Replace the next line with your own code\n","        \n","        # Change the strings to word ids as vocabulary is ids. 0 == UNK\n","        word_ids = []\n","        print(words)\n","        for word in words:\n","              if word in self.vocab_words:\n","                    word_ids.append(self.vocab_words[word])\n","              else:\n","                    word_ids.append(0)\n","\n","        predicted_tags = []\n","        for i in range(len(word_ids)):\n","              curr_feature = self.featurize(words, i, predicted_tags)\n","              #print(\"feature: \", curr_feature.view(1,4))\n","              res = self.model.forward(curr_feature.view(1,4))\n","              not_used, tag = torch.max(res.squeeze(0), 0)\n","              #print(\"tag: \", tag)\n","              predicted_tags.append(tag)\n","        \n","        # Convert tag ids to strings\n","        predicted_tag_strings = []\n","        for pred_tag_id in predicted_tags:\n","          for key, val in self.vocab_tags.items():\n","                if val == pred_tag_id:\n","                      predicted_tag_strings.append(key)\n","                      \n","        return predicted_tag_strings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9FkK0UZ9rC9","outputId":"b7918235-e9ca-429f-e082-1e3cc72de7b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["feature:  tensor([[   14,     0, 12445,     0]])\n","feature size:  torch.Size([1, 4])\n","target_embedding size:  torch.Size([100])\n","target_embedding size:  torch.Size([150])\n","target_embedding size:  torch.Size([160])\n","tag:  tensor(6)\n","feature:  tensor([[12445,    14, 10620,     6]])\n","feature size:  torch.Size([1, 4])\n","target_embedding size:  torch.Size([100])\n","target_embedding size:  torch.Size([150])\n","target_embedding size:  torch.Size([160])\n","tag:  tensor(11)\n","feature:  tensor([[10620, 12445,   177,    11]])\n","feature size:  torch.Size([1, 4])\n","target_embedding size:  torch.Size([100])\n","target_embedding size:  torch.Size([150])\n","target_embedding size:  torch.Size([160])\n","tag:  tensor(11)\n","feature:  tensor([[  177, 10620,    14,    11]])\n","feature size:  torch.Size([1, 4])\n","target_embedding size:  torch.Size([100])\n","target_embedding size:  torch.Size([150])\n","target_embedding size:  torch.Size([160])\n","tag:  tensor(11)\n","feature:  tensor([[  14,  177, 2657,   11]])\n","feature size:  torch.Size([1, 4])\n","target_embedding size:  torch.Size([100])\n","target_embedding size:  torch.Size([150])\n","target_embedding size:  torch.Size([160])\n","tag:  tensor(11)\n","feature:  tensor([[2657,   14,    0,   11]])\n","feature size:  torch.Size([1, 4])\n","target_embedding size:  torch.Size([100])\n","target_embedding size:  torch.Size([150])\n","target_embedding size:  torch.Size([160])\n","tag:  tensor(4)\n"]},{"data":{"text/plain":["['DET', 'SCONJ', 'SCONJ', 'SCONJ', 'SCONJ', 'NOUN']"]},"execution_count":121,"metadata":{},"output_type":"execute_result"}],"source":["tagger = FixedWindowTagger(words, tags)\n","tagger.predict([words[\"the\"], words[\"cat\"], words[\"sat\"], words[\"on\"], words[\"the\"], words[\"floor\"]])\n","#[words[\"the\"], words[\"cat\"]]"]},{"cell_type":"markdown","metadata":{"id":"ggFe6sgn8tAk"},"source":["Complete the skeleton code by implementing the methods of this interface:\n","\n","**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n","\n","> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n","\n","**featurize** (*self*, *words*, *i*, *pred_tags*)\n","\n","> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n","\n","**predict** (*self*, *words*)\n","\n","> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."]},{"cell_type":"markdown","metadata":{"id":"oJT1Jpp08tAk"},"source":["### Problem 4.3: Generate the training examples"]},{"cell_type":"markdown","metadata":{"id":"bYoQhWXo8tAk"},"source":["Your next task is to implement a function that generates the training examples for the tagger. You will train the tagger as usual, using minibatch training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HJq42a68tAk","outputId":"6baf581a-2550-4291-cf4f-37fcd55c071f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([100, 4])\n","torch.Size([100])\n"]}],"source":["def training_examples(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n","    batch_x, batch_y = [], []\n","\n","    for sentence in gold_data:\n","        word_ids = []\n","        tag_ids = []\n","        for word, tag in sentence:\n","            word_ids.append(vocab_words[word])\n","            tag_ids.append(vocab_tags[tag])\n","\n","        for idx, tag in enumerate(tag_ids):\n","            feature = tagger.featurize(word_ids, idx, tag_ids)\n","            batch_x.append(feature)\n","            batch_y.append(tag)\n","\n","            # If batch contains batch size examples, yield them and empty x and y.\n","            if len(batch_x) == batch_size:\n","                yield torch.stack(batch_x, dim=0), torch.LongTensor(batch_y)\n","                batch_x, batch_y = [], []\n","    \n","    # yield what we have in the end if not empty.\n","    #if len(batch_x) != 0:\n","    #    yield torch.stack(batch_x, dim=0), torch.LongTensor(batch_y)\n","        \n","\n","tagger = FixedWindowTagger(words, tags)\n","for x, y in training_examples(words, tags, list(train_data), tagger):\n","  print(x.size())\n","  print(y.size())\n","  break"]},{"cell_type":"markdown","metadata":{"id":"VedTADI_8tAk"},"source":["Your code should comply with the following specification:\n","\n","**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n","\n","> Iterates through the given *gold_data* (an iterable of tagged sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."]},{"cell_type":"markdown","metadata":{"id":"q97lunKL8tAk"},"source":["### Problem 4.4: Training loop"]},{"cell_type":"markdown","metadata":{"id":"Hn24xPtR8tAk"},"source":["What remains to be done is the implementation of the training loop. This should be a straightforward generalization of the training loops that you have seen so far. Complete the skeleton code in the cell below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5hw3hXS8tAk"},"outputs":[],"source":["import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def train_fixed_window(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n","    # TODO: Replace the next line with your own code\n","    tagger = FixedWindowTagger(words, tags)\n","    vocab_words, vocab_tags = make_vocabs(train_data)\n","    model = tagger.model\n","  \n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    train_losses = []\n","  \n","    for epoch in range(n_epochs): \n","        count = 1\n","        progress = 0\n","        train_loss = 0\n","        avg_loss = 0\n","\n","        for x, y in training_examples(vocab_words, vocab_tags,train_data, tagger):\n","            optimizer.zero_grad()\n","\n","            #Forward pass\n","            output = model.forward(x) #Expect float not long\n","\n","            #Compute the loss\n","            loss = F.cross_entropy(output,  y) #Expect long not float\n","            train_loss += loss.item()\n","            progress += len(y)\n","            \n","            #clear_output()\n","            print(\"Running Train loss: \" + str(train_loss/count))\n","\n","            count += 1\n","            \n","            #Backward pass\n","            loss.backward()\n","            \n","            #Update the parameter of the model\n","            optimizer.step()\n","\n","    return tagger"]},{"cell_type":"markdown","metadata":{"id":"OKPBWQ7v8tAk"},"source":["Here is the specification of the training function:\n","\n","**train_fixed_window** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n","\n","> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over tagged sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."]},{"cell_type":"markdown","metadata":{"id":"daVPwZdQ8tAk"},"source":["The next code cell trains a tagger and evaluates it on the development data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RnJ4MMqT8tAk","outputId":"eba022ff-1b0c-417c-aa16-fa4b13613f1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running Train loss: 3.061155080795288\n","Running Train loss: 2.8173261880874634\n","Running Train loss: 2.665748357772827\n","Running Train loss: 2.4726132452487946\n","Running Train loss: 2.3327579498291016\n","Running Train loss: 2.27816371122996\n","Running Train loss: 2.242201089859009\n","Running Train loss: 2.2271691858768463\n","Running Train loss: 2.188116855091519\n","Running Train loss: 2.161466658115387\n","Running Train loss: 2.1416098963130605\n","Running Train loss: 2.1180026729901633\n","Running Train loss: 2.097307874606206\n","Running Train loss: 2.0724367925098965\n","Running Train loss: 2.029468544324239\n","Running Train loss: 2.014624021947384\n","Running Train loss: 1.9865440901587992\n","Running Train loss: 1.966756456428104\n","Running Train loss: 1.9471854598898637\n","Running Train loss: 1.9282167971134185\n","Running Train loss: 1.897838791211446\n","Running Train loss: 1.858651421286843\n","Running Train loss: 1.8423854112625122\n","Running Train loss: 1.9278228829304378\n","Running Train loss: 1.9060990715026855\n","Running Train loss: 1.8895486684945912\n","Running Train loss: 1.869085219171312\n","Running Train loss: 1.8508412667683192\n","Running Train loss: 1.8285215640890187\n","Running Train loss: 1.804396915435791\n","Running Train loss: 1.7856953990074895\n","Running Train loss: 1.77319510653615\n","Running Train loss: 1.7575276078599873\n","Running Train loss: 1.738610597217784\n","Running Train loss: 1.7259512049811228\n","Running Train loss: 1.7147337860531278\n","Running Train loss: 1.7113887683765308\n","Running Train loss: 1.7058585192027844\n","Running Train loss: 1.695932461665227\n","Running Train loss: 1.6875255972146987\n","Running Train loss: 1.6768229298475312\n","Running Train loss: 1.6653988503274464\n","Running Train loss: 1.6582318405772365\n","Running Train loss: 1.6486241113055835\n","Running Train loss: 1.63641340997484\n","Running Train loss: 1.6263543989347375\n","Running Train loss: 1.612820991810332\n","Running Train loss: 1.6002435324092705\n","Running Train loss: 1.5898886201333027\n","Running Train loss: 1.5762457513809205\n","Running Train loss: 1.5667655631607653\n","Running Train loss: 1.55604065152315\n","Running Train loss: 1.5462999298887432\n","Running Train loss: 1.5301729716636516\n","Running Train loss: 1.5202805963429538\n","Running Train loss: 1.5138670088989394\n","Running Train loss: 1.507382895862847\n","Running Train loss: 1.499422966406263\n","Running Train loss: 1.4992546075481479\n","Running Train loss: 1.4923717727263768\n","Running Train loss: 1.4845239852295546\n","Running Train loss: 1.4767683004179308\n","Running Train loss: 1.4675684003602891\n","Running Train loss: 1.4595043286681175\n","Running Train loss: 1.4462228499926053\n","Running Train loss: 1.4422240419821306\n","Running Train loss: 1.4335719053425007\n","Running Train loss: 1.4293614108772839\n","Running Train loss: 1.4277325220729993\n","Running Train loss: 1.41946530852999\n","Running Train loss: 1.412669171749706\n","Running Train loss: 1.4052873452504475\n","Running Train loss: 1.3985480168094373\n","Running Train loss: 1.389809372457298\n","Running Train loss: 1.3831494720776876\n","Running Train loss: 1.3748043391265368\n","Running Train loss: 1.3701041062156876\n","Running Train loss: 1.3647489616504083\n","Running Train loss: 1.3590729500673995\n","Running Train loss: 1.354476373642683\n","Running Train loss: 1.3507619470725825\n","Running Train loss: 1.34702316071929\n","Running Train loss: 1.345448079597519\n","Running Train loss: 1.346649054260481\n","Running Train loss: 1.3433584626983195\n","Running Train loss: 1.3395141896813414\n","Running Train loss: 1.3469208526885372\n","Running Train loss: 1.3491819196126678\n","Running Train loss: 1.3454779219091608\n","Running Train loss: 1.3429024239381155\n","Running Train loss: 1.3413504591354957\n","Running Train loss: 1.3414444515238637\n","Running Train loss: 1.3398661991601348\n","Running Train loss: 1.3373441486916644\n","Running Train loss: 1.3311181375854895\n","Running Train loss: 1.3298687382290761\n","Running Train loss: 1.3247504185155494\n","Running Train loss: 1.3203919031182114\n","Running Train loss: 1.317574502241732\n","Running Train loss: 1.3128022080659867\n","Running Train loss: 1.308812134336717\n","Running Train loss: 1.3045578978809655\n","Running Train loss: 1.3013889118305688\n","Running Train loss: 1.2968388939133058\n","Running Train loss: 1.2939016325133188\n","Running Train loss: 1.287926822900772\n","Running Train loss: 1.282138241785709\n","Running Train loss: 1.275942423277431\n","Running Train loss: 1.2720589167481169\n","Running Train loss: 1.2676001592115922\n","Running Train loss: 1.2629239918949369\n","Running Train loss: 1.2585898877254553\n","Running Train loss: 1.2526901527843644\n","Running Train loss: 1.2510251246000592\n","Running Train loss: 1.2479183217753536\n","Running Train loss: 1.2483840872501504\n","Running Train loss: 1.2450230045196338\n","Running Train loss: 1.2423736276262898\n","Running Train loss: 1.242284471748256\n","Running Train loss: 1.239215648174286\n","Running Train loss: 1.2362965128638528\n","Running Train loss: 1.2325811982154846\n","Running Train loss: 1.2318686886531551\n","Running Train loss: 1.2378276423100503\n","Running Train loss: 1.2444911069869995\n","Running Train loss: 1.243193336895534\n","Running Train loss: 1.2385135799881042\n","Running Train loss: 1.2349274633452296\n","Running Train loss: 1.2309746252473934\n","Running Train loss: 1.226039119867178\n","Running Train loss: 1.2211023059510093\n","Running Train loss: 1.218272476033731\n","Running Train loss: 1.2177949243022086\n","Running Train loss: 1.2131754991723531\n","Running Train loss: 1.2122722347577413\n","Running Train loss: 1.2090306860559128\n","Running Train loss: 1.2078632098914932\n","Running Train loss: 1.2047064956547557\n","Running Train loss: 1.2019925722115332\n","Running Train loss: 1.1980778643063137\n","Running Train loss: 1.1937304709820038\n","Running Train loss: 1.1926212352766117\n","Running Train loss: 1.1892152666211961\n","Running Train loss: 1.1876146180762186\n","Running Train loss: 1.186761785375661\n","Running Train loss: 1.1850092696816954\n","Running Train loss: 1.182954610205021\n","Running Train loss: 1.1817953381183985\n","Running Train loss: 1.1795556285237305\n","Running Train loss: 1.1776367183526357\n","Running Train loss: 1.1765378457031503\n","Running Train loss: 1.1730402939413722\n","Running Train loss: 1.1707464635761735\n","Running Train loss: 1.1678294411727361\n","Running Train loss: 1.1652865532905825\n","Running Train loss: 1.1606810788313549\n","Running Train loss: 1.1583155010156572\n","Running Train loss: 1.1569377840319766\n","Running Train loss: 1.1534722758539069\n","Running Train loss: 1.1511885602027179\n","Running Train loss: 1.147660574187403\n","Running Train loss: 1.145789071733569\n","Running Train loss: 1.1434540536506044\n","Running Train loss: 1.14124237219008\n","Running Train loss: 1.1382258552493472\n","Running Train loss: 1.136215606726796\n","Running Train loss: 1.1340130770991663\n","Running Train loss: 1.1326891407370567\n","Running Train loss: 1.1302765188132518\n","Running Train loss: 1.1303940026199117\n","Running Train loss: 1.129168554704789\n","Running Train loss: 1.1271538179974223\n","Running Train loss: 1.1254630068134022\n","Running Train loss: 1.1234141752637665\n","Running Train loss: 1.1223029490879604\n","Running Train loss: 1.1190264963290908\n","Running Train loss: 1.1158625052473639\n","Running Train loss: 1.1117445690913146\n","Running Train loss: 1.109648571500565\n","Running Train loss: 1.1084106678764025\n","Running Train loss: 1.1059644714574128\n","Running Train loss: 1.1037329080013127\n","Running Train loss: 1.1037273574722268\n","Running Train loss: 1.1022405451082664\n","Running Train loss: 1.1005860787791175\n","Running Train loss: 1.0988100496351079\n","Running Train loss: 1.0967593873567123\n","Running Train loss: 1.0941428610302033\n","Running Train loss: 1.0926472092116322\n","Running Train loss: 1.09239525653814\n","Running Train loss: 1.0907271743132807\n","Running Train loss: 1.0892167400258284\n","Running Train loss: 1.0908633703703707\n","Running Train loss: 1.0902926825063746\n","Running Train loss: 1.0896313515993266\n","Running Train loss: 1.087123164260874\n","Running Train loss: 1.0852286435928442\n","Running Train loss: 1.085939688664494\n","Running Train loss: 1.0845880629728788\n","Running Train loss: 1.0836031205952168\n","Running Train loss: 1.0818676454807394\n","Running Train loss: 1.0820922826481338\n","Running Train loss: 1.0803812191697764\n","Running Train loss: 1.0791156673256088\n","Running Train loss: 1.0784332489095083\n","Running Train loss: 1.0771680802685544\n","Running Train loss: 1.075074964531378\n","Running Train loss: 1.073787806269068\n","Running Train loss: 1.0728713914252925\n","Running Train loss: 1.071273820882752\n","Running Train loss: 1.0718303965448768\n","Running Train loss: 1.0702231522157508\n","Running Train loss: 1.070471874565026\n","Running Train loss: 1.070083757287988\n","Running Train loss: 1.068719923634862\n","Running Train loss: 1.0664348291854064\n","Running Train loss: 1.0648000164790088\n","Running Train loss: 1.0631200008162665\n","Running Train loss: 1.0614884486209313\n","Running Train loss: 1.05919231379574\n","Running Train loss: 1.0577268725876354\n","Running Train loss: 1.056420642379168\n","Running Train loss: 1.0554023699108261\n","Running Train loss: 1.0548646704160742\n","Running Train loss: 1.0552080486880409\n","Running Train loss: 1.0535973227393312\n","Running Train loss: 1.0520113080346112\n","Running Train loss: 1.0514419728465247\n","Running Train loss: 1.0519266046551117\n","Running Train loss: 1.0508216234652892\n","Running Train loss: 1.0494741790996485\n","Running Train loss: 1.0468043191679592\n","Running Train loss: 1.0456292872265174\n","Running Train loss: 1.04352993486274\n","Running Train loss: 1.0423219701077076\n","Running Train loss: 1.0407315722461474\n","Running Train loss: 1.03932101515275\n","Running Train loss: 1.03667051301283\n","Running Train loss: 1.0348091322507817\n","Running Train loss: 1.03349025969704\n","Running Train loss: 1.0322729613771082\n","Running Train loss: 1.030834244548782\n","Running Train loss: 1.0304116113686268\n","Running Train loss: 1.0283441853816393\n","Running Train loss: 1.025957265435433\n","Running Train loss: 1.0242193443019216\n","Running Train loss: 1.0224023241745799\n","Running Train loss: 1.020888737613155\n","Running Train loss: 1.0201560960237281\n","Running Train loss: 1.0181308138370513\n","Running Train loss: 1.0161659150009612\n","Running Train loss: 1.0149096172480356\n","Running Train loss: 1.0129271981273245\n","Running Train loss: 1.0115154444232701\n","Running Train loss: 1.0099772121392043\n","Running Train loss: 1.0080188477877527\n","Running Train loss: 1.0060843319744452\n","Running Train loss: 1.0047131698260936\n","Running Train loss: 1.0027412272788383\n","Running Train loss: 1.001329323190909\n","Running Train loss: 0.9990237918164995\n","Running Train loss: 0.9975384056340647\n","Running Train loss: 0.9958505046911565\n","Running Train loss: 0.994503679036191\n","Running Train loss: 0.9918829080068841\n","Running Train loss: 0.9898138135895693\n","Running Train loss: 0.9879452496878663\n","Running Train loss: 0.9870568044594864\n","Running Train loss: 0.98503767059195\n","Running Train loss: 0.9830863330099318\n","Running Train loss: 0.9812873016864171\n","Running Train loss: 0.979633363073363\n","Running Train loss: 0.978010071284605\n","Running Train loss: 0.9765985729920603\n","Running Train loss: 0.9747753304784949\n","Running Train loss: 0.973522293286911\n","Running Train loss: 0.9715928655884326\n","Running Train loss: 0.9703421053483332\n","Running Train loss: 0.9682013402062077\n","Running Train loss: 0.965764485512461\n","Running Train loss: 0.9640860338015913\n","Running Train loss: 0.9628014058297407\n","Running Train loss: 0.9613719036006253\n","Running Train loss: 0.9598609243690128\n","Running Train loss: 0.9583520919607397\n","Running Train loss: 0.9569787817609894\n","Running Train loss: 0.9547170836751054\n","Running Train loss: 0.9529500028325452\n","Running Train loss: 0.9516096139861638\n","Running Train loss: 0.9500947446658694\n","Running Train loss: 0.9482212667817512\n","Running Train loss: 0.9466426308228545\n","Running Train loss: 0.9460248457897238\n","Running Train loss: 0.9453662653155879\n","Running Train loss: 0.9441753380379434\n","Running Train loss: 0.943478778910798\n","Running Train loss: 0.9424745921133343\n","Running Train loss: 0.941432973382457\n","Running Train loss: 0.9404607735150634\n","Running Train loss: 0.9385099466641744\n","Running Train loss: 0.9376957919510496\n","Running Train loss: 0.9367527501867307\n","Running Train loss: 0.9361791341218224\n","Running Train loss: 0.9343843163040123\n","Running Train loss: 0.9330199416543616\n","Running Train loss: 0.9314395968430962\n","Running Train loss: 0.9298594209580934\n","Running Train loss: 0.9285374574072949\n","Running Train loss: 0.9276153910121485\n","Running Train loss: 0.9260914128634238\n","Running Train loss: 0.9246323838877908\n","Running Train loss: 0.9234877278407415\n","Running Train loss: 0.9220962533935572\n","Running Train loss: 0.9205948333641526\n","Running Train loss: 0.9203938687604571\n","Running Train loss: 0.9202091687276394\n","Running Train loss: 0.9190078412508739\n","Running Train loss: 0.9180970408444135\n","Running Train loss: 0.9168921764180952\n","Running Train loss: 0.9152464183978737\n","Running Train loss: 0.9139784124224357\n","Running Train loss: 0.9126184796509536\n","Running Train loss: 0.9123313571831021\n","Running Train loss: 0.9113748963048429\n","Running Train loss: 0.9098570510974298\n","Running Train loss: 0.9084136731054154\n","Running Train loss: 0.9066611809103496\n","Running Train loss: 0.9056469507333709\n","Running Train loss: 0.9043294811864754\n","Running Train loss: 0.9032283819986112\n","Running Train loss: 0.9018366936470447\n","Running Train loss: 0.900198221027133\n","Running Train loss: 0.8993482904749232\n","Running Train loss: 0.8983194244478991\n","Running Train loss: 0.8966198245091225\n","Running Train loss: 0.8962353522933665\n","Running Train loss: 0.8945273094608805\n","Running Train loss: 0.8934738779738105\n","Running Train loss: 0.8923623548672263\n","Running Train loss: 0.8912872113725718\n","Running Train loss: 0.8902708915146914\n","Running Train loss: 0.8889003285364798\n","Running Train loss: 0.8878885459795638\n","Running Train loss: 0.8871479118459447\n","Running Train loss: 0.88607716517172\n","Running Train loss: 0.8852025607245506\n","Running Train loss: 0.8843542611392843\n","Running Train loss: 0.8830370175084848\n","Running Train loss: 0.8824659030895179\n","Running Train loss: 0.8812013047933579\n","Running Train loss: 0.8804496551341141\n","Running Train loss: 0.8798453893343156\n","Running Train loss: 0.8797853562885911\n","Running Train loss: 0.8791165938653515\n","Running Train loss: 0.880175085890461\n","Running Train loss: 0.8793012969446986\n","Running Train loss: 0.8784633712107394\n","Running Train loss: 0.8776947202462724\n","Running Train loss: 0.8779232486210826\n","Running Train loss: 0.8775766178137726\n","Running Train loss: 0.8778624598174214\n","Running Train loss: 0.8771728602397507\n","Running Train loss: 0.8768329412648173\n","Running Train loss: 0.875874328989904\n","Running Train loss: 0.8754886338972065\n","Running Train loss: 0.8764340121075104\n","Running Train loss: 0.876194371433284\n","Running Train loss: 0.875988123614503\n","Running Train loss: 0.8750130659357965\n","Running Train loss: 0.8747016496754981\n","Running Train loss: 0.8751835680875495\n","Running Train loss: 0.8756472589347952\n","Running Train loss: 0.8748222631839261\n","Running Train loss: 0.8770711700068438\n","Running Train loss: 0.8776015247503917\n","Running Train loss: 0.8774574938765232\n","Running Train loss: 0.877496082444406\n","Running Train loss: 0.8770448145254579\n","Running Train loss: 0.8767906355826396\n","Running Train loss: 0.8760726088756009\n","Running Train loss: 0.8755118322028262\n","Running Train loss: 0.8746895102774286\n","Running Train loss: 0.8740861922424705\n","Running Train loss: 0.8730006240463505\n","Running Train loss: 0.8727380415835938\n","Running Train loss: 0.8740366438664303\n","Running Train loss: 0.873578813109903\n","Running Train loss: 0.8732943853613028\n","Running Train loss: 0.8721448705557993\n","Running Train loss: 0.8719529732679709\n","Running Train loss: 0.871543653480842\n","Running Train loss: 0.8711166526285969\n","Running Train loss: 0.8707766919645644\n","Running Train loss: 0.8702281507743796\n","Running Train loss: 0.8689260020286222\n","Running Train loss: 0.8683563220230016\n","Running Train loss: 0.8673905161976514\n","Running Train loss: 0.8664883750317683\n","Running Train loss: 0.8667197907927042\n","Running Train loss: 0.8672130127996206\n","Running Train loss: 0.8670291626393943\n","Running Train loss: 0.8662391046238183\n","Running Train loss: 0.8653338185462999\n","Running Train loss: 0.8648442167899396\n","Running Train loss: 0.8636867905104602\n","Running Train loss: 0.8642086377137987\n","Running Train loss: 0.8634819185558057\n","Running Train loss: 0.8629494105630061\n","Running Train loss: 0.8624877322711105\n","Running Train loss: 0.8617611359532287\n","Running Train loss: 0.8614368178403581\n","Running Train loss: 0.8606097560050419\n","Running Train loss: 0.8596899988576229\n","Running Train loss: 0.8588265051012454\n","Running Train loss: 0.8590635697525668\n","Running Train loss: 0.8582510680246812\n","Running Train loss: 0.8573977612763\n","Running Train loss: 0.8566650967848929\n","Running Train loss: 0.8562187801397502\n","Running Train loss: 0.8555097792829786\n","Running Train loss: 0.8546791584927792\n","Running Train loss: 0.8542677027071822\n","Running Train loss: 0.854884738195027\n","Running Train loss: 0.8543141735893376\n","Running Train loss: 0.8541155108283548\n","Running Train loss: 0.8538404935402489\n","Running Train loss: 0.8539822313768802\n","Running Train loss: 0.8533825006718948\n","Running Train loss: 0.8526067121045573\n","Running Train loss: 0.8512793427289919\n","Running Train loss: 0.8503727446852594\n","Running Train loss: 0.8503955663354309\n","Running Train loss: 0.8496089372568814\n","Running Train loss: 0.8490278731842744\n","Running Train loss: 0.8493739347348268\n","Running Train loss: 0.8485158831564659\n","Running Train loss: 0.8480718837586366\n","Running Train loss: 0.8478107193153198\n","Running Train loss: 0.8513183650096077\n","Running Train loss: 0.8505420117893002\n","Running Train loss: 0.8514382565508083\n","Running Train loss: 0.8534503896716493\n","Running Train loss: 0.8529366585121197\n","Running Train loss: 0.8539909392192557\n","Running Train loss: 0.853585540913464\n","Running Train loss: 0.8531106438471063\n","Running Train loss: 0.8530619799824103\n","Running Train loss: 0.8523685401970786\n","Running Train loss: 0.8518693471671743\n","Running Train loss: 0.8512330821487639\n","Running Train loss: 0.8510273837065221\n","Running Train loss: 0.850238313585256\n","Running Train loss: 0.8499267303917318\n","Running Train loss: 0.8492014307019994\n","Running Train loss: 0.848387476834622\n","Running Train loss: 0.8485303899567378\n","Running Train loss: 0.8488940378117091\n","Running Train loss: 0.8497579175181784\n","Running Train loss: 0.8502457037340841\n","Running Train loss: 0.8494543198010196\n","Running Train loss: 0.849023405422099\n","Running Train loss: 0.8482823277061636\n","Running Train loss: 0.8470683155770435\n","Running Train loss: 0.8460934968215639\n","Running Train loss: 0.8460205065947707\n","Running Train loss: 0.8452039692279095\n","Running Train loss: 0.8445949994777459\n","Running Train loss: 0.8445402974756355\n","Running Train loss: 0.8434585275680526\n","Running Train loss: 0.8438797434593769\n","Running Train loss: 0.8434621923288722\n","Running Train loss: 0.8427136536624472\n","Running Train loss: 0.8426309477451236\n","Running Train loss: 0.8418561448402042\n","Running Train loss: 0.8411318720641889\n","Running Train loss: 0.8408254395638194\n","Running Train loss: 0.8402783202800111\n","Running Train loss: 0.8395861844263316\n","Running Train loss: 0.8389317130121657\n","Running Train loss: 0.8391116113091509\n","Running Train loss: 0.8390767952880344\n","Running Train loss: 0.838538191687022\n","Running Train loss: 0.8380226403413106\n","Running Train loss: 0.8375615465862691\n","Running Train loss: 0.8370248350285993\n","Running Train loss: 0.8371950150639923\n","Running Train loss: 0.837338765972204\n","Running Train loss: 0.8371690658882993\n","Running Train loss: 0.8363695497527445\n","Running Train loss: 0.8360023381150499\n","Running Train loss: 0.8352212519242661\n","Running Train loss: 0.8351522903980279\n","Running Train loss: 0.8359524409983753\n","Running Train loss: 0.8350741853598158\n","Running Train loss: 0.8346138448426218\n","Running Train loss: 0.8339970689867774\n","Running Train loss: 0.833778536175818\n","Running Train loss: 0.8337594377228534\n","Running Train loss: 0.8330841183901311\n","Running Train loss: 0.8325874205827714\n","Running Train loss: 0.8317763549006152\n","Running Train loss: 0.8311026602746006\n","Running Train loss: 0.8305742930820874\n","Running Train loss: 0.8295656019851328\n","Running Train loss: 0.8284036449571647\n","Running Train loss: 0.8272166370815439\n","Running Train loss: 0.8257619619398898\n","Running Train loss: 0.8245874274463401\n","Running Train loss: 0.8242129474614833\n","Running Train loss: 0.8239058427512645\n","Running Train loss: 0.8233278431800247\n","Running Train loss: 0.8225529524643207\n","Running Train loss: 0.8219945511070841\n","Running Train loss: 0.8223332808088468\n","Running Train loss: 0.8249675205755003\n","Running Train loss: 0.8248809599281512\n","Running Train loss: 0.8245540172639853\n","Running Train loss: 0.8245149868249203\n","Running Train loss: 0.823736560898824\n","Running Train loss: 0.8232504041865468\n","Running Train loss: 0.8229712449915121\n","Running Train loss: 0.8224199803196379\n","Running Train loss: 0.8218815017413227\n","Running Train loss: 0.8211201011480493\n","Running Train loss: 0.8205241964686485\n","Running Train loss: 0.8202186489314861\n","Running Train loss: 0.8196909821943947\n","Running Train loss: 0.8193596645089035\n","Running Train loss: 0.8188617514104835\n","Running Train loss: 0.8183150630655154\n","Running Train loss: 0.8175355024958509\n","Running Train loss: 0.8175563426422221\n","Running Train loss: 0.8170407014788874\n","Running Train loss: 0.8164111795078279\n","Running Train loss: 0.8159606455224697\n","Running Train loss: 0.8157200698337671\n","Running Train loss: 0.8157509244159835\n","Running Train loss: 0.8153071366527488\n","Running Train loss: 0.8144228111862694\n","Running Train loss: 0.814111632839949\n","Running Train loss: 0.8136709849875207\n","Running Train loss: 0.8138426705727498\n","Running Train loss: 0.813961212216271\n","Running Train loss: 0.8136524829383501\n","Running Train loss: 0.8126862284377081\n","Running Train loss: 0.8117555695493798\n","Running Train loss: 0.8108359956790367\n","Running Train loss: 0.8114945183841199\n","Running Train loss: 0.8110210277913698\n","Running Train loss: 0.8103720930625092\n","Running Train loss: 0.8100375471579184\n","Running Train loss: 0.809711811132729\n","Running Train loss: 0.8091142232147522\n","Running Train loss: 0.8083173144299416\n","Running Train loss: 0.8087896407321766\n","Running Train loss: 0.8081676326558102\n","Running Train loss: 0.8077783393180349\n","Running Train loss: 0.8071518172526658\n","Running Train loss: 0.8062373078919693\n","Running Train loss: 0.8056560716591775\n","Running Train loss: 0.8050971887673711\n","Running Train loss: 0.8043297088151513\n","Running Train loss: 0.8031822151469719\n","Running Train loss: 0.8021862523907677\n","Running Train loss: 0.8011722123596521\n","Running Train loss: 0.8007671980338678\n","Running Train loss: 0.8002962706449591\n","Running Train loss: 0.7997462219414366\n","Running Train loss: 0.7992519617787773\n","Running Train loss: 0.798899869215593\n","Running Train loss: 0.7982624684527334\n","Running Train loss: 0.797756407318728\n","Running Train loss: 0.7974361235220603\n","Running Train loss: 0.7970752827044356\n","Running Train loss: 0.7970187670122022\n","Running Train loss: 0.796477549489484\n","Running Train loss: 0.7961883381577257\n","Running Train loss: 0.7958374460546822\n","Running Train loss: 0.7953977090578417\n","Running Train loss: 0.7953739636802468\n","Running Train loss: 0.7958716668970819\n","Running Train loss: 0.7951440375771924\n","Running Train loss: 0.7950780108366577\n","Running Train loss: 0.7944494448984935\n","Running Train loss: 0.7943380439255991\n","Running Train loss: 0.7938891508753185\n","Running Train loss: 0.7934068152887248\n","Running Train loss: 0.7928289500299562\n","Running Train loss: 0.7923182149770305\n","Running Train loss: 0.791723426002062\n","Running Train loss: 0.7912024895324925\n","Running Train loss: 0.7904640097246581\n","Running Train loss: 0.7901308940971682\n","Running Train loss: 0.7896694250354662\n","Running Train loss: 0.7890671530065416\n","Running Train loss: 0.7891403774062059\n","Running Train loss: 0.7888571130146134\n","Running Train loss: 0.7886706863134402\n","Running Train loss: 0.7883677057088516\n","Running Train loss: 0.7878213980173071\n","Running Train loss: 0.7874960859062866\n","Running Train loss: 0.787602034258189\n","Running Train loss: 0.7868230822148607\n","Running Train loss: 0.7867822292698732\n","Running Train loss: 0.7864141415835412\n","Running Train loss: 0.7856774689512874\n","Running Train loss: 0.7852726072392118\n","Running Train loss: 0.7846969310561881\n","Running Train loss: 0.7843915624742829\n","Running Train loss: 0.7839272171136785\n","Running Train loss: 0.7835092880502466\n","Running Train loss: 0.7829608450601109\n","Running Train loss: 0.7821908682998009\n","Running Train loss: 0.7813207651169952\n","Running Train loss: 0.7804458344491517\n","Running Train loss: 0.7797388410824654\n","Running Train loss: 0.7789006484989409\n","Running Train loss: 0.7781684484610087\n","Running Train loss: 0.7776656666099544\n","Running Train loss: 0.7776248117728579\n","Running Train loss: 0.7775045468347468\n","Running Train loss: 0.7775077349431453\n","Running Train loss: 0.7773325777288234\n","Running Train loss: 0.7767734066822017\n","Running Train loss: 0.7764692751526833\n","Running Train loss: 0.7757538950886018\n","Running Train loss: 0.7754676585015878\n","Running Train loss: 0.7752717263213579\n","Running Train loss: 0.7746572202287721\n","Running Train loss: 0.7739292333168643\n","Running Train loss: 0.7733400650263399\n","Running Train loss: 0.7727822177701548\n","Running Train loss: 0.7721986722724886\n","Running Train loss: 0.7717502786181515\n","Running Train loss: 0.7715059214337604\n","Running Train loss: 0.7709982112260922\n","Running Train loss: 0.7705405152035845\n","Running Train loss: 0.7703743075899383\n","Running Train loss: 0.7704349965766562\n","Running Train loss: 0.7702947858371771\n","Running Train loss: 0.7697943250144513\n","Running Train loss: 0.7694589358368581\n","Running Train loss: 0.769091271883685\n","Running Train loss: 0.7686614792747712\n","Running Train loss: 0.7682629297523536\n","Running Train loss: 0.7678349633173492\n","Running Train loss: 0.7673415386073372\n","Running Train loss: 0.7665610635009261\n","Running Train loss: 0.7660662695444053\n","Running Train loss: 0.7654617940577176\n","Running Train loss: 0.7649783178156788\n","Running Train loss: 0.7642975463846161\n","Running Train loss: 0.7643512922466075\n","Running Train loss: 0.7637736099325007\n","Running Train loss: 0.7634323798176896\n","Running Train loss: 0.7629584836455562\n","Running Train loss: 0.7622620025295282\n","Running Train loss: 0.7619916303111608\n","Running Train loss: 0.7620628239284517\n","Running Train loss: 0.761614774991617\n","Running Train loss: 0.7613548886780479\n","Running Train loss: 0.7611336389899794\n","Running Train loss: 0.7611590644195788\n","Running Train loss: 0.7606049117306808\n","Running Train loss: 0.760229296175609\n","Running Train loss: 0.759906428454964\n","Running Train loss: 0.7600743842200897\n","Running Train loss: 0.7596467704941591\n","Running Train loss: 0.7600315472581639\n","Running Train loss: 0.7601143523049888\n","Running Train loss: 0.7597545433439725\n","Running Train loss: 0.7595966214513672\n","Running Train loss: 0.7588878276288775\n","Running Train loss: 0.7584473378514147\n","Running Train loss: 0.7575918619610645\n","Running Train loss: 0.7567664049676007\n","Running Train loss: 0.7560758039003109\n","Running Train loss: 0.7558884564732204\n","Running Train loss: 0.7559546091222096\n","Running Train loss: 0.7559990630759036\n","Running Train loss: 0.7555158106219051\n","Running Train loss: 0.7552213364136009\n","Running Train loss: 0.7545813157456337\n","Running Train loss: 0.7542279998265338\n","Running Train loss: 0.7536846316100037\n","Running Train loss: 0.7531304984481099\n","Running Train loss: 0.75277353542297\n","Running Train loss: 0.7523342153858826\n","Running Train loss: 0.751777133321814\n","Running Train loss: 0.7512533086257568\n","Running Train loss: 0.7507524221122006\n","Running Train loss: 0.7505290301441285\n","Running Train loss: 0.7501575357964717\n","Running Train loss: 0.7495500787942313\n","Running Train loss: 0.7490359710167638\n","Running Train loss: 0.7486815000295468\n","Running Train loss: 0.7477849911112696\n","Running Train loss: 0.7473322291215375\n","Running Train loss: 0.7468195134009074\n","Running Train loss: 0.7464966605603695\n","Running Train loss: 0.745883985333538\n","Running Train loss: 0.7455609711444277\n","Running Train loss: 0.745200952650134\n","Running Train loss: 0.7448518703141335\n","Running Train loss: 0.7443917330063826\n","Running Train loss: 0.7436533925511344\n","Running Train loss: 0.7431307332814161\n","Running Train loss: 0.7432302408088736\n","Running Train loss: 0.7437143566031719\n","Running Train loss: 0.7435656471781328\n","Running Train loss: 0.7431797405125891\n","Running Train loss: 0.7425524124645451\n","Running Train loss: 0.7422269180855945\n","Running Train loss: 0.7417477403326529\n","Running Train loss: 0.7414331240045441\n","Running Train loss: 0.7408385831841876\n","Running Train loss: 0.7405270208998205\n","Running Train loss: 0.7401511776040524\n","Running Train loss: 0.7398631740801855\n","Running Train loss: 0.7398748328909278\n","Running Train loss: 0.7397866248051105\n","Running Train loss: 0.7395482233150183\n","Running Train loss: 0.7393612901823154\n","Running Train loss: 0.7392492090758368\n","Running Train loss: 0.7389851310951956\n","Running Train loss: 0.7390889250173056\n","Running Train loss: 0.7392060393275388\n","Running Train loss: 0.7391842199837441\n","Running Train loss: 0.7386243114623513\n","Running Train loss: 0.7382167720019001\n","Running Train loss: 0.7377525482596843\n","Running Train loss: 0.7373096103591671\n","Running Train loss: 0.7368436096106512\n","Running Train loss: 0.7363826657876332\n","Running Train loss: 0.7359006208830139\n","Running Train loss: 0.7356479644329975\n","Running Train loss: 0.7352262662781789\n","Running Train loss: 0.7353824247230036\n","Running Train loss: 0.73523234000306\n","Running Train loss: 0.7349344331871819\n","Running Train loss: 0.7345415318583348\n","Running Train loss: 0.734363109375428\n","Running Train loss: 0.7340200829995273\n","Running Train loss: 0.733497648449835\n","Running Train loss: 0.7329327647517991\n","Running Train loss: 0.7323456924139015\n","Running Train loss: 0.7318781768900643\n","Running Train loss: 0.7315286162662953\n","Running Train loss: 0.7309685508423399\n","Running Train loss: 0.730656626880169\n","Running Train loss: 0.7301667887623237\n","Running Train loss: 0.729925917758745\n","Running Train loss: 0.7296657167464139\n","Running Train loss: 0.7290773460695851\n","Running Train loss: 0.7285814138240372\n","Running Train loss: 0.7281908371815922\n","Running Train loss: 0.7277013147089881\n","Running Train loss: 0.7273613725811007\n","Running Train loss: 0.7271308985505651\n","Running Train loss: 0.7264808304019664\n","Running Train loss: 0.7260832115518279\n","Running Train loss: 0.7258068989776564\n","Running Train loss: 0.7254855337688214\n","Running Train loss: 0.7249984335329832\n","Running Train loss: 0.7242934332174413\n","Running Train loss: 0.7236621354368585\n","Running Train loss: 0.7233107747629071\n","Running Train loss: 0.7235696829739027\n","Running Train loss: 0.7230077911221624\n","Running Train loss: 0.722987191224253\n","Running Train loss: 0.722527795245366\n","Running Train loss: 0.7220702012856081\n","Running Train loss: 0.7212711077226859\n","Running Train loss: 0.7207280270350996\n","Running Train loss: 0.7202829341734609\n","Running Train loss: 0.7199119688923826\n","Running Train loss: 0.7195888970962976\n","Running Train loss: 0.7192074416688904\n","Running Train loss: 0.7190550375964124\n","Running Train loss: 0.71870529708954\n","Running Train loss: 0.7185975701265542\n","Running Train loss: 0.7182891012533851\n","Running Train loss: 0.7178176897619571\n","Running Train loss: 0.7174144127615252\n","Running Train loss: 0.7171866286712087\n","Running Train loss: 0.7168993756895453\n","Running Train loss: 0.716783260413016\n","Running Train loss: 0.716397395631686\n","Running Train loss: 0.7159591674880232\n","Running Train loss: 0.7155449711446521\n","Running Train loss: 0.7149099718341333\n","Running Train loss: 0.7146093723260694\n","Running Train loss: 0.714202650765152\n","Running Train loss: 0.7139994507134712\n","Running Train loss: 0.7135663078078683\n","Running Train loss: 0.7135208102139696\n","Running Train loss: 0.7130949380743309\n","Running Train loss: 0.7126182506519153\n","Running Train loss: 0.712344699997329\n","Running Train loss: 0.7117286711372435\n","Running Train loss: 0.7110666355278608\n","Running Train loss: 0.7109295897687462\n","Running Train loss: 0.710590373144797\n","Running Train loss: 0.710565816833457\n","Running Train loss: 0.7100134357717467\n","Running Train loss: 0.7099177687066068\n","Running Train loss: 0.7093517820372753\n","Running Train loss: 0.7087321629618654\n","Running Train loss: 0.7084521906926988\n","Running Train loss: 0.7079787911456308\n","Running Train loss: 0.7074986419533684\n","Running Train loss: 0.7072542531076323\n","Running Train loss: 0.707059573313407\n","Running Train loss: 0.7068592510952704\n","Running Train loss: 0.7066624793172614\n","Running Train loss: 0.7069696454221711\n","Running Train loss: 0.706593663448803\n","Running Train loss: 0.7064518533228079\n","Running Train loss: 0.7061848779810508\n","Running Train loss: 0.7058039502763167\n","Running Train loss: 0.7056394685853848\n","Running Train loss: 0.7054205547356548\n","Running Train loss: 0.7048567574412391\n","Running Train loss: 0.704572221861013\n","Running Train loss: 0.7044938253995144\n","Running Train loss: 0.7042067588169407\n","Running Train loss: 0.7040345568971161\n","Running Train loss: 0.7038372355886704\n","Running Train loss: 0.7036467716187707\n","Running Train loss: 0.7034815604069147\n","Running Train loss: 0.7032304672988313\n","Running Train loss: 0.7027769215548267\n","Running Train loss: 0.7036572208925456\n","Running Train loss: 0.7038217927340409\n","Running Train loss: 0.7034874817211471\n","Running Train loss: 0.7032130050103059\n","Running Train loss: 0.7030119379290258\n","Running Train loss: 0.7030095624112855\n","Running Train loss: 0.7027361136511483\n","Running Train loss: 0.702949628730615\n","Running Train loss: 0.7028165259060764\n","Running Train loss: 0.7023687923322665\n","Running Train loss: 0.7022903438564722\n","Running Train loss: 0.7020024879001329\n","Running Train loss: 0.7020463858835796\n","Running Train loss: 0.7018781071154502\n","Running Train loss: 0.7016050302222319\n","Running Train loss: 0.7014294695109129\n","Running Train loss: 0.701328688607199\n","Running Train loss: 0.7011142095046885\n","Running Train loss: 0.7008145683730671\n","Running Train loss: 0.700825383230834\n","Running Train loss: 0.7006314548140813\n","Running Train loss: 0.7004947948162673\n","Running Train loss: 0.7004722841650421\n","Running Train loss: 0.7003623054615248\n","Running Train loss: 0.7002741043111407\n","Running Train loss: 0.7002044767141342\n","Running Train loss: 0.69984865216354\n","Running Train loss: 0.699493510050829\n","Running Train loss: 0.6991872674981338\n","Running Train loss: 0.6991983610972451\n","Running Train loss: 0.6992267282163834\n","Running Train loss: 0.6989675976887897\n","Running Train loss: 0.6987122969820321\n","Running Train loss: 0.6987562156164068\n","Running Train loss: 0.6984784277940154\n","Running Train loss: 0.698427590150987\n","Running Train loss: 0.6982088767343615\n","Running Train loss: 0.6982814530531566\n","Running Train loss: 0.6978581225420386\n","Running Train loss: 0.6978280013854351\n","Running Train loss: 0.6978448033332825\n","Running Train loss: 0.6976831507628243\n","Running Train loss: 0.6974893817901612\n","Running Train loss: 0.6969741264945024\n","Running Train loss: 0.6966326146162494\n","Running Train loss: 0.696466766603308\n","Running Train loss: 0.6962258716932879\n","Running Train loss: 0.6961601467464458\n","Running Train loss: 0.695894401558547\n","Running Train loss: 0.6955318087521865\n","Running Train loss: 0.6953805335549509\n","Running Train loss: 0.6951044287650564\n","Running Train loss: 0.6949885012234671\n","Running Train loss: 0.6951535985411963\n","Running Train loss: 0.6947458257904988\n","Running Train loss: 0.6943848537331497\n","Running Train loss: 0.6942174003718272\n","Running Train loss: 0.6939412961682577\n","Running Train loss: 0.6936818568493782\n","Running Train loss: 0.6934363069054524\n","Running Train loss: 0.6932318561187917\n","Running Train loss: 0.6931371248308444\n","Running Train loss: 0.6927244800262611\n","Running Train loss: 0.6922821210464463\n","Running Train loss: 0.6919336740421744\n","Running Train loss: 0.6914591023777269\n","Running Train loss: 0.6911396924427009\n","Running Train loss: 0.691263737330834\n","Running Train loss: 0.6909441119855834\n","Running Train loss: 0.6908122332324739\n","Running Train loss: 0.6906481278464116\n","Running Train loss: 0.6902316209522232\n","Running Train loss: 0.6899801780011773\n","Running Train loss: 0.689720302770864\n","Running Train loss: 0.6894438838649873\n","Running Train loss: 0.689186358717701\n","Running Train loss: 0.6889201725768571\n","Running Train loss: 0.6884997562228978\n","Running Train loss: 0.6880749037937066\n","Running Train loss: 0.6877393111619249\n","Running Train loss: 0.687521463564035\n","Running Train loss: 0.6874268493356277\n","Running Train loss: 0.6872267274420119\n","Running Train loss: 0.6870108186124975\n","Running Train loss: 0.6865684462380123\n","Running Train loss: 0.6863926346038421\n","Running Train loss: 0.6862854601022857\n","Running Train loss: 0.6858766554652349\n","Running Train loss: 0.6855997709349105\n","Running Train loss: 0.6854683045871605\n","Running Train loss: 0.6856278616487012\n","Running Train loss: 0.6856289786081036\n","Running Train loss: 0.6854292865862718\n","Running Train loss: 0.685449620833536\n","Running Train loss: 0.685058521333893\n","Running Train loss: 0.6848108918598756\n","Running Train loss: 0.6843757702055737\n","Running Train loss: 0.684012332093972\n","Running Train loss: 0.6836269195846789\n","Running Train loss: 0.6839022722910658\n","Running Train loss: 0.6835734344148432\n","Running Train loss: 0.6834860866693919\n","Running Train loss: 0.6829539131193875\n","Running Train loss: 0.682612612668393\n","Running Train loss: 0.6821457530543796\n","Running Train loss: 0.6818153277706744\n","Running Train loss: 0.6814533744058924\n","Running Train loss: 0.6810802330679082\n","Running Train loss: 0.6812145337224893\n","Running Train loss: 0.6815968031526372\n","Running Train loss: 0.681413079970462\n","Running Train loss: 0.6811020372719583\n","Running Train loss: 0.6810723797038749\n","Running Train loss: 0.6808679404079788\n","Running Train loss: 0.6804470113363538\n","Running Train loss: 0.67995497999312\n","Running Train loss: 0.6796591398713461\n","Running Train loss: 0.6792606811774404\n","Running Train loss: 0.6790046150375992\n","Running Train loss: 0.6787110304131228\n","Running Train loss: 0.6784912128576076\n","Running Train loss: 0.6782913112727851\n","Running Train loss: 0.6779645291298472\n","Running Train loss: 0.6776221525781324\n","Running Train loss: 0.6771432048910713\n","Running Train loss: 0.6767756672953266\n","Running Train loss: 0.6764593071248907\n","Running Train loss: 0.6761580748793979\n","Running Train loss: 0.6759222953054088\n","Running Train loss: 0.6758006786977923\n","Running Train loss: 0.6753417601806971\n","Running Train loss: 0.6750449186878827\n","Running Train loss: 0.6752980919545178\n","Running Train loss: 0.6751500787541239\n","Running Train loss: 0.6750975019898509\n","Running Train loss: 0.6748045623641122\n","Running Train loss: 0.6745661939230004\n","Running Train loss: 0.6747740005555841\n","Running Train loss: 0.6747530930657343\n","Running Train loss: 0.6753070895165327\n","Running Train loss: 0.6753269339483786\n","Running Train loss: 0.675076127098204\n","Running Train loss: 0.6746462336106178\n","Running Train loss: 0.6743547483271018\n","Running Train loss: 0.673990282379739\n","Running Train loss: 0.6737509181985826\n","Running Train loss: 0.6736641878583701\n","Running Train loss: 0.6735369982616025\n","Running Train loss: 0.6734640507614211\n","Running Train loss: 0.6733509155707301\n","Running Train loss: 0.6732208043458139\n","Running Train loss: 0.672993517246067\n","Running Train loss: 0.6728537045608317\n","Running Train loss: 0.672612279608211\n","Running Train loss: 0.6723651411776123\n","Running Train loss: 0.6721481982965459\n","Running Train loss: 0.6721014184434686\n","Running Train loss: 0.6719432594047652\n","Running Train loss: 0.6715156346499379\n","Running Train loss: 0.6713747462828553\n","Running Train loss: 0.6713515942820609\n","Running Train loss: 0.6712829685457036\n","Running Train loss: 0.6712410777807236\n","Running Train loss: 0.6710789687931538\n","Running Train loss: 0.6707727001716524\n","Running Train loss: 0.6706212893546464\n","Running Train loss: 0.6705712169050813\n","Running Train loss: 0.6702763821929694\n","Running Train loss: 0.6700725539789333\n","Running Train loss: 0.6698329598157705\n","Running Train loss: 0.6694974343209775\n","Running Train loss: 0.6692973410792322\n","Running Train loss: 0.668982751645259\n","Running Train loss: 0.6686601165837607\n","Running Train loss: 0.6683129523367489\n","Running Train loss: 0.6679102997960789\n","Running Train loss: 0.6674845131968837\n","Running Train loss: 0.667117758154279\n","Running Train loss: 0.6667963192824912\n","Running Train loss: 0.666536399925297\n","Running Train loss: 0.6661321986668908\n","Running Train loss: 0.6657091701730233\n","Running Train loss: 0.6654875482097635\n","Running Train loss: 0.6651899605021467\n","Running Train loss: 0.6649111382582663\n","Running Train loss: 0.6649507405803574\n","Running Train loss: 0.6646573644303246\n","Running Train loss: 0.6644469833111062\n","Running Train loss: 0.6640974957638226\n","Running Train loss: 0.6638480882559504\n","Running Train loss: 0.6636044004475034\n","Running Train loss: 0.6632794238830684\n","Running Train loss: 0.663057584311904\n","Running Train loss: 0.6626472158128756\n","Running Train loss: 0.6623882112014375\n","Running Train loss: 0.6620264595903543\n","Running Train loss: 0.6616316016752588\n","Running Train loss: 0.661418375011208\n","Running Train loss: 0.661112233215923\n","Running Train loss: 0.6607433530509241\n","Running Train loss: 0.660470832972305\n","Running Train loss: 0.660138548876031\n","Running Train loss: 0.659695147704963\n","Running Train loss: 0.6593963330230428\n","Running Train loss: 0.659089368745584\n","Running Train loss: 0.6587364296204552\n","Running Train loss: 0.6585339091079986\n","Running Train loss: 0.6581699576085577\n","Running Train loss: 0.6578132566726631\n","Running Train loss: 0.6580960146382079\n","Running Train loss: 0.6586462735758303\n","Running Train loss: 0.6594212535783021\n","Running Train loss: 0.6597957320047908\n","Running Train loss: 0.6598863466491435\n","Running Train loss: 0.6599013413575682\n","Running Train loss: 0.6598613359289998\n","Running Train loss: 0.659797568779769\n","Running Train loss: 0.6602934884883108\n","Running Train loss: 0.6604446258775174\n","Running Train loss: 0.6605063596804106\n","Running Train loss: 0.6603909513408076\n","Running Train loss: 0.6602055684467194\n","Running Train loss: 0.6602671656704627\n","Running Train loss: 0.6603369226425209\n","Running Train loss: 0.6602074829412161\n","Running Train loss: 0.6601282872668061\n","Running Train loss: 0.6600206213615434\n","Running Train loss: 0.6596879569848754\n","Running Train loss: 0.6594379944665381\n","Running Train loss: 0.659226752705381\n","Running Train loss: 0.6588933387541703\n","Running Train loss: 0.6585902070724651\n","Running Train loss: 0.6584487988775325\n","Running Train loss: 0.6582321381619232\n","Running Train loss: 0.6580321760437928\n","Running Train loss: 0.6578161078138968\n","Running Train loss: 0.6577628077163776\n","Running Train loss: 0.6574228429766459\n","Running Train loss: 0.6571892936940954\n","Running Train loss: 0.65700178669849\n","Running Train loss: 0.6567106712036676\n","Running Train loss: 0.6564465643668086\n","Running Train loss: 0.6563200490003408\n","Running Train loss: 0.6558907826787699\n","Running Train loss: 0.655582289966368\n","Running Train loss: 0.6553747360499538\n","Running Train loss: 0.6552403484841427\n","Running Train loss: 0.6550897949133758\n","Running Train loss: 0.6547235075546347\n","Running Train loss: 0.6543284037086747\n","Running Train loss: 0.6541597591652875\n","Running Train loss: 0.653867843681156\n","Running Train loss: 0.6537531780207761\n","Running Train loss: 0.6537014168050846\n","Running Train loss: 0.6534935444620846\n","Running Train loss: 0.6535413944819832\n","Running Train loss: 0.6533714499738481\n","Running Train loss: 0.6532760998251241\n","Running Train loss: 0.6531225827776107\n","Running Train loss: 0.6529798433730454\n","Running Train loss: 0.652874907412228\n","Running Train loss: 0.652803150647519\n","Running Train loss: 0.6526445060015813\n","Running Train loss: 0.6525049602517682\n","Running Train loss: 0.6521739981993829\n","Running Train loss: 0.6521795470206464\n","Running Train loss: 0.6522890371664531\n","Running Train loss: 0.6519833871451292\n","Running Train loss: 0.6517141590759388\n","Running Train loss: 0.6514174945233305\n","Running Train loss: 0.6514207994732549\n","Running Train loss: 0.6511688194629075\n","Running Train loss: 0.6508417204224686\n","Running Train loss: 0.6504550105193209\n","Running Train loss: 0.6501963520060817\n","Running Train loss: 0.6498925692606919\n","Running Train loss: 0.6496165070155379\n","Running Train loss: 0.6492674587546168\n","Running Train loss: 0.64900112991715\n","Running Train loss: 0.6488235000815752\n","Running Train loss: 0.6485568546059961\n","Running Train loss: 0.6483402705920235\n","Running Train loss: 0.6481498114196709\n","Running Train loss: 0.6480237528628345\n","Running Train loss: 0.6478536551353007\n","Running Train loss: 0.6477584930300073\n","Running Train loss: 0.6476063407138589\n","Running Train loss: 0.6475864337491137\n","Running Train loss: 0.6472975773858131\n","Running Train loss: 0.6471182701315684\n","Running Train loss: 0.646965728444068\n","Running Train loss: 0.6467265429976148\n","Running Train loss: 0.6465440922578176\n","Running Train loss: 0.6463889648764002\n","Running Train loss: 0.6462844742698381\n","Running Train loss: 0.6462571495443794\n","Running Train loss: 0.6458884071100064\n","Running Train loss: 0.6457145930391497\n","Running Train loss: 0.6455185202820759\n","Running Train loss: 0.64533425485814\n","Running Train loss: 0.6451709061887342\n","Running Train loss: 0.6447806705242742\n","Running Train loss: 0.6443812948622892\n","Running Train loss: 0.6441309586508384\n","Running Train loss: 0.6438496073375382\n","Running Train loss: 0.6434727475466124\n","Running Train loss: 0.6432278523511066\n","Running Train loss: 0.6429632820450423\n","Running Train loss: 0.6429226577517028\n","Running Train loss: 0.6427183383896347\n","Running Train loss: 0.6424835779926595\n","Running Train loss: 0.6421327083339016\n","Running Train loss: 0.6420028771765888\n","Running Train loss: 0.6417606776063355\n","Running Train loss: 0.641394380392359\n","Running Train loss: 0.6409782232645289\n","Running Train loss: 0.6406496822496826\n","Running Train loss: 0.6404159955226857\n","Running Train loss: 0.6400900682892414\n","Running Train loss: 0.6398019209219556\n","Running Train loss: 0.6396297580996287\n","Running Train loss: 0.6392887701220694\n","Running Train loss: 0.6389412237194193\n","Running Train loss: 0.6385733396794556\n","Running Train loss: 0.6383879614886067\n","Running Train loss: 0.6380966062269886\n","Running Train loss: 0.6377834679239057\n","Running Train loss: 0.6374325444215331\n","Running Train loss: 0.6370272173848673\n","Running Train loss: 0.6365383819784334\n","Running Train loss: 0.63622452619126\n","Running Train loss: 0.636057332714962\n","Running Train loss: 0.6359999389543554\n","Running Train loss: 0.6359548941318515\n","Running Train loss: 0.6357778208817083\n","Running Train loss: 0.6356167703905232\n","Running Train loss: 0.6352693551786441\n","Running Train loss: 0.6350638866488241\n","Running Train loss: 0.634866558371651\n","Running Train loss: 0.6347728825838594\n","Running Train loss: 0.6345949080301876\n","Running Train loss: 0.6344673754218998\n","Running Train loss: 0.634390300265018\n","Running Train loss: 0.6345339430445412\n","Running Train loss: 0.6350248964755031\n","Running Train loss: 0.6349986826839694\n","Running Train loss: 0.6350629173276079\n","Running Train loss: 0.6348080204010514\n","Running Train loss: 0.6345178669193959\n","Running Train loss: 0.6345898383525523\n","Running Train loss: 0.6344955469893382\n","Running Train loss: 0.6344144895131624\n","Running Train loss: 0.6342582380281219\n","Running Train loss: 0.6340267316298481\n","Running Train loss: 0.6338610538950766\n","Running Train loss: 0.6336215508267634\n","Running Train loss: 0.6333893641353255\n","Running Train loss: 0.633205186725915\n","Running Train loss: 0.6334015975333281\n","Running Train loss: 0.6333654779478307\n","Running Train loss: 0.6332162659418553\n","Running Train loss: 0.6334740036608686\n","Running Train loss: 0.6335027010183952\n","Running Train loss: 0.6332688811842514\n","Running Train loss: 0.633329425321436\n","Running Train loss: 0.6335930631049785\n","Running Train loss: 0.6338020440647759\n","Running Train loss: 0.6339583545736969\n","Running Train loss: 0.6336706766888164\n","Running Train loss: 0.6334497654460334\n","Running Train loss: 0.6338449508038443\n","Running Train loss: 0.6338623277533589\n","Running Train loss: 0.6338296986281624\n","Running Train loss: 0.6337034125344374\n","Running Train loss: 0.6337594637136471\n","Running Train loss: 0.6334737966897965\n","Running Train loss: 0.6332913012335476\n","Running Train loss: 0.6331456160803964\n","Running Train loss: 0.6329307024152588\n","Running Train loss: 0.6327812860815143\n","Running Train loss: 0.6325492316462339\n","Running Train loss: 0.6323536180107068\n","Running Train loss: 0.6321073191953294\n","Running Train loss: 0.632125605222475\n","Running Train loss: 0.6319496106653304\n","Running Train loss: 0.6317162849009037\n","Running Train loss: 0.6316254646062753\n","Running Train loss: 0.6316197981660971\n","Running Train loss: 0.6314981698953256\n","Running Train loss: 0.6311386903464404\n","Running Train loss: 0.6310383545711502\n","Running Train loss: 0.6309127894601498\n","Running Train loss: 0.6308978339908075\n","Running Train loss: 0.6307234755651888\n","Running Train loss: 0.6305437394840598\n","Running Train loss: 0.63043600821364\n","Running Train loss: 0.6302152398558435\n","Running Train loss: 0.629982935201104\n","Running Train loss: 0.6297956297944272\n","Running Train loss: 0.629425852638125\n","Running Train loss: 0.6290587356067825\n","Running Train loss: 0.6287946867510508\n","Running Train loss: 0.6286294271649137\n","Running Train loss: 0.6285939979574924\n","Running Train loss: 0.6282531260636995\n","Running Train loss: 0.6282384429361689\n","Running Train loss: 0.6282924035138618\n","Running Train loss: 0.628178183091504\n","Running Train loss: 0.6280886098910589\n","Running Train loss: 0.6279762334305787\n","Running Train loss: 0.6279906621225304\n","Running Train loss: 0.6277200126918662\n","Running Train loss: 0.6275780480369986\n","Running Train loss: 0.6275037456584877\n","Running Train loss: 0.6272193628222157\n","Running Train loss: 0.626991960966291\n","Running Train loss: 0.6269865006960039\n","Running Train loss: 0.6268572303831578\n","Running Train loss: 0.6269109876190635\n","Running Train loss: 0.6266263789131333\n","Running Train loss: 0.6264583208053377\n","Running Train loss: 0.6263118307187892\n","Running Train loss: 0.6261759673278645\n","Running Train loss: 0.6261520844937605\n","Running Train loss: 0.6260200560199617\n","Running Train loss: 0.6260666018603621\n","Running Train loss: 0.6260791675700281\n","Running Train loss: 0.6259193145152595\n","Running Train loss: 0.6256675448484311\n","Running Train loss: 0.625433584501031\n","Running Train loss: 0.625356643430458\n","Running Train loss: 0.6251696941466366\n","Running Train loss: 0.6247965097368471\n","Running Train loss: 0.6246977257726223\n","Running Train loss: 0.6243967696695037\n","Running Train loss: 0.6243073477788772\n","Running Train loss: 0.6241591314486484\n","Running Train loss: 0.6239329526332889\n","Running Train loss: 0.6237236405865683\n","Running Train loss: 0.6235989239476856\n","Running Train loss: 0.6233140416121445\n","Running Train loss: 0.6230688681606689\n","Running Train loss: 0.6229675445895569\n","Running Train loss: 0.6228041273357819\n","Running Train loss: 0.6226021282042296\n","Running Train loss: 0.6227049027308705\n","Running Train loss: 0.6225755953632639\n","Running Train loss: 0.6223050551547203\n","Running Train loss: 0.6221792338522126\n","Running Train loss: 0.6219954230356235\n","Running Train loss: 0.621735177608969\n","Running Train loss: 0.6215863086085918\n","Running Train loss: 0.6213634424462393\n","Running Train loss: 0.6212870079392501\n","Running Train loss: 0.6212650075114876\n","Running Train loss: 0.6210293498943034\n","Running Train loss: 0.6207504511584692\n","Running Train loss: 0.6205825351525185\n","Running Train loss: 0.6203542555156464\n","Running Train loss: 0.6203374819593626\n","Running Train loss: 0.62008400438303\n","Running Train loss: 0.6200548269752834\n","Running Train loss: 0.6198676948669334\n","Running Train loss: 0.6196524482507857\n","Running Train loss: 0.6194522604001607\n","Running Train loss: 0.6195152645129911\n","Running Train loss: 0.6193915879535987\n","Running Train loss: 0.6195856396567363\n","Running Train loss: 0.6194743401643682\n","Running Train loss: 0.6193266536885967\n","Running Train loss: 0.6191419977880586\n","Running Train loss: 0.6189717216462902\n","Running Train loss: 0.6190474857144429\n","Running Train loss: 0.6187128514952809\n","Running Train loss: 0.6187656180621014\n","Running Train loss: 0.6188161366701764\n","Running Train loss: 0.618670219559766\n","Running Train loss: 0.6186855849589555\n","Running Train loss: 0.618648869921059\n","Running Train loss: 0.6184957618427622\n","Running Train loss: 0.6183902258727265\n","Running Train loss: 0.6181832667116978\n","Running Train loss: 0.6184003257422847\n","Running Train loss: 0.618458986910779\n","Running Train loss: 0.6184908247516747\n","Running Train loss: 0.6182434169437074\n","Running Train loss: 0.6180959363737009\n","Running Train loss: 0.6179025925413677\n","Running Train loss: 0.6178505176975816\n","Running Train loss: 0.6178759155617449\n","Running Train loss: 0.617574837669809\n","Running Train loss: 0.6175818685667303\n","Running Train loss: 0.6176700577364778\n","Running Train loss: 0.6176271953452857\n","Running Train loss: 0.6175272350831865\n","Running Train loss: 0.6174980926394732\n","Running Train loss: 0.6173817539984252\n","Running Train loss: 0.6173336154480178\n","Running Train loss: 0.6171412074044179\n","Running Train loss: 0.616896389074646\n","Running Train loss: 0.616713830758241\n","Running Train loss: 0.6164683613540798\n","Running Train loss: 0.616377207106642\n","Running Train loss: 0.6162719593333181\n","Running Train loss: 0.6161914924932845\n","Running Train loss: 0.6159820330951513\n","Running Train loss: 0.6157164686234964\n","Running Train loss: 0.6157273132798832\n","Running Train loss: 0.6157071749491535\n","Running Train loss: 0.6153797910960971\n","Running Train loss: 0.6153975295947278\n","Running Train loss: 0.6153501591013212\n","Running Train loss: 0.6153456464520618\n","Running Train loss: 0.6152550176859877\n","Running Train loss: 0.6150754456032889\n","Running Train loss: 0.6148355387825551\n","Running Train loss: 0.6145527033374166\n","Running Train loss: 0.6141955394491001\n","Running Train loss: 0.6139598263194612\n","Running Train loss: 0.6140181516853751\n","Running Train loss: 0.6140162336247018\n","Running Train loss: 0.6140284877947285\n","Running Train loss: 0.6137221486641032\n","Running Train loss: 0.6135631875137751\n","Running Train loss: 0.6133205760695869\n","Running Train loss: 0.6131957921532801\n","Running Train loss: 0.6129817557223058\n","Running Train loss: 0.6130562939900247\n","Running Train loss: 0.6131077306146661\n","Running Train loss: 0.6130283902756181\n","Running Train loss: 0.6128621951554597\n","Running Train loss: 0.6125886353262621\n","Running Train loss: 0.6127528838567682\n","Running Train loss: 0.6126153997880561\n","Running Train loss: 0.6124360785215286\n","Running Train loss: 0.6121790409251525\n","Running Train loss: 0.6119911309475878\n","Running Train loss: 0.6117125268454534\n","Running Train loss: 0.6115977778965587\n","Running Train loss: 0.6113588474419391\n","Running Train loss: 0.6113580854993533\n","Running Train loss: 0.6113044895296794\n","Running Train loss: 0.6111822808710011\n","Running Train loss: 0.6111873531075152\n","Running Train loss: 0.6109620865404649\n","Running Train loss: 0.6108058053291847\n","Running Train loss: 0.6104974954553148\n","Running Train loss: 0.6102850753058126\n","Running Train loss: 0.6100413857498608\n","Running Train loss: 0.6098645199386756\n","Running Train loss: 0.6097733332672501\n","Running Train loss: 0.6096433515612014\n","Running Train loss: 0.6094304448161745\n","Running Train loss: 0.6093130532227468\n","Running Train loss: 0.6091050549417669\n","Running Train loss: 0.6091036444384959\n","Running Train loss: 0.6089708416248673\n","Running Train loss: 0.6087531041916755\n","Running Train loss: 0.6085379977853156\n","Running Train loss: 0.6084223330053019\n","Running Train loss: 0.6082837629718312\n","Running Train loss: 0.6080014955311202\n","Running Train loss: 0.6078971512161703\n","Running Train loss: 0.6076221252847623\n","Running Train loss: 0.6074471254443047\n","Running Train loss: 0.6073474430443135\n","Running Train loss: 0.6072450141988966\n","Running Train loss: 0.6071214090340904\n","Running Train loss: 0.6070045893327157\n","Running Train loss: 0.6069817383017673\n","Running Train loss: 0.6067882120789839\n","Running Train loss: 0.6066648991369878\n","Running Train loss: 0.6067677965208729\n","Running Train loss: 0.6067811327683044\n","Running Train loss: 0.6066109982094785\n","Running Train loss: 0.606288824129892\n","Running Train loss: 0.6061036563124785\n","Running Train loss: 0.6058909410122015\n","Running Train loss: 0.6057597891995875\n","Running Train loss: 0.6056933824141792\n","Running Train loss: 0.6055543194541422\n","Running Train loss: 0.6052658693045588\n","Running Train loss: 0.6050871780872766\n","Running Train loss: 0.6050701391446472\n","Running Train loss: 0.6048070535631607\n","Running Train loss: 0.6045482653247032\n","Running Train loss: 0.6042934949283368\n","Running Train loss: 0.6040461776909274\n","Running Train loss: 0.6038870212637061\n","Running Train loss: 0.6037681915662376\n","Running Train loss: 0.6034893173768917\n","Running Train loss: 0.6033329670212828\n","Running Train loss: 0.6030019162778268\n","Running Train loss: 0.602838681355056\n","Running Train loss: 0.602609310975297\n","Running Train loss: 0.6023749484724942\n","Running Train loss: 0.602115269904165\n","Running Train loss: 0.6018524709958714\n","Running Train loss: 0.6017407721245647\n","Running Train loss: 0.6019178896195478\n","Running Train loss: 0.6018217069623941\n","Running Train loss: 0.6017417941434696\n","Running Train loss: 0.6016736275279564\n","Running Train loss: 0.6015253428598051\n","Running Train loss: 0.6014501800710075\n","Running Train loss: 0.6014179608762595\n","Running Train loss: 0.6013628642299342\n","Running Train loss: 0.6012435996335828\n","Running Train loss: 0.600967441935617\n","Running Train loss: 0.6007371819765409\n","Running Train loss: 0.6004545297822189\n","Running Train loss: 0.6001727444034617\n","Running Train loss: 0.5998565849187465\n","Running Train loss: 0.5995902922442428\n","Running Train loss: 0.5992873271961994\n","Running Train loss: 0.5990623465240084\n","Running Train loss: 0.5988212191673951\n","Running Train loss: 0.5986445561411052\n","Running Train loss: 0.5984686315932576\n","Running Train loss: 0.5982203394656914\n","Running Train loss: 0.5979043991347879\n","Running Train loss: 0.5978036453432444\n","Running Train loss: 0.59766783741434\n","Running Train loss: 0.5975139781801992\n","Running Train loss: 0.5973185187559792\n","Running Train loss: 0.5971315365197809\n","Running Train loss: 0.5970240759933297\n","Running Train loss: 0.596711157216397\n","Running Train loss: 0.5965979888647652\n","Running Train loss: 0.5963286266250666\n","Running Train loss: 0.5963405754954458\n","Running Train loss: 0.5961247282204488\n","Running Train loss: 0.5960317523050227\n","Running Train loss: 0.5957358496627004\n","Running Train loss: 0.5955817047366904\n","Running Train loss: 0.5954887322543508\n","Running Train loss: 0.5953898949686811\n","Running Train loss: 0.5951394001896284\n","Running Train loss: 0.5949262310956547\n","Running Train loss: 0.594763777993174\n","Running Train loss: 0.5946634691069503\n","Running Train loss: 0.5945398227906858\n","Running Train loss: 0.5944274245732921\n","Running Train loss: 0.5941421831920137\n","Running Train loss: 0.5938263555626734\n","Running Train loss: 0.5937970957702406\n","Running Train loss: 0.5935378007898063\n","Running Train loss: 0.593360471156602\n","Running Train loss: 0.5932555769109227\n","Running Train loss: 0.5930227305031257\n","Running Train loss: 0.5928732607254931\n","Running Train loss: 0.592851625414028\n","Running Train loss: 0.5926509196659933\n","Running Train loss: 0.5924794568177588\n","Running Train loss: 0.5923336855490976\n","Running Train loss: 0.5921051400633509\n","Running Train loss: 0.5918606713194027\n","Running Train loss: 0.5916286535461077\n","Running Train loss: 0.591477888026028\n","Running Train loss: 0.5913217537674363\n","Running Train loss: 0.591132740660434\n","Running Train loss: 0.5909082067250168\n","Running Train loss: 0.5905754339136806\n","Running Train loss: 0.5904512981321404\n","Running Train loss: 0.5903455078721286\n","Running Train loss: 0.5901242901231641\n","Running Train loss: 0.5899762071276443\n","Running Train loss: 0.5897639332016309\n","Running Train loss: 0.5894588184587007\n","Running Train loss: 0.5892939583637743\n","Running Train loss: 0.5889785248124433\n","Running Train loss: 0.5889103232902733\n","Running Train loss: 0.5888606156423638\n","Running Train loss: 0.588683532785926\n","Running Train loss: 0.5886437036813927\n","Running Train loss: 0.5886134299541026\n","Running Train loss: 0.5884378444800873\n","Running Train loss: 0.5882842354032377\n","Running Train loss: 0.5880916035388338\n","Running Train loss: 0.5880029183176775\n","Running Train loss: 0.5878904962965139\n","Running Train loss: 0.5877484181598124\n","Running Train loss: 0.5876782968886222\n","Running Train loss: 0.587539800238798\n","Running Train loss: 0.5875853735833241\n","Running Train loss: 0.5874705144028732\n","Running Train loss: 0.5875033166669088\n","Running Train loss: 0.5873856697427599\n","Running Train loss: 0.587330141481328\n","Running Train loss: 0.5872303792783371\n","Running Train loss: 0.586966047534329\n","Running Train loss: 0.5867439598979287\n","Running Train loss: 0.5865860098502675\n","Running Train loss: 0.5862393701427066\n","Running Train loss: 0.5860054892729956\n","Running Train loss: 0.585891581337938\n","Running Train loss: 0.5857746290485207\n","Running Train loss: 0.5857135028304422\n","Running Train loss: 0.5854839620166437\n","Running Train loss: 0.5852501237103546\n","Running Train loss: 0.5851113792904942\n","Running Train loss: 0.5850721434259003\n","Running Train loss: 0.5850657548670466\n","Running Train loss: 0.5851000538265604\n","Running Train loss: 0.5851165927006156\n","Running Train loss: 0.5850837444002475\n","Running Train loss: 0.5848730611381902\n","Running Train loss: 0.5846983351853567\n","Running Train loss: 0.5846904381205515\n","Running Train loss: 0.5845924731025088\n","Running Train loss: 0.5845120263709044\n","Running Train loss: 0.5845458735663618\n","Running Train loss: 0.5844173140293379\n","Running Train loss: 0.5842404215925531\n","Running Train loss: 0.5841000857796104\n","Running Train loss: 0.5840077638332504\n","Running Train loss: 0.5838321609539205\n","Running Train loss: 0.5838061532065753\n","Running Train loss: 0.5836237007205637\n","Running Train loss: 0.5834817802797542\n","Running Train loss: 0.5833589889406582\n","Running Train loss: 0.5831652821965775\n","Running Train loss: 0.5830429308451832\n","Running Train loss: 0.5829480501456823\n","Running Train loss: 0.5828674028082658\n","Running Train loss: 0.582633190782351\n","Running Train loss: 0.5824810583419223\n","Running Train loss: 0.5822828188896753\n","Running Train loss: 0.5821005681948468\n","Running Train loss: 0.581971747791168\n","Running Train loss: 0.5816880450324797\n","Running Train loss: 0.5814864529020456\n","Running Train loss: 0.5812426883453569\n","Running Train loss: 0.5810037139365462\n","Running Train loss: 0.5808316996557628\n","Running Train loss: 0.580592373459676\n","Running Train loss: 0.5805179832670588\n","Running Train loss: 0.5803515454172898\n","Running Train loss: 0.5800978587258591\n","Running Train loss: 0.5799645497499191\n","Running Train loss: 0.5798209405611331\n","Running Train loss: 0.5796699941210364\n","Running Train loss: 0.5796976272619906\n","Running Train loss: 0.5797727468782837\n","Running Train loss: 0.5798095022089019\n","Running Train loss: 0.5799622187421614\n","Running Train loss: 0.5798005261225478\n","Running Train loss: 0.5797295167264116\n","Running Train loss: 0.5795718990549078\n","Running Train loss: 0.5794049289275324\n","Running Train loss: 0.5792612219164625\n","Running Train loss: 0.5789813545198566\n","Running Train loss: 0.5788751898198286\n","Running Train loss: 0.5789090433001781\n","Running Train loss: 0.578613077513944\n","Running Train loss: 0.5784650371393532\n","Running Train loss: 0.578251929598514\n","Running Train loss: 0.5780633366590588\n","Running Train loss: 0.5778548493536481\n","Running Train loss: 0.5777361150807584\n","Running Train loss: 0.577577905585593\n","Running Train loss: 0.5774646766058437\n","Running Train loss: 0.5773483265600048\n","Running Train loss: 0.577165629525662\n","Running Train loss: 0.5770112579955522\n","Running Train loss: 0.5768280664577763\n","Running Train loss: 0.5768345505013139\n","Running Train loss: 0.5767562295566313\n","Running Train loss: 0.576644221601134\n","Running Train loss: 0.5765978412379449\n","Running Train loss: 0.5764984798292889\n","Running Train loss: 0.5763911052381717\n","Running Train loss: 0.5763057870726533\n","Running Train loss: 0.57615326178651\n","Running Train loss: 0.5760789952810573\n","Running Train loss: 0.5758381880129414\n","Running Train loss: 0.5757351073119824\n","Running Train loss: 0.5755003165823888\n","Running Train loss: 0.5754383585486887\n","Running Train loss: 0.5752099374235231\n","Running Train loss: 0.5752047105297973\n","Running Train loss: 0.5749944857906786\n","Running Train loss: 0.5748145275694483\n","Running Train loss: 0.5746782579998679\n","Running Train loss: 0.5746658306244021\n","Running Train loss: 0.5744853248366135\n","Running Train loss: 0.5743539234562337\n","Running Train loss: 0.5742600254323563\n","Running Train loss: 0.5740864265923843\n","Running Train loss: 0.5740909154254727\n","Running Train loss: 0.5739552636716333\n","Running Train loss: 0.5740288252566065\n","Running Train loss: 0.5740364690079138\n","Running Train loss: 0.5739195125857952\n","Running Train loss: 0.5738685535771867\n","Running Train loss: 0.5736921202157983\n","Running Train loss: 0.5735511954020802\n","Running Train loss: 0.5733060172981829\n","Running Train loss: 0.5731596600887857\n","Running Train loss: 0.5729891143296388\n","Running Train loss: 0.5728414554607445\n","Running Train loss: 0.5725911980428741\n","Running Train loss: 0.5723819136049951\n","Running Train loss: 0.5721583562108408\n","Running Train loss: 0.5720216959473667\n","Running Train loss: 0.5720851774212605\n","Running Train loss: 0.571993252181785\n","Running Train loss: 0.5717746520619386\n","Running Train loss: 0.5716891511298584\n","Running Train loss: 0.5714775498266741\n","Running Train loss: 0.5713177711129805\n","Running Train loss: 0.5712198595835889\n","Running Train loss: 0.5710998369907354\n","Running Train loss: 0.570947798095725\n","Running Train loss: 0.5707943380290658\n","Running Train loss: 0.5706978146036527\n","Running Train loss: 0.5705323014148551\n","Running Train loss: 0.5705846755951643\n","Running Train loss: 0.5707423823966286\n","Running Train loss: 0.5705444400264154\n","Running Train loss: 0.5703446194191485\n","Running Train loss: 0.5702245618850713\n","Running Train loss: 0.5701313600776959\n","Running Train loss: 0.57009259654335\n","Running Train loss: 0.5699302376229395\n","Running Train loss: 0.569830240578412\n","Running Train loss: 0.5702019961120818\n","Running Train loss: 0.5700166418228343\n","Running Train loss: 0.5698760208000374\n","Running Train loss: 0.5697730892056556\n","Running Train loss: 0.5695580362319337\n","Running Train loss: 0.5694398944637774\n","Running Train loss: 0.5693584182039575\n","Running Train loss: 0.569245651540976\n","Running Train loss: 0.5692280873982543\n","Running Train loss: 0.5690765765337028\n","Running Train loss: 0.5689417641786239\n","Running Train loss: 0.5688268164473915\n","Running Train loss: 0.5687041543914593\n","Running Train loss: 0.5687123756102648\n","Running Train loss: 0.5685415990623933\n","Running Train loss: 0.5684347487741296\n","Running Train loss: 0.5682521212078742\n","Running Train loss: 0.5683556492273698\n","Running Train loss: 0.5682606326885962\n","Running Train loss: 0.5680377651005983\n","Running Train loss: 0.5678313387543442\n","Running Train loss: 0.5676352991389909\n","Running Train loss: 0.5676076902738112\n","Running Train loss: 0.5677843792840183\n","Running Train loss: 0.5676848338723218\n","Running Train loss: 0.5675202193968617\n","Running Train loss: 0.5672889713341652\n","Running Train loss: 0.567160752158908\n","Running Train loss: 0.5672138997005265\n","Running Train loss: 0.5669577584107588\n","Running Train loss: 0.5667129920243053\n","Running Train loss: 0.5665113415128205\n","Running Train loss: 0.5663533668341649\n","Running Train loss: 0.5660929609041653\n","Running Train loss: 0.5658704660551229\n","Running Train loss: 0.5657812658732553\n","Running Train loss: 0.5656816542214524\n","Running Train loss: 0.5655057296960808\n","Running Train loss: 0.5653073841406271\n","Running Train loss: 0.565149582665769\n","Running Train loss: 0.5649374131400526\n","Running Train loss: 0.5647751605664106\n","Running Train loss: 0.5646126797463282\n","Running Train loss: 0.5644614971915892\n","Running Train loss: 0.564441662058628\n","Running Train loss: 0.5642955005671741\n","Running Train loss: 0.564177821473904\n","Running Train loss: 0.5640092743986481\n","Running Train loss: 0.5639456977198085\n","Running Train loss: 0.5638794822714333\n","Running Train loss: 0.5637788843884497\n","Running Train loss: 0.5636607761772578\n","Running Train loss: 0.5634747566181725\n","Running Train loss: 0.563444429597269\n","Running Train loss: 0.5632578201235315\n","Running Train loss: 0.5630864505585427\n","Running Train loss: 0.5628966801259504\n","Running Train loss: 0.5627368419628703\n","Running Train loss: 0.5630847627951193\n","Running Train loss: 0.5633920730092715\n","Running Train loss: 0.5633687925757433\n","Running Train loss: 0.5631886343861562\n","Running Train loss: 0.5630687770868269\n","Running Train loss: 0.5632284389101359\n","Running Train loss: 0.5634858429289217\n","Running Train loss: 0.5633810081534313\n","Running Train loss: 0.5632388207251611\n","Running Train loss: 0.5630520854905023\n","Running Train loss: 0.5629835826537308\n","Running Train loss: 0.5628208693147085\n","Running Train loss: 0.5626040617442496\n","Running Train loss: 0.5624108554203214\n","Running Train loss: 0.5621931247352899\n","Running Train loss: 0.5620587016703178\n","Running Train loss: 0.5619143624970043\n","Running Train loss: 0.5617608327693558\n","Running Train loss: 0.561564176439466\n","Running Train loss: 0.5613463261585322\n","Running Train loss: 0.5612397705519515\n","Running Train loss: 0.561045910805138\n","Running Train loss: 0.5609479318937911\n","Running Train loss: 0.5607348532233943\n","Running Train loss: 0.5605965892344046\n","Running Train loss: 0.5604577690906113\n","Running Train loss: 0.5606466412642566\n","Running Train loss: 0.5605633418319251\n","Running Train loss: 0.5603092143840469\n","Running Train loss: 0.5600792316384913\n","Running Train loss: 0.5598816499263304\n","Running Train loss: 0.5597236755429777\n","Running Train loss: 0.5595212867957412\n","Running Train loss: 0.5593437086279903\n","Running Train loss: 0.5593930576241303\n","Running Train loss: 0.5592352784936184\n","Running Train loss: 0.5591116065209379\n","Running Train loss: 0.5590113952879113\n","Running Train loss: 0.5588336216354811\n","Running Train loss: 0.5587866172504214\n","Running Train loss: 0.5587491288709078\n","Running Train loss: 0.5586845289356371\n","Running Train loss: 0.5585567253266396\n","Running Train loss: 0.5584901207486507\n","Running Train loss: 0.558356742582441\n","Running Train loss: 0.558302167775564\n","Running Train loss: 0.5581097032513492\n","Running Train loss: 0.5578855407654462\n","Running Train loss: 0.5576211188505628\n","Running Train loss: 0.5574035210089836\n","Running Train loss: 0.5573372787015378\n","Running Train loss: 0.5572335416076039\n","Running Train loss: 0.5570611998990574\n","Running Train loss: 0.5568928972244431\n","Running Train loss: 0.5567307155631916\n","Running Train loss: 0.5565288917226696\n","Running Train loss: 0.5563246238676256\n","Running Train loss: 0.5561468759163156\n","Running Train loss: 0.5560054497672638\n","Running Train loss: 0.5558256054955728\n","Running Train loss: 0.5557559781388648\n","Running Train loss: 0.5556386017394582\n","Running Train loss: 0.5554987564662021\n","Running Train loss: 0.5553638066238399\n","Running Train loss: 0.5551893857277925\n","Running Train loss: 0.5549700933745848\n","Running Train loss: 0.5547377197735275\n","Running Train loss: 0.5545704816676876\n","Running Train loss: 0.5544724751441251\n","Running Train loss: 0.5542420756847735\n","Running Train loss: 0.5540648714504265\n","Running Train loss: 0.5539993557774814\n","Running Train loss: 0.5538345189872336\n","Running Train loss: 0.5536246643573379\n","Running Train loss: 0.5534169822619289\n","Running Train loss: 0.5531898287112459\n","Running Train loss: 0.5530704357627545\n","Running Train loss: 0.5529797025203472\n","Running Train loss: 0.5528540465845869\n","Running Train loss: 0.5526487516521379\n","Running Train loss: 0.5525354920478582\n","Running Train loss: 0.5523426253887285\n","Running Train loss: 0.5521911463621029\n","Running Train loss: 0.5520845322579974\n","Running Train loss: 0.5520167194828102\n","Running Train loss: 0.5519138244430345\n","Running Train loss: 0.5516787458911613\n","Running Train loss: 0.5517214503895054\n","Running Train loss: 0.5515069547142843\n","Running Train loss: 0.551273388895366\n","Running Train loss: 0.5511693830875547\n","Running Train loss: 0.5511658775302853\n","Running Train loss: 0.550931787323217\n","Running Train loss: 0.5507548243404258\n","Running Train loss: 0.5505289298079574\n","Running Train loss: 0.5502751294798968\n","Running Train loss: 0.5501624375881737\n","Running Train loss: 0.5500296498232962\n","Running Train loss: 0.5498111259941392\n","Running Train loss: 0.5496357563854347\n","Running Train loss: 0.5495955500901402\n","Running Train loss: 0.5494829074831062\n","Running Train loss: 0.5493201413563052\n","Running Train loss: 0.5490725747524545\n","Running Train loss: 0.5488812494665007\n","Running Train loss: 0.5486922471361843\n","Running Train loss: 0.5485559822914859\n","Running Train loss: 0.5484856419253016\n","Running Train loss: 0.5483825216901629\n","Running Train loss: 0.5484202110513572\n","Running Train loss: 0.5482830955301828\n","Running Train loss: 0.5481066816553669\n","Running Train loss: 0.5479477058776322\n","Running Train loss: 0.5477375077032814\n","Running Train loss: 0.5476459044635557\n","Running Train loss: 0.5475574528420238\n","Running Train loss: 0.547523794519937\n","Running Train loss: 0.5473778480308429\n","Running Train loss: 0.5472996614710999\n","Running Train loss: 0.5471106039498139\n","Running Train loss: 0.546980800144638\n","Running Train loss: 0.5467927029155706\n","Running Train loss: 0.5465995681539477\n","Running Train loss: 0.5464994109053488\n","Running Train loss: 0.5463435191930247\n","Running Train loss: 0.5461824813495663\n","Running Train loss: 0.5461765677646425\n","Running Train loss: 0.5460810100522353\n","Running Train loss: 0.5458407814929963\n","Running Train loss: 0.5456659112827911\n","Running Train loss: 0.545560161013844\n","Running Train loss: 0.545543693886965\n","Running Train loss: 0.5453932785392291\n","Running Train loss: 0.5451795482413995\n","Running Train loss: 0.54507162604483\n","Running Train loss: 0.5449926836705781\n","Running Train loss: 0.5449014910096197\n","Running Train loss: 0.5447601166844143\n","Running Train loss: 0.544708772387225\n","Running Train loss: 0.5445454993530112\n","Running Train loss: 0.5443880306872778\n","Running Train loss: 0.5441963793519399\n","Running Train loss: 0.5440522656225434\n","Running Train loss: 0.5439814372348689\n","Running Train loss: 0.5437871661739827\n","Running Train loss: 0.5435999903675853\n","Running Train loss: 0.5436119344775323\n","Running Train loss: 0.5434539546978224\n","Running Train loss: 0.5432330105423767\n","Running Train loss: 0.5430271343247204\n","Running Train loss: 0.5428436416484133\n","Running Train loss: 0.542704558676025\n","Running Train loss: 0.5425537089797792\n","Running Train loss: 0.5424028488025467\n","Running Train loss: 0.5422048888511195\n","Running Train loss: 0.5420421777297066\n","Running Train loss: 0.5418733981578698\n","Running Train loss: 0.5417068550977439\n","Running Train loss: 0.5416674847384294\n","Running Train loss: 0.5415101133640419\n","Running Train loss: 0.5414547252433536\n","Running Train loss: 0.5412793429201435\n","Running Train loss: 0.5410468677884566\n","Running Train loss: 0.5409461956015451\n","Running Train loss: 0.5410031358721819\n","Running Train loss: 0.5408739517210299\n","Running Train loss: 0.5406635314784054\n","Running Train loss: 0.5405594811986365\n","Running Train loss: 0.540406539611301\n","Running Train loss: 0.5401524225800977\n","Running Train loss: 0.5401130895044953\n","Running Train loss: 0.5399607725666394\n","Running Train loss: 0.5397734023993485\n","Running Train loss: 0.5395721892377845\n","Running Train loss: 0.5394807725707448\n","Running Train loss: 0.5393996196670997\n","Running Train loss: 0.5392938216609956\n","Running Train loss: 0.5392568194048741\n","Running Train loss: 0.5391595515243139\n","Running Train loss: 0.5390430265263446\n","Running Train loss: 0.538948122254864\n","Running Train loss: 0.5387732736291354\n","Running Train loss: 0.5386961006616812\n","Running Train loss: 0.5385213167592883\n","Running Train loss: 0.5383374340565721\n","Running Train loss: 0.5382771059796293\n","Running Train loss: 0.5381565605100838\n","Running Train loss: 0.5380302287203831\n","Running Train loss: 0.5380687805239767\n","Running Train loss: 0.5380654589205605\n","Running Train loss: 0.5380098808734562\n","Running Train loss: 0.5378457765803678\n","Running Train loss: 0.5377517104355717\n","Running Train loss: 0.5375485136968459\n","Running Train loss: 0.5373802256035998\n","Running Train loss: 0.5372378697100575\n","Running Train loss: 0.5371850253386112\n","Running Train loss: 0.5370256308030217\n","Running Train loss: 0.5368826317277496\n","Running Train loss: 0.536723643737711\n","Running Train loss: 0.5365765909793075\n","Running Train loss: 0.536380055722731\n","Running Train loss: 0.536162324115797\n","Running Train loss: 0.5359973241904906\n","Running Train loss: 0.5359132093964518\n","Running Train loss: 0.5357890322467198\n","Running Train loss: 0.5355899574112067\n","Running Train loss: 0.5355013904883962\n","Running Train loss: 0.5352926277978853\n","Running Train loss: 0.5351822709573795\n","Running Train loss: 0.5350295866365469\n","Running Train loss: 0.534846667673717\n","Running Train loss: 0.5346212550677154\n","Running Train loss: 0.5344790878671902\n","Running Train loss: 0.5343081250435666\n","Running Train loss: 0.5342906064047299\n","Running Train loss: 0.5342033648342125\n","Running Train loss: 0.5340089639157611\n","Running Train loss: 0.5339580689193513\n","Running Train loss: 0.5342640870516304\n","Running Train loss: 0.5347745444916023\n","Running Train loss: 0.5347816449473732\n","Running Train loss: 0.5347477053534653\n","Running Train loss: 0.534605454497961\n","Running Train loss: 0.5345175254807867\n","Running Train loss: 0.5343457510719799\n","Running Train loss: 0.534125633183071\n","Running Train loss: 0.5340595640215271\n","Running Train loss: 0.5339498583428505\n","Running Train loss: 0.5337896096823548\n","Running Train loss: 0.5337049377002071\n","Running Train loss: 0.5336499965522109\n","Running Train loss: 0.5334686116205998\n","Running Train loss: 0.5332639778137971\n","Running Train loss: 0.5331614278562401\n","Running Train loss: 0.5330359395459814\n","Running Train loss: 0.5329505191897498\n","Running Train loss: 0.5328520663490589\n","Running Train loss: 0.5327657004623004\n","Running Train loss: 0.5326167692683622\n","Running Train loss: 0.5325184329929221\n","Running Train loss: 0.5323475049000016\n","Running Train loss: 0.5321739401350738\n","Running Train loss: 0.5320387865530745\n","Running Train loss: 0.5318689923230056\n","Running Train loss: 0.5317194877433515\n","Running Train loss: 0.5316208888716347\n","Running Train loss: 0.5314523931649606\n","Running Train loss: 0.531308731643374\n","Running Train loss: 0.5312194537128855\n","Running Train loss: 0.5311378826196417\n","Running Train loss: 0.53104562983409\n","Running Train loss: 0.5309387730172499\n","Running Train loss: 0.5307460584332646\n","Running Train loss: 0.5305650592368155\n","Running Train loss: 0.5304204425500596\n","Running Train loss: 0.5302903352827363\n","Running Train loss: 0.530152118286418\n","Running Train loss: 0.5299730496485776\n","Running Train loss: 0.5298185687274616\n","Running Train loss: 0.5296053036455711\n","Running Train loss: 0.5295177129604849\n","Running Train loss: 0.5293089212730203\n","Running Train loss: 0.5290990704745807\n","Running Train loss: 0.5289341595074645\n","Running Train loss: 0.5288855769961892\n","Running Train loss: 0.5286509875101059\n","Running Train loss: 0.5285176102714162\n","Running Train loss: 0.5283664058359053\n","Running Train loss: 0.5281982914509941\n","Running Train loss: 0.5280236088481901\n","Running Train loss: 0.5278115485526876\n","Running Train loss: 0.5276587503269697\n","Running Train loss: 0.527505413796659\n","Running Train loss: 0.5272945620209383\n","Running Train loss: 0.5271226346444144\n","Running Train loss: 0.5270253659262917\n","Running Train loss: 0.5269071821145781\n","Running Train loss: 0.5266814805641957\n","Running Train loss: 0.5264986728360962\n","Running Train loss: 0.5264771448454829\n","Running Train loss: 0.5264299203873845\n","Running Train loss: 0.5263602273811723\n","Running Train loss: 0.5262272546123713\n","Running Train loss: 0.5260487061937799\n","Running Train loss: 0.5259984960457632\n","Running Train loss: 0.5258412374814957\n","Running Train loss: 0.5257590087837059\n","Running Train loss: 0.525638310226642\n","Running Train loss: 0.525488951528521\n","Running Train loss: 0.5253209718997257\n","Running Train loss: 0.5252014832604542\n","Running Train loss: 0.5249926606281264\n","Running Train loss: 0.5248417170468106\n","Running Train loss: 0.5246984758120251\n","Running Train loss: 0.5245843096571432\n","Running Train loss: 0.5244409856393423\n","Running Train loss: 0.5242668019280267\n","Running Train loss: 0.5241580476517564\n","Running Train loss: 0.5241091489655129\n","Running Train loss: 0.524009299944268\n","Running Train loss: 0.5239943103888788\n","Running Train loss: 0.5239921667552662\n","Running Train loss: 0.523816904733603\n","Running Train loss: 0.523684675682231\n","Running Train loss: 0.5235928513688191\n","Running Train loss: 0.5235039473180275\n","Running Train loss: 0.5233344263470779\n","Running Train loss: 0.5232798005382956\n","Running Train loss: 0.523110434514087\n","Running Train loss: 0.5230004208876425\n","Running Train loss: 0.5228953503744578\n","Running Train loss: 0.5227577804457882\n","Running Train loss: 0.5226104364074363\n","Running Train loss: 0.5225396629593485\n","Running Train loss: 0.5224355687478953\n","Running Train loss: 0.522345212199787\n","Running Train loss: 0.5222341474878255\n","Running Train loss: 0.5220586265975047\n","Running Train loss: 0.5220652960427471\n","Running Train loss: 0.5219539183634985\n","Running Train loss: 0.5218547895912655\n","Running Train loss: 0.52186200048093\n","Running Train loss: 0.521724706681847\n","Running Train loss: 0.521614470429233\n","Running Train loss: 0.5214110899599859\n","Running Train loss: 0.5212472035762468\n","Running Train loss: 0.521039105935383\n","Running Train loss: 0.5208726122483501\n"]},{"ename":"ValueError","evalue":"too many dimensions 'str'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[160], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tagger \u001b[39m=\u001b[39m train_fixed_window(train_data)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(accuracy(tagger, dev_data)))\n","Cell \u001b[1;32mIn[111], line 9\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(tagger, gold_data)\u001b[0m\n\u001b[0;32m      7\u001b[0m     tot \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[0;32m      8\u001b[0m     gold_words \u001b[39m=\u001b[39m [i[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m words]\n\u001b[1;32m----> 9\u001b[0m     pred_words \u001b[39m=\u001b[39m tagger\u001b[39m.\u001b[39;49mpredict(words)\n\u001b[0;32m     11\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(gold_words,pred_words) \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m y)\n\u001b[0;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m tot\n","Cell \u001b[1;32mIn[149], line 38\u001b[0m, in \u001b[0;36mFixedWindowTagger.predict\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     36\u001b[0m predicted_tags \u001b[39m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word_ids)):\n\u001b[1;32m---> 38\u001b[0m       curr_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeaturize(words, i, predicted_tags)\n\u001b[0;32m     39\u001b[0m       \u001b[39m#print(\"feature: \", curr_feature.view(1,4))\u001b[39;00m\n\u001b[0;32m     40\u001b[0m       res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(curr_feature\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m))\n","Cell \u001b[1;32mIn[149], line 21\u001b[0m, in \u001b[0;36mFixedWindowTagger.featurize\u001b[1;34m(self, words, i, pred_tags)\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mLongTensor([words[i], words[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m0\u001b[39m, pred_tags[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]])\n\u001b[0;32m     20\u001b[0m \u001b[39melif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m#If current word beginning of list\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m   \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mLongTensor([words[i], \u001b[39m0\u001b[39;49m, words[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m], \u001b[39m0\u001b[39;49m])\n\u001b[0;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# If middle of words\u001b[39;00m\n\u001b[0;32m     23\u001b[0m  \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mLongTensor([words[i], words[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], words[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], pred_tags[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]])\n","\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"]}],"source":["tagger = train_fixed_window(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlIMd01I9rC_","outputId":"5d83c836-286d-4fa7-a13a-de2363694f03"},"outputs":[{"ename":"ValueError","evalue":"too many dimensions 'str'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[175], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(gold_words,pred_words) \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m y)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m tot\n\u001b[1;32m---> 15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(accuracy_convert(tagger, dev_data, words, tags)))\n","Cell \u001b[1;32mIn[175], line 9\u001b[0m, in \u001b[0;36maccuracy_convert\u001b[1;34m(tagger, gold_data, vocab_words, vocab_tags)\u001b[0m\n\u001b[0;32m      7\u001b[0m     tot \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[0;32m      8\u001b[0m     gold_words \u001b[39m=\u001b[39m [vocab_tags[i[\u001b[39m1\u001b[39m]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m words]\n\u001b[1;32m----> 9\u001b[0m     pred_words \u001b[39m=\u001b[39m [vocab_words[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tagger\u001b[39m.\u001b[39;49mpredict(words)]\n\u001b[0;32m     11\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(gold_words,pred_words) \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m y)\n\u001b[0;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m tot\n","Cell \u001b[1;32mIn[149], line 38\u001b[0m, in \u001b[0;36mFixedWindowTagger.predict\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m     36\u001b[0m predicted_tags \u001b[39m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word_ids)):\n\u001b[1;32m---> 38\u001b[0m       curr_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeaturize(words, i, predicted_tags)\n\u001b[0;32m     39\u001b[0m       \u001b[39m#print(\"feature: \", curr_feature.view(1,4))\u001b[39;00m\n\u001b[0;32m     40\u001b[0m       res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(curr_feature\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m))\n","Cell \u001b[1;32mIn[149], line 21\u001b[0m, in \u001b[0;36mFixedWindowTagger.featurize\u001b[1;34m(self, words, i, pred_tags)\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mLongTensor([words[i], words[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], \u001b[39m0\u001b[39m, pred_tags[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]])\n\u001b[0;32m     20\u001b[0m \u001b[39melif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m#If current word beginning of list\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m   \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mLongTensor([words[i], \u001b[39m0\u001b[39;49m, words[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m], \u001b[39m0\u001b[39;49m])\n\u001b[0;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# If middle of words\u001b[39;00m\n\u001b[0;32m     23\u001b[0m  \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mLongTensor([words[i], words[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], words[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], pred_tags[i \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]])\n","\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"]}],"source":["def accuracy_convert(tagger, gold_data, vocab_words, vocab_tags):\n","    # TODO: Replace the next line with your own code\n","    tot = 0\n","    correct = 0\n","\n","    for words in gold_data:\n","        tot += len(words)\n","        gold_words = [vocab_tags[i[1]] for i in words]\n","        pred_words = [vocab_tags[i] for i in tagger.predict(words)]\n","\n","        correct += sum(1 for x,y in zip(gold_words,pred_words) if x == y)\n","    \n","    return correct / tot\n","\n","print('{:.4f}'.format(accuracy_convert(tagger, dev_data, words, tags)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7lj8ZDO8tAk"},"outputs":[],"source":["class Dataset():\n","\n","    def __init__(self, filename):\n","        self.filename = filename\n","\n","    def __iter__(self):\n","        with open(self.filename, 'rt', encoding='utf-8') as lines:\n","            tmp = []\n","            for line in lines:\n","                if not line.startswith('#'):  # Skip lines with comments\n","                    line = line.rstrip()\n","                    if line:\n","                        columns = line.split('\\t')\n","                        if columns[0].isdigit():  # Skip range tokens\n","                            tmp.append(columns)\n","                    else:\n","                        yield tmp\n","                        tmp = []\n","\n","with open('en_ewt-ud-dev-retagged.conllu', 'wt') as target:\n","    for sentence in Dataset('en_ewt-ud-dev.conllu'):\n","        words = [columns[1] for columns in sentence]\n","        for i, t in enumerate(tagger.predict(words)):\n","            sentence[i][3] = t\n","        for columns in sentence:\n","            print('\\t'.join(c for c in columns), file=target)\n","        print(file=target)\n","\n","# (columns[1], columns[3], int(columns[6])))"]},{"cell_type":"markdown","metadata":{"id":"vybae2IK8tAk"},"source":["**âš ï¸ Your submitted notebook must contain output demonstrating at least 88% accuracy on the development set.**"]},{"cell_type":"markdown","metadata":{"id":"hoxPACOE8tAk"},"source":["## Problem 5: Pre-trained embeddings (reflection)"]},{"cell_type":"markdown","metadata":{"id":"U7jhDcs48tAk"},"source":["Many neural systems for natural language processing use pre-trained word embeddings, either to augment or to replace randomly initialised task-based embeddings. In this problem, you will investigate whether pre-trained embeddings help your part-of-speech tagger.\n","\n","The file `glove.pt` contains a PyTorch tensor containing 50-dimensional pre-trained word embeddings from the [GloVe project](https://nlp.stanford.edu/projects/glove/). You can load this tensor using the command\n","\n","```\n","glove = torch.load('glove.pt')\n","```\n","\n","and should be able to use it as a drop-in replacement for your randomly initialized word embeddings, assuming that the words in your vocabulary are numbered in the order in which they are found in the training data. Have a look at the documentation of the class [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) to learn how to do this.\n","\n","Run experiments to assess the effect that pre-trained embeddings have on (a)&nbsp;the accuracy of the tagger, and (b)&nbsp;the speed of learning, i.e., the number of training examples it takes to reach a certain loss. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n","\n","* How did you integrate the pre-trained embeddings into your system? What did you measure? What results did you get?\n","* Based on what you know about word embeddings and transfer learning, did you expect your results? How do you explain them?\n","* What did you learn? How, exactly, did you learn it? Why does this learning matter?"]},{"cell_type":"markdown","metadata":{"id":"PWF1JnLC8tAk"},"source":["*TODO: Insert your report here*"]},{"cell_type":"markdown","metadata":{"id":"R8mqRXOp8tAk"},"source":["**ðŸ¥³ Congratulations on finishing this lab! ðŸ¥³**"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"latex_envs":{"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":0},"vscode":{"interpreter":{"hash":"bc143343ca8435bba8c44b3b1f47f9edcb7f00f13cf7dc8cb9f5e5ffbd446b7a"}}},"nbformat":4,"nbformat_minor":0}