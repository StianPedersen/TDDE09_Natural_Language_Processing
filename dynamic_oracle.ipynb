{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). The code below defines an iterable-style dataset for parser data in the [CoNLL-U format](https://universaldependencies.org/format.html) that the project uses to distribute its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = [Dataset.ROOT]\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append((columns[1], columns[3], int(columns[6])))\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = [Dataset.ROOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('en_ewt-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('en_ewt-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<root>', '<root>', 0),\n",
       " ('They', 'PRON', 4),\n",
       " ('are', 'AUX', 4),\n",
       " ('merely', 'ADV', 4),\n",
       " ('imposters', 'NOUN', 0),\n",
       " ('.', 'PUNCT', 4)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = list(train_data)[1000]\n",
    "example_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger evaluation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**accuracy** (*tagger*, *gold_data*)\n",
    "\n",
    "> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tagger, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        \n",
    "        nr_words += len(words)\n",
    "\n",
    "        correct_tags = [tokens[1] for tokens in sentence]\n",
    "        predicted_tags = tagger.predict(words)\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            if predicted_tags[i] == correct_tags[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabularies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make_vocabs** (*gold_data*)\n",
    "\n",
    "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over tagged sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    tags = {PAD: 0}\n",
    "    for sentence in gold_data:\n",
    "        for pair in sentence:\n",
    "            word = pair[0]\n",
    "            tag = pair[1]\n",
    "            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            \n",
    "            if tag not in tags:\n",
    "                tags[tag] = len(tags)\n",
    "                    \n",
    "    return vocab, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19676\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "vocab, tags = make_vocabs(train_data)\n",
    "print(len(vocab))\n",
    "print(len(tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input to the network takes the form of a $k$-dimensional vector of word ids and/or tag ids. Each integer $i$ is mapped to an $e_i$-dimensional embedding vector. These vectors are concatenated to form a vector of length $e_1 + \\cdots + e_k$, and sent through a feed-forward network with a single hidden layer and a rectified linear unit (ReLU).\n",
    "\n",
    "#### Default features\n",
    "\n",
    "A fixed-window model with the following features ($k=4$):\n",
    "\n",
    "0. current word\n",
    "1. previous word\n",
    "2. next word\n",
    "3. tag predicted for the previous word\n",
    "\n",
    "Whenever the value of a feature is undefined, the special value `PAD` is used.\n",
    "\n",
    "#### Embedding specifications\n",
    "\n",
    " An embedding specification is a triple $(m, n, e)$ consisting of three integers. Such a triple specifies that the model should include $m$ instances of an embedding from $n$ items to vectors of size $e$. All of the $m$ instances are to share their weights. The embeddings are embeddings for words and tags. For example, to instantiate the default feature model:\n",
    "\n",
    "``\n",
    "[(3, num_words, word_dim), (1, num_tags, tag_dim)]\n",
    "``\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The network architecture introduces a number of hyperparameters. The following choices are reasonable defaults:\n",
    "\n",
    "* width of each word embedding: 50\n",
    "* width of each tag embedding: 10\n",
    "* size of the hidden layer: 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n",
    "\n",
    "> A fixed-window model is initialized with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n",
    "\n",
    "**forward** (*self*, *features*)\n",
    "\n",
    "> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([\n",
    "                        ['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                        ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 1 * 10,\n",
    "\n",
    "        # Create RELU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags \n",
    "        words = features[:,:-1]\n",
    "        tags = features[:,-1]\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 1 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, tag_embs], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger interface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict** (*self*, *sentence*)\n",
    "\n",
    "> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *i*, *pred_tags*)\n",
    "\n",
    "> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n",
    "\n",
    "**predict** (*self*, *words*)\n",
    "\n",
    "> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowTaggerModel(embedding_specs, hidden_dim, len(vocab_tags)).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        feature = []\n",
    "        if len(words) == 1:\n",
    "            feature = [words[i], 0, 0, 0]\n",
    "\n",
    "        elif i == 0: # first word\n",
    "            # Wi, PAD, PAD, PAD\n",
    "            feature = [words[i], words[i+1], 0, 0]\n",
    "        elif i == len(words)-1: # last word\n",
    "            # Wi, Wi+1, PAD, PAD\n",
    "            feature = [words[i], 0, words[i-1], pred_tags[i-1]]\n",
    "        else:\n",
    "            # Wi, Wi+1, Wi-1, Ti-1\n",
    "            feature = [words[i], words[i+1], words[i-1], pred_tags[i-1]]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if not word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "\n",
    "        # predict tags\n",
    "        pred_tags_idxs = [0] * len(words)\n",
    "        for i in range(0, len(words_idxs)):\n",
    "            feature = self.featurize(words_idxs, i, pred_tags_idxs)\n",
    "            pred_tags = self.model.forward(feature)\n",
    "            # Find tag index with highest probability\n",
    "            pred_tags_idxs[i] = torch.argmax(pred_tags).item()\n",
    "        \n",
    "        # convert tag indexes\n",
    "        pred_tags = []\n",
    "        for tag_idx in pred_tags_idxs:\n",
    "            tag = [k for k, v in self.vocab_tags.items() if v == tag_idx][0]\n",
    "            pred_tags.append(tag)\n",
    "        \n",
    "        return pred_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training examples for the Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training_examples_tagger** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of tagged sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_tagger(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
    "    batch = []\n",
    "    gold_label = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "\n",
    "        for word, tag, _ in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "\n",
    "        for i in range(0, len(all_words_idx)):\n",
    "            batch.append(tagger.featurize(all_words_idx, i, all_tags_idx))\n",
    "            gold_label.append(all_tags_idx[i])\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 4).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(gold_label).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                gold_label = []\n",
    "\n",
    "        # Yield remaining batch\n",
    "        if sentence_idx == len(list(gold_data))-1:\n",
    "            remainder = len(batch)\n",
    "            batch_tensor = torch.Tensor(remainder, 4).long().to(device)\n",
    "            bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "            by = torch.Tensor(gold_label).long().to(device)\n",
    "            yield bx, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_fixed_window_tagger** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over tagged sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_tagger(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "    tagger.model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        nr_iterations += len(words)\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                batch = 0\n",
    "                tagger.model.train()\n",
    "                for bx, by in training_examples_tagger(vocab_words, vocab_tags, train_data, tagger, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = tagger.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(loss.item()), batch=batch+1)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "tagger.model = torch.load('tagger_model', map_location=device)\n",
    "print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger = train_fixed_window_tagger(train_data)\n",
    "# vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "# tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "# print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predicted part-of-speech tags dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tagger to create predicted part-of-speech tags dataset for parser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedDataset():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = []\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append(columns)\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-train-projectivized-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "    for sentence in TaggedDataset('en_ewt-ud-train-projectivized.conllu'):\n",
    "        words = [columns[1] for columns in sentence]\n",
    "        for i, t in enumerate(tagger.predict(words)):\n",
    "            sentence[i][3] = t\n",
    "        for columns in sentence:\n",
    "            print('\\t'.join(c for c in columns), file=target)\n",
    "        print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-dev-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "    for sentence in TaggedDataset('en_ewt-ud-dev.conllu'):\n",
    "        words = [columns[1] for columns in sentence]\n",
    "        for i, t in enumerate(tagger.predict(words)):\n",
    "            sentence[i][3] = t\n",
    "        for columns in sentence:\n",
    "            print('\\t'.join(c for c in columns), file=target)\n",
    "        print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_retagged = Dataset('en_ewt-ud-train-projectivized-retagged.conllu')\n",
    "dev_data_retagged = Dataset('en_ewt-ud-dev-retagged.conllu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser evaluation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**uas** (*parser*, *gold_data*)\n",
    "\n",
    "> Computes the unlabelled attachment score of the specified *parser* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. The unlabelled attachment score is the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). The calculation excludes the pseudo-roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        tags = [tokens[1] for tokens in sentence]\n",
    "        correct_head = [tokens[2] for tokens in sentence]\n",
    "        # Do not include pseudo-root\n",
    "        nr_words += (len(words) - 1)\n",
    "\n",
    "        \n",
    "        predicted_head = parser.predict(words, tags)\n",
    "\n",
    "        # skip pseudo-root\n",
    "        for i in range(1, len(words)):\n",
    "            if predicted_head[i] == correct_head[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The arc-hybrid algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcHybridParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return (0, [], [0] * num_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        valid_moves = []\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            valid_moves.append(ArcHybridParser.SH)\n",
    "        if len(stack) > 1 and buffer < len(config[2]):\n",
    "            valid_moves.append(ArcHybridParser.LA)\n",
    "        if len(stack) > 1:\n",
    "            valid_moves.append(ArcHybridParser.RA)\n",
    "        return valid_moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        buffer, stack, heads = config\n",
    "        # SHIFT\n",
    "        if move == ArcHybridParser.SH:\n",
    "            stack.append(buffer)\n",
    "            buffer += 1\n",
    "        # LEFT ARC\n",
    "        elif move == ArcHybridParser.LA:\n",
    "            heads[stack[-1]] = buffer\n",
    "            stack = stack[:-1]\n",
    "        # RIGHT ARC\n",
    "        elif move == ArcHybridParser.RA:\n",
    "            heads[stack[-1]] = stack[-2]\n",
    "            stack = stack[:-1]\n",
    "            \n",
    "        return (buffer, stack, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        buffer, stack, heads = config\n",
    "        return buffer == len(heads) and len(stack) == 1 and stack[0] == 0\n",
    "    \n",
    "    # Buffer = 0\n",
    "    # stack = 1\n",
    "    # heads = 2\n",
    "    @staticmethod\n",
    "    def zero_cost_shift(current_config, gold_config):\n",
    "        if current_config[0] == len(current_config[2]):\n",
    "            return False\n",
    "        if len(current_config[1]) == 0:\n",
    "            return True\n",
    "        item = current_config[0]\n",
    "\n",
    "        # SH 1\n",
    "        if item in gold_config:\n",
    "            for d in current_config[1][0:-1]:\n",
    "                if item == gold_config[d]:\n",
    "                    return False\n",
    "\n",
    "        # SH 2\n",
    "        for h in current_config[1][0:-1]:\n",
    "            if h == gold_config[item]:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    # Buffer = 0\n",
    "    # stack = 1\n",
    "    # heads = 2\n",
    "    @staticmethod\n",
    "    def zero_cost_la(current_config, gold_config):\n",
    "        s0 = current_config[1][-1]\n",
    "        if len(current_config[1]) < 2:\n",
    "            s1 = None\n",
    "        else:\n",
    "            s1 = current_config[1][-2]\n",
    "\n",
    "        # LA 1\n",
    "        for buffer_item in range(current_config[0],len(current_config[2])):\n",
    "            if s0 == gold_config[buffer_item]:\n",
    "                return False \n",
    "    \n",
    "        # LA 2\n",
    "        if s1 == gold_config[s0]:\n",
    "            return False\n",
    "         \n",
    "        # LA 3\n",
    "        for buffer_item in range(current_config[0]+1,len(current_config[2])):\n",
    "            if buffer_item == gold_config[s0]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    # Buffer = 0\n",
    "    # stack = 1\n",
    "    # heads = 2\n",
    "    @staticmethod\n",
    "    def zero_cost_ra(current_config, gold_config):\n",
    "        s0 = current_config[1][-1]\n",
    "\n",
    "        # RA 1\n",
    "        for buffer_item in range(current_config[0],len(current_config[2])):\n",
    "            if s0 == gold_config[buffer_item]:\n",
    "                return False\n",
    "\n",
    "        # RA 2\n",
    "        for buffer_item in range(current_config[0],len(current_config[2])):\n",
    "            if buffer_item == gold_config[s0]:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test zero cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArcHybridParser()\n",
    "\n",
    "gold_config = [0, 2, 0, 5, 2, 2]\n",
    "sh_cost_1 = (5, [0, 2, 3, 4], [0, 2, 0, 0, 0, 0]) # stack[..., d(3), ...] buffer[h(5), ...]\n",
    "assert not parser.zero_cost_shift(sh_cost_1, gold_config)\n",
    "\n",
    "gold_config = [0, 2, 0, 4, 2, 2]\n",
    "sh_cost_2 = (5, [0, 2, 4], [0, 2, 0, 4, 0, 0]) # stack[..., h(2), ...] buffer[d(5), ...]\n",
    "assert not parser.zero_cost_shift(sh_cost_2, gold_config)\n",
    "\n",
    "gold_config = [0, 2, 0, 4, 2, 2]\n",
    "la_cost_1 = (4, [0, 2], [0, 2, 0, 0, 0, 0]) # stack[..., h(2)] buffer[b, ..., d(5)]\n",
    "assert not parser.zero_cost_la(la_cost_1, gold_config)\n",
    "\n",
    "gold_config = [0, 2, 0, 4, 2, 2]\n",
    "la_cost_2 = (5, [0, 2, 4], [0, 2, 0, 4, 0, 0]) # stack[..., h(2), d(4)] buffer[b, ...]\n",
    "assert not parser.zero_cost_la(la_cost_2, gold_config)\n",
    "\n",
    "gold_config = [0, 2, 0, 5, 2, 2]\n",
    "la_cost_3 = (4, [0, 2, 3], [0, 2, 0, 0, 0, 0]) # stack[..., d(3)] buffer[b, ..., h(5)]\n",
    "assert not parser.zero_cost_la(la_cost_3, gold_config)\n",
    "\n",
    "gold_config = [0, 2, 0, 5, 2, 2]\n",
    "ra_cost_1 = (3, [0, 2], [0, 2, 0, 0, 0, 0]) # stack[..., h(2)] buffer[..., d(4)]\n",
    "assert not parser.zero_cost_ra(ra_cost_1, gold_config)\n",
    "\n",
    "gold_config = [0, 2, 0, 5, 2, 2]\n",
    "ra_cost_2 = (4, [0, 2, 3], [0, 2, 0, 0, 0, 0]) # stack[..., d(3)] buffer[..., h(5)]\n",
    "assert not parser.zero_cost_ra(ra_cost_2, gold_config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dynamic oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT, LA, RA = 0,1,2\n",
    "def dynamic_oracle(gold_config, current_config, legal_transition,parser):\n",
    "    moves = []\n",
    "    if SHIFT in legal_transition and parser.zero_cost_shift(current_config,gold_config):\n",
    "        moves.append(SHIFT)\n",
    "    if LA in legal_transition and parser.zero_cost_la(current_config,gold_config):\n",
    "        moves.append(LA)\n",
    "    if RA in legal_transition and parser.zero_cost_ra(current_config,gold_config):\n",
    "        moves.append(RA)\n",
    "    return moves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                                         ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 12 * 50 + 12 * 10,\n",
    "\n",
    "        # Create ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags\n",
    "        words, tags = torch.split(features, 12, dim=1)\n",
    "        \n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 12 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 12 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        concat_tags = tag_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, concat_tags], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcHybridParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        num_moves = len(ArcHybridParser.MOVES)\n",
    "        embedding_specs = [(12, len(vocab_words), word_dim), (12, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowParserModel(embedding_specs, hidden_dim, num_moves).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, tags, gold_heads, config):\n",
    "        buffer, stack, heads = config\n",
    "    \n",
    "        s0_w = self.vocab_words[PAD]\n",
    "        s0_t = self.vocab_tags[PAD]\n",
    "        s1_w = self.vocab_words[PAD]\n",
    "        s1_t = self.vocab_tags[PAD]\n",
    "        s2_w = self.vocab_words[PAD]\n",
    "        s2_t = self.vocab_tags[PAD]\n",
    "\n",
    "        b0_w = self.vocab_words[PAD]\n",
    "        b0_t = self.vocab_tags[PAD]\n",
    "        b1_w = self.vocab_words[PAD]\n",
    "        b1_t = self.vocab_tags[PAD]\n",
    "        b2_w = self.vocab_words[PAD]\n",
    "        b2_t = self.vocab_tags[PAD]\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            b0_w = words[buffer]\n",
    "            b0_t = tags[buffer]\n",
    "            if buffer + 1 < len(heads):\n",
    "                b1_w = words[buffer + 1]\n",
    "                b1_t = tags[buffer + 1]\n",
    "                if buffer + 2 < len(heads):\n",
    "                    b2_w = words[buffer + 2]\n",
    "                    b2_t = tags[buffer + 2]\n",
    "        \n",
    "        if len(stack) >= 1:\n",
    "            s0_w = words[stack[-1]]\n",
    "            s0_t = tags[stack[-1]]\n",
    "            if len(stack) >= 2:\n",
    "                s1_w = words[stack[-2]]\n",
    "                s1_t = tags[stack[-2]]\n",
    "                if len(stack) >= 3:\n",
    "                    s2_w = words[stack[-3]]\n",
    "                    s2_t = tags[stack[-3]]\n",
    "        \n",
    "        s0_b1_w = self.vocab_words[PAD]\n",
    "        s0_b2_w = self.vocab_words[PAD]\n",
    "        s0_b1_t = self.vocab_tags[PAD]\n",
    "        s0_b2_t = self.vocab_tags[PAD]\n",
    "        for idx, head in enumerate(gold_heads[0:s0_w]):\n",
    "            if head == s0_w and s0_b1_w == self.vocab_tags[PAD]:\n",
    "                s0_b1_w = words[idx]\n",
    "                s0_b1_t = tags[idx]\n",
    "            if head == s0_w and s0_b2_w == self.vocab_tags[PAD]:\n",
    "                s0_b2_w = words[idx]\n",
    "                s0_b2_t = tags[idx]\n",
    "\n",
    "\n",
    "        s0_f1_w = self.vocab_words[PAD]\n",
    "        s0_f2_w = self.vocab_words[PAD]\n",
    "        s0_f1_t = self.vocab_tags[PAD]\n",
    "        s0_f2_t = self.vocab_tags[PAD]\n",
    "        if len(stack) >= 1:\n",
    "            for idx, head in enumerate(gold_heads[s0_w:]):\n",
    "                if head == s0_w and s0_f1_w == self.vocab_tags[PAD]:\n",
    "                    s0_f1_w = words[idx]\n",
    "                    s0_f1_t = tags[idx]\n",
    "                if head == s0_w and s0_f2_w == self.vocab_tags[PAD]:\n",
    "                    s0_f2_w = tags[idx]\n",
    "                    s0_f2_t = tags[idx]\n",
    "\n",
    "\n",
    "        n0_b1_w = self.vocab_words[PAD]\n",
    "        n0_b2_w = self.vocab_words[PAD]\n",
    "        n0_b1_t = self.vocab_tags[PAD]\n",
    "        n0_b2_t = self.vocab_tags[PAD]\n",
    "        for idx, head in enumerate(gold_heads[0:b0_w]):\n",
    "            if head == b0_w and n0_b1_w == self.vocab_tags[PAD]:\n",
    "                n0_b1_w = words[idx]\n",
    "                n0_b1_t = tags[idx]\n",
    "            if head == b0_w and n0_b2_w == self.vocab_tags[PAD]:\n",
    "                n0_b2_w = words[idx]\n",
    "                n0_b2_t = tags[idx]\n",
    "\n",
    "\n",
    "        feature = [b0_w, b1_w, b2_w, s0_w, s1_w, s2_w,\n",
    "                   s0_b1_w, s0_b2_w, s0_f1_w, s0_f2_w, n0_b1_w, n0_b2_w,\n",
    "                   b0_t, b1_t, b2_t, s0_t, s1_t, s2_t,\n",
    "                   s0_b1_t, s0_b2_t, s0_f1_t, s0_f2_t, n0_b1_t, n0_b2_t]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "\n",
    "        # find tag indexes for given tags\n",
    "        tags_idxs = []\n",
    "        for tag in tags:\n",
    "            if tag in self.vocab_tags:\n",
    "                tags_idxs.append(self.vocab_tags[tag])\n",
    "            else:\n",
    "                tags_idxs.append(self.vocab_tags[PAD])\n",
    "\n",
    "        config = self.initial_config(len(words))\n",
    "\n",
    "        while not self.is_final_config(config):\n",
    "            valid_moves = self.valid_moves(config)\n",
    "            feature = self.featurize(words_idxs, tags_idxs, list(config[2]), config)\n",
    "            pred_moves = self.model.forward(feature)\n",
    "            _, sorted_indexes = torch.sort(pred_moves, descending=True)\n",
    "            # find valid move with highest score (SH, LA, RA)\n",
    "            if len(valid_moves) > 0:\n",
    "                sorted_move_list = sorted_indexes.tolist()[0]\n",
    "                # choose first valid move as default move\n",
    "                new_move = valid_moves[0]\n",
    "                for move in sorted_move_list:\n",
    "                    if move in valid_moves:\n",
    "                        new_move = move\n",
    "                        break\n",
    "                config = self.next_config(config, new_move)\n",
    "\n",
    "        return config[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_fixed_window_parser** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window parser from a set of training data *train_data* (an iterable over parsed sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_move(scores, legal_transitions):\n",
    "    _, sorted_indexes = torch.sort(scores, descending=True)\n",
    "    # find valid move with highest score (SH, LA, RA)\n",
    "    if len(legal_transitions) > 0:\n",
    "        sorted_move_list = sorted_indexes.tolist()[0]\n",
    "        # choose first valid move as default move\n",
    "        t_p = legal_transitions[0]\n",
    "        for move in sorted_move_list:\n",
    "            if move in legal_transitions:\n",
    "                return t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_fixed_window_parser(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "    arc_parser = ArcHybridParser()\n",
    "\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        nr_iterations += 1\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                total_moves = 1\n",
    "                train_loss = 0\n",
    "                batch_loss = []\n",
    "                batch_iter = 0\n",
    "\n",
    "                parser.model.train()\n",
    "                for sentence in train_data:\n",
    "                    all_words_idx = []\n",
    "                    all_tags_idx = []\n",
    "                    all_heads = []\n",
    "\n",
    "                    for word, tag, head in sentence:\n",
    "                        all_words_idx.append(vocab_words[word])\n",
    "                        all_tags_idx.append(vocab_tags[tag])\n",
    "                        all_heads.append(head)\n",
    "                    \n",
    "                    config = arc_parser.initial_config(len(all_heads))\n",
    "\n",
    "                    while not arc_parser.is_final_config(config):\n",
    "                        # Get all legal moves\n",
    "                        legal_transitions = arc_parser.valid_moves(config)\n",
    "\n",
    "                        # Compute which move should be taken next\n",
    "                        scores = parser.model.forward(parser.featurize(all_words_idx, all_tags_idx, list(config[2]), config)).to(device)\n",
    "\n",
    "                        # Get legal move with highest probability\n",
    "                        t_p = find_highest_move(scores, legal_transitions)\n",
    "\n",
    "                        # Extract scores to list\n",
    "                        scores_list = scores.tolist()[0]\n",
    "\n",
    "                        # Compute which moves are zero cost\n",
    "                        zero_cost_moves = dynamic_oracle(all_heads, config, legal_transitions, arc_parser)\n",
    "                        \n",
    "                        # Get the best legal zero cost move\n",
    "                        t_o = max(zero_cost_moves, key=lambda p: scores_list[p])\n",
    "                        # Target vector    \n",
    "                        y = torch.tensor([t_o]).long().to(device)\n",
    "\n",
    "                        loss = F.cross_entropy(scores, y)\n",
    "                        batch_loss.append(loss)\n",
    "                        train_loss += loss.item()\n",
    "\n",
    "                        # If predicted transition is not in the zero cost moves, update weights.\n",
    "                        if t_p not in zero_cost_moves:\n",
    "                            # choose random transition from zero_cost. Might be bad move but such is life.\n",
    "                            config = parser.next_config(config, random.choice(zero_cost_moves))\n",
    "                        else:\n",
    "                            config = parser.next_config(config, t_p)\n",
    "\n",
    "                        pbar.set_postfix(loss=(train_loss/total_moves), configs=total_moves)\n",
    "                        total_moves += 1\n",
    "                        batch_iter += 1\n",
    "\n",
    "                        # Update the parameters\n",
    "                        if len(batch_loss) > 0 and batch_iter >= batch_size:\n",
    "                            optimizer.zero_grad()\n",
    "                            loss = sum(batch_loss)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            batch_loss = []\n",
    "                            batch_iter = 0\n",
    "                    \n",
    "                    #if batch_iter == 10:\n",
    "                    #    break\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12544/12544 [11:16<00:00, 18.53it/s, configs=421696, loss=0.267]\n"
     ]
    }
   ],
   "source": [
    "parser = train_fixed_window_parser(train_data_retagged, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6717\n"
     ]
    }
   ],
   "source": [
    "print('{:.4f}'.format(uas(parser, dev_data_retagged)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc143343ca8435bba8c44b3b1f47f9edcb7f00f13cf7dc8cb9f5e5ffbd446b7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
