{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). The code below defines an iterable-style dataset for parser data in the [CoNLL-U format](https://universaldependencies.org/format.html) that the project uses to distribute its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = [Dataset.ROOT]\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append((columns[1], columns[3], int(columns[6])))\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = [Dataset.ROOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('data/no_nynorsk-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('data/no_nynorsk-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<root>', '<root>', 0),\n",
       " ('-', 'PUNCT', 2),\n",
       " ('Ja', 'INTJ', 0),\n",
       " ('.', 'PUNCT', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = list(train_data)[1000]\n",
    "example_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tagger, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        \n",
    "        nr_words += len(words)\n",
    "\n",
    "        correct_tags = [tokens[1] for tokens in sentence]\n",
    "        predicted_tags = tagger.predict(words)\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            if predicted_tags[i] == correct_tags[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    tags = {PAD: 0}\n",
    "    for sentence in gold_data:\n",
    "        for pair in sentence:\n",
    "            word = pair[0]\n",
    "            tag = pair[1]\n",
    "            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            \n",
    "            if tag not in tags:\n",
    "                tags[tag] = len(tags)\n",
    "                    \n",
    "    return vocab, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29136\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "vocab, tags = make_vocabs(train_data)\n",
    "print(len(vocab))\n",
    "print(len(tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([\n",
    "                        ['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                        ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 1 * 10,\n",
    "\n",
    "        # Create RELU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags \n",
    "        words = features[:,:-1]\n",
    "        tags = features[:,-1]\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 1 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, tag_embs], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowTaggerModel(embedding_specs, hidden_dim, len(vocab_tags)).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        feature = []\n",
    "        if len(words) == 1:\n",
    "            feature = [words[i], 0, 0, 0]\n",
    "\n",
    "        elif i == 0: # first word\n",
    "            # Wi, PAD, PAD, PAD\n",
    "            feature = [words[i], words[i+1], 0, 0]\n",
    "        elif i == len(words)-1: # last word\n",
    "            # Wi, Wi+1, PAD, PAD\n",
    "            feature = [words[i], 0, words[i-1], pred_tags[i-1]]\n",
    "        else:\n",
    "            # Wi, Wi+1, Wi-1, Ti-1\n",
    "            feature = [words[i], words[i+1], words[i-1], pred_tags[i-1]]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if not word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "\n",
    "        # predict tags\n",
    "        pred_tags_idxs = [0] * len(words)\n",
    "        for i in range(0, len(words_idxs)):\n",
    "            feature = self.featurize(words_idxs, i, pred_tags_idxs)\n",
    "            pred_tags = self.model.forward(feature)\n",
    "            # Find tag index with highest probability\n",
    "            pred_tags_idxs[i] = torch.argmax(pred_tags).item()\n",
    "        \n",
    "        # convert tag indexes\n",
    "        pred_tags = []\n",
    "        for tag_idx in pred_tags_idxs:\n",
    "            tag = [k for k, v in self.vocab_tags.items() if v == tag_idx][0]\n",
    "            pred_tags.append(tag)\n",
    "        \n",
    "        return pred_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training examples for the Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_tagger(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
    "    batch = []\n",
    "    gold_label = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "\n",
    "        for word, tag, _ in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "\n",
    "        for i in range(0, len(all_words_idx)):\n",
    "            batch.append(tagger.featurize(all_words_idx, i, all_tags_idx))\n",
    "            gold_label.append(all_tags_idx[i])\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 4).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(gold_label).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                gold_label = []\n",
    "\n",
    "        # Yield remaining batch\n",
    "        if sentence_idx == len(list(gold_data))-1:\n",
    "            remainder = len(batch)\n",
    "            batch_tensor = torch.Tensor(remainder, 4).long().to(device)\n",
    "            bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "            by = torch.Tensor(gold_label).long().to(device)\n",
    "            yield bx, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_init(model, std=0.01):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data.normal_(mean=0.0, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_tagger(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "    #tagger.model.to(device)\n",
    "    \n",
    "    # Initialize embedding weights\n",
    "    var_init(tagger.model)\n",
    "    \n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        nr_iterations += len(words)\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                batch = 0\n",
    "                tagger.model.train()\n",
    "                for bx, by in training_examples_tagger(vocab_words, vocab_tags, train_data, tagger, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = tagger.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(loss.item()), batch=batch+1)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger = train_fixed_window_tagger(train_data)\n",
    "# print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8951\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "tagger.model = torch.load('nynorsk_tagger_model', map_location=device)\n",
    "print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predicted part-of-speech tags dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tagger to create predicted part-of-speech tags dataset for parser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedDataset():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = []\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append(columns)\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/en_ewt-ud-train-projectivized-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "#     for sentence in TaggedDataset('data/en_ewt-ud-train-projectivized.conllu'):\n",
    "#         words = [columns[1] for columns in sentence]\n",
    "#         for i, t in enumerate(tagger.predict(words)):\n",
    "#             sentence[i][3] = t\n",
    "#         for columns in sentence:\n",
    "#             print('\\t'.join(c for c in columns), file=target)\n",
    "#         print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/en_ewt-ud-dev-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "#     for sentence in TaggedDataset('en_ewt-ud-dev.conllu'):\n",
    "#         words = [columns[1] for columns in sentence]\n",
    "#         for i, t in enumerate(tagger.predict(words)):\n",
    "#             sentence[i][3] = t\n",
    "#         for columns in sentence:\n",
    "#             print('\\t'.join(c for c in columns), file=target)\n",
    "#         print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_retaged = Dataset('data/no_nynorsk-ud-train-projectivized-retagged.conllu')\n",
    "dev_data_retaged = Dataset('data/no_nynorsk-ud-dev-retagged.conllu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        tags = [tokens[1] for tokens in sentence]\n",
    "        # Do not include pseudo-root\n",
    "        nr_words += (len(words) - 1)\n",
    "\n",
    "        correct_head = [tokens[2] for tokens in sentence]\n",
    "        predicted_head = parser.predict(words, tags)\n",
    "\n",
    "        # skip pseudo-root\n",
    "        for i in range(1, len(words)):\n",
    "            if predicted_head[i] == correct_head[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The arc-standard algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandardParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return (0, [], [0] * num_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        valid_moves = []\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            valid_moves.append(ArcStandardParser.SH)\n",
    "        if len(stack) > 2:\n",
    "            valid_moves.append(ArcStandardParser.LA)\n",
    "        if len(stack) > 1:\n",
    "            valid_moves.append(ArcStandardParser.RA)\n",
    "        return valid_moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        buffer, stack, heads = config\n",
    "        # SHIFT\n",
    "        if move == ArcStandardParser.SH:\n",
    "            stack.append(buffer)\n",
    "            buffer += 1\n",
    "        # LEFT ARC\n",
    "        elif move == ArcStandardParser.LA:\n",
    "            heads[stack[-2]] = stack[-1]\n",
    "            top = stack[-1]\n",
    "            stack = stack[:-2] \n",
    "            stack.append(top)\n",
    "        # RIGHT ARC\n",
    "        elif move == ArcStandardParser.RA:\n",
    "            heads[stack[-1]] = stack[-2]\n",
    "            stack = stack[:-1]\n",
    "            \n",
    "        return (buffer, stack, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        buffer, stack, heads = config\n",
    "        return buffer == len(heads) and len(stack) == 1 and stack[0] == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT, LA, RA = 0,1,2\n",
    "def dynamic_oracle(gold_config, current_config, legal_transition,parser):\n",
    "    moves = []\n",
    "    if SHIFT in legal_transition and parser.zero_cost_shift(current_config,gold_config):\n",
    "        moves.append(SHIFT)\n",
    "    if LA in legal_transition and parser.zero_cost_la(current_config,gold_config):\n",
    "        moves.append(LA)\n",
    "    if RA in legal_transition and parser.zero_cost_ra(current_config,gold_config):\n",
    "        moves.append(RA)\n",
    "    return moves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                                         ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 3 * 10,\n",
    "\n",
    "        # Create ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags for buffer 1, stack 1, stack 2\n",
    "        words, tags = torch.split(features, 3, dim=1)\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 3 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        concat_tags = tag_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, concat_tags], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcStandardParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        num_moves = len(ArcStandardParser.MOVES)\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (3, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowParserModel(embedding_specs, hidden_dim, num_moves).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, tags, config):\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        # stack might be empty or not have enough words, set words and tags to PAD\n",
    "        word_2 = self.vocab_words[PAD]\n",
    "        tag_2 = self.vocab_tags[PAD]\n",
    "        word_3 = self.vocab_words[PAD]\n",
    "        tag_3 = self.vocab_tags[PAD]\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            word_1 = words[buffer]\n",
    "            tag_1 = tags[buffer]\n",
    "        else:\n",
    "            word_1 = self.vocab_words[PAD]\n",
    "            tag_1 = self.vocab_tags[PAD]\n",
    "        \n",
    "        if len(stack) >= 2 and len(stack) <= len(words):\n",
    "            word_2 = words[stack[-1]]\n",
    "            tag_2 = tags[stack[-1]]\n",
    "            word_3 = words[stack[-2]]\n",
    "            tag_3 = tags[stack[-2]]\n",
    "\n",
    "        elif len(stack) == 1:\n",
    "            word_2 = words[stack[-1]]\n",
    "            tag_2 = tags[stack[-1]]\n",
    "\n",
    "        # next word in buffer, topmost word on stack, 2nd topmost word on stack,\n",
    "        # tag of next word in buffer, tag of topmost word on stack, tag of 2nd topmost word on stack\n",
    "        feature = [word_1, word_2, word_3, tag_1, tag_2, tag_3]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "\n",
    "        # find tag indexes for given tags\n",
    "        tags_idxs = []\n",
    "        for tag in tags:\n",
    "            if tag in self.vocab_tags:\n",
    "                tags_idxs.append(self.vocab_tags[tag])\n",
    "            else:\n",
    "                tags_idxs.append(self.vocab_tags[PAD])\n",
    "\n",
    "        config = self.initial_config(len(words))\n",
    "\n",
    "        while not self.is_final_config(config):\n",
    "            valid_moves = self.valid_moves(config)\n",
    "            feature = self.featurize(words_idxs, tags_idxs, config)\n",
    "            pred_moves = self.model.forward(feature)\n",
    "            _, sorted_indexes = torch.sort(pred_moves, descending=True)\n",
    "            # find valid move with highest score (SH, LA, RA)\n",
    "            if len(valid_moves) > 0:\n",
    "                sorted_move_list = sorted_indexes.tolist()[0]\n",
    "                # choose first valid move as default move\n",
    "                new_move = valid_moves[0]\n",
    "                for move in sorted_move_list:\n",
    "                    if move in valid_moves:\n",
    "                        new_move = move\n",
    "                        break\n",
    "                config = self.next_config(config, new_move)\n",
    "\n",
    "        return config[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training examples for the Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_parser(vocab_words, vocab_tags, gold_data, parser, batch_size=100):\n",
    "    batch = []\n",
    "    moves = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "        all_heads = []\n",
    "\n",
    "        for word, tag, head in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "            all_heads.append(head)\n",
    "\n",
    "        for c, m in oracle_moves(all_heads):\n",
    "            batch.append(parser.featurize(all_words_idx, all_tags_idx, c))\n",
    "            moves.append(m)\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 6).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(moves).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                moves = []\n",
    "\n",
    "    # Yield remaining batch\n",
    "    if sentence_idx == len(list(gold_data))-1:\n",
    "        remainder = len(batch)\n",
    "        batch_tensor = torch.Tensor(remainder, 6).long().to(device)\n",
    "        bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "        by = torch.Tensor(moves).long().to(device)\n",
    "        yield bx, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_parser(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        nr_iterations += 2 * len(sentence) - 1\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                batch = 1\n",
    "                train_loss = 0\n",
    "\n",
    "                parser.model.train()\n",
    "                for bx, by in training_examples_parser(vocab_words, vocab_tags, train_data, parser, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = parser.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    train_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(train_loss/batch), batch=batch)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/504834 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'oracle_moves' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parser \u001b[39m=\u001b[39m train_fixed_window_parser(train_data_retaged, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(uas(parser, dev_data_retaged)))\n",
      "Cell \u001b[1;32mIn[27], line 21\u001b[0m, in \u001b[0;36mtrain_fixed_window_parser\u001b[1;34m(train_data, n_epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     18\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     20\u001b[0m parser\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m bx, by \u001b[39min\u001b[39;00m training_examples_parser(vocab_words, vocab_tags, train_data, parser, batch_size):\n\u001b[0;32m     22\u001b[0m     curr_batch_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(bx)\n\u001b[0;32m     24\u001b[0m     score \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(bx)\n",
      "Cell \u001b[1;32mIn[26], line 16\u001b[0m, in \u001b[0;36mtraining_examples_parser\u001b[1;34m(vocab_words, vocab_tags, gold_data, parser, batch_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m     all_tags_idx\u001b[39m.\u001b[39mappend(vocab_tags[tag])\n\u001b[0;32m     14\u001b[0m     all_heads\u001b[39m.\u001b[39mappend(head)\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m c, m \u001b[39min\u001b[39;00m oracle_moves(all_heads):\n\u001b[0;32m     17\u001b[0m     batch\u001b[39m.\u001b[39mappend(parser\u001b[39m.\u001b[39mfeaturize(all_words_idx, all_tags_idx, c))\n\u001b[0;32m     18\u001b[0m     moves\u001b[39m.\u001b[39mappend(m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'oracle_moves' is not defined"
     ]
    }
   ],
   "source": [
    "parser = train_fixed_window_parser(train_data_retaged, n_epochs=1)\n",
    "print('{:.4f}'.format(uas(parser, dev_data_retaged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = train_fixed_window_parser(train_data, n_epochs=1)\n",
    "print('{:.4f}'.format(uas(parser, dev_data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsbb34",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42172f64df1a6ea7ffef16afee88427bf191e3efe91ff673607a65a00e26eb83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
