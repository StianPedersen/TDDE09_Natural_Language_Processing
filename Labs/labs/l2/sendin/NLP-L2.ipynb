{"cells":[{"cell_type":"markdown","metadata":{"id":"CgG5ryTDzG8N"},"source":["# L2: Language modelling"]},{"cell_type":"markdown","metadata":{"id":"8z3KhmuCzG8P"},"source":["In this lab you will implement and train two neural language models: the fixed-window model mentioned in Lecture&nbsp;2.3, and the recurrent neural network model from Lecture&nbsp;2.5. You will evaluate these models by computing their perplexity on a benchmark dataset."]},{"cell_type":"code","execution_count":88,"metadata":{"id":"IeHpIanNzG8R"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26538,"status":"ok","timestamp":1675399795669,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"},"user_tz":-60},"id":"T6jULJIMzLTN","outputId":"fa0deae1-9264-4d55-b5dc-377419de42aa"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"6P1KKHqBzG8S"},"source":["For this lab, you should use the GPU if you have one:"]},{"cell_type":"code","execution_count":90,"metadata":{"id":"9WsW-mx0zG8T"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"id":"9R4lrT01ArUA"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"M9fLhzT9zG8V"},"source":["The data for this lab is [WikiText](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/), a collection of more than 100 million tokens extracted from the set of &lsquo;Good&rsquo; and &lsquo;Featured&rsquo; articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.\n","\n","The next cell contains code for an object that will act as a container for the &lsquo;training&rsquo; and the &lsquo;validation&rsquo; section of the data. We fill this container by reading the corresponding text files. The only processing that we do is to whitespace-tokenize, and to replace each newline character with a special token `<eos>` (end-of-sentence)."]},{"cell_type":"code","execution_count":91,"metadata":{"id":"tQe4TVnTzG8X"},"outputs":[],"source":["class WikiText(object):\n","    \n","    def __init__(self):\n","        self.vocab = {}\n","        self.train = self.read_data('wiki.train.tokens')\n","        self.valid = self.read_data('wiki.valid.tokens')\n","    \n","    def read_data(self, path):\n","        ids = []\n","        with open(path, encoding=\"utf-8\") as source:\n","            for line in source:\n","                for token in line.split() + ['<eos>']:\n","                    if token not in self.vocab:\n","                        self.vocab[token] = len(self.vocab)\n","                    ids.append(self.vocab[token])\n","        return ids"]},{"cell_type":"markdown","metadata":{"id":"dWUEeDTIzG8Y"},"source":["The cell below loads the data and prints the total number of tokens and the size of the vocabulary."]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4601,"status":"ok","timestamp":1675399800802,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"},"user_tz":-60},"id":"3szKS_ipArUK","outputId":"f2c4a2cc-2b6c-42e4-95c5-7ff094886904"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens in train: 2088628\n","Tokens in valid: 217646\n","Vocabulary size: 33278\n"]}],"source":["wikitext = WikiText()\n","\n","print('Tokens in train:', len(wikitext.train))\n","print('Tokens in valid:', len(wikitext.valid))\n","print('Vocabulary size:', len(wikitext.vocab))"]},{"cell_type":"markdown","metadata":{"id":"acEbvPE2ArUi"},"source":["## Problem 1: Fixed-window neural language model"]},{"cell_type":"markdown","metadata":{"id":"_ZCkyT9yArUj"},"source":["In this section you will implement and train the fixed-window neural language model proposed by [Bengio et al. (2003)](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) and introduced in Lecture&nbsp;2.3. Recall that an input to the network takes the form of a vector of $n-1$ integers representing the preceding words. Each integer is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU) and a final softmax layer."]},{"cell_type":"markdown","metadata":{"id":"9Hr11DbAzG8c"},"source":["### Problem 1.1: Vectorize the data"]},{"cell_type":"markdown","metadata":{"id":"yALrz8BczG8c"},"source":["Your first task is to write code for transforming the data in the WikiText container into a vectorized form that can be fed to the fixed-window model. Complete the skeleton code in the cell below:"]},{"cell_type":"code","execution_count":93,"metadata":{"id":"XpyWHQGCzG8d"},"outputs":[],"source":["def vectorize_fixed_window(wikitext_data, n):\n","    # TODO: Replace the following line with your own code\n","\n","    X = []\n","    y = []\n","    for idx, word in enumerate(wikitext_data):\n","      if idx == len(wikitext_data) - n:\n","        break\n","      temp = wikitext_data[idx: idx + (n - 1)]\n","      \n","      X.append(temp)\n","      y.append(wikitext_data[idx + n - 1])\n","\n","    return torch.LongTensor(X), torch.LongTensor(y)"]},{"cell_type":"markdown","metadata":{"id":"S2aUSkTNzG8d"},"source":["Your function should meet the following specification:\n","\n","**vectorize_fixed_window** (*wikitext_data*, *n*)\n","\n","> Transforms WikiText data (a list of word ids) into a pair of tensors $\\mathbf{X}$, $\\mathbf{y}$ that can be used to train the fixed-window model. Let $N$ be the total number of $n$-grams from the token list; then $\\mathbf{X}$ is a matrix with shape $(N, n-1)$ and $\\mathbf{y}$ is a vector with length $N$.\n","\n","⚠️ Your function should be able to handle arbitrary values of $n \\geq 1$."]},{"cell_type":"markdown","metadata":{"id":"95vpg5p2zG8e"},"source":["#### 🤞 Test your code\n","\n","Test your implementation by running the code in the next cell. Does the output match your expectation?"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3541,"status":"ok","timestamp":1675399804911,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"},"user_tz":-60},"id":"ZcVeNEVXzG8f","outputId":"33486268-408b-4f98-cdf8-28dd61a2136d"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([217643, 2])\n","tensor([[    0,     1],\n","        [    1, 32966],\n","        [32966, 32967],\n","        ...,\n","        [    1,     1],\n","        [    1,     1],\n","        [    1,     0]])\n","tensor([32966, 32967,     1,  ...,     1,     0,     0])\n"]}],"source":["valid_x, valid_y = vectorize_fixed_window(wikitext.valid, 3)\n","train_x, train_y = vectorize_fixed_window(wikitext.train, 3)\n","\n","print(valid_x.size())\n","print(valid_x)\n","print(valid_y)"]},{"cell_type":"markdown","metadata":{"id":"-xxsusIzzG8f"},"source":["### Problem 1.2: Implement the model"]},{"cell_type":"markdown","metadata":{"id":"sIJw2KB3zG8g"},"source":["Your next task is to implement the fixed-window model based on the graphical specification given in the lecture."]},{"cell_type":"code","execution_count":95,"metadata":{"id":"m8j-hVCGzG8g"},"outputs":[],"source":["import torch.nn as nn\n","\n","class FixedWindowModel(nn.Module):\n","\n","    def __init__(self, n, n_words, embedding_dim=50, hidden_dim=50):\n","        super().__init__()\n","        self.embedding = nn.Embedding(n_words, embedding_dim, padding_idx = 0)\n","        self.relu = nn.ReLU()\n","        self.linear_ff = nn.Linear(embedding_dim * (n - 1), hidden_dim)\n","        self.linear_soft = nn.Linear(hidden_dim, n_words)\n","        # TODO: Add your own code\n","\n","    def forward(self, x):\n","        # TODO: Replace the next line with your own code\n","        #print(x.size())\n","        inputs = self.embedding(x).view((len(x), -1))\n","        #print(inputs.size())\n","        test = self.linear_ff(inputs)\n","        #print(test)\n","        out = self.relu(test)\n","        return self.linear_soft(out)"]},{"cell_type":"markdown","metadata":{"id":"AOtmWWiAzG8h"},"source":["Here is the specification of the two methods:\n","\n","**__init__** (*self*, *n*, *n_words*, *embedding_dim*=50, *hidden_dim*=50)\n","\n","> Creates a new fixed-window neural language model. The argument *n* specifies the model&rsquo;s $n$-gram order. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 50.\n","\n","**forward** (*self*, *x*)\n","\n","> Computes the network output on an input batch *x*. The shape of *x* is $(B, n-1)$, where $B$ is the batch size. The output of the forward pass is a tensor of shape $(B, V)$ where $V$ is the number of words in the vocabulary.\n","\n","**Hint:** The most efficient way to implement the vector concatenation in this model is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method.\n","\n","#### 🤞 Test your code\n","\n","Test your code by instantiating the model and feeding it a batch of examples from the training data."]},{"cell_type":"markdown","metadata":{"id":"v6uy-C13zG8i"},"source":["### Problem 1.3: Train the model"]},{"cell_type":"markdown","metadata":{"id":"aKHAZ8XvzG8i"},"source":["Your final task is to write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function.\n","\n","For your convenience, the following cell contains a utility function that randomly samples minibatches of a specified size from a pair of tensors:"]},{"cell_type":"code","execution_count":96,"metadata":{"id":"JVoQqzueArUv"},"outputs":[],"source":["def batchify(x, y, batch_size):\n","    random_indices = torch.randperm(len(x))\n","    for i in range(0, len(x) - batch_size + 1, batch_size):\n","        indices = random_indices[i:i+batch_size]\n","        yield x[indices].to(device), y[indices].to(device)\n","    remainder = len(x) % batch_size\n","    if remainder:\n","        indices = random_indices[-remainder:]\n","        yield x[indices].to(device), y[indices].to(device)"]},{"cell_type":"markdown","metadata":{"id":"Y_mOeWJJzG8j"},"source":["What remains to be done is the implementation of the training loop. This should be a straightforward generalization of the training loops that you have seen so far. Complete the skeleton code in the cell below:"]},{"cell_type":"code","execution_count":97,"metadata":{"id":"RLu7Bkj3zG8j"},"outputs":[],"source":["import torch.optim as optim\n","import torch.nn.functional as F\n","import tqdm\n","\n","def train_fixed_window(n, n_epochs=1, batch_size=3200, lr=1e-2):\n","    # TODO: Replace the following line with your own code\n","    model = FixedWindowModel(n, len(wikitext.vocab))\n","    model.to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    train_losses = []\n","    dev_losses = []\n","    dev_accuracies = []\n","    perplexity = []\n","\n","    #print(valid_x[0:batch_size])\n","\n","    with tqdm.tqdm(total = n_epochs) as pbar:\n","      for t in range (n_epochs):\n","        pbar.set_description(f'Epoch{t+1}')\n","        model.train()\n","        running_loss = 0\n","\n","        for x, y in batchify(train_x, train_y, batch_size):\n","          optimizer.zero_grad()\n","\n","          output = model.forward(x)\n","\n","          loss = F.cross_entropy(output, y)\n","          loss.backward()\n","          optimizer.step()\n","          running_loss += loss.item() * len(x)\n","          # perplexity.append(torch.exp(loss))\n","          # print('Perp Loss:', loss, 'PP:', perplexity)\n","      \n","        print('Running loss', running_loss) \n","        \n","        # Evaluation\n","        model.eval()\n","        for x, y in batchify(valid_x, valid_y, batch_size):\n","          with torch.no_grad():\n","                output_valid = model.forward(x)\n","                dev_losses.append(F.cross_entropy(output_valid, y))\n","              \n","        avg_loss = sum(dev_losses)/len(dev_losses)\n","        perplexity.append(torch.exp(torch.tensor(avg_loss)).item())\n","        pbar.update()\n","        train_losses.append(running_loss / len(valid_x))\n","\n","        print(\"perp \", perplexity)\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"p5M378wozG8j"},"source":["Here is the specification of the training function:\n","\n","**train_fixed_window** (*n*, *n_epochs* = 1, *batch_size* = 3200, *lr* = 0.01)\n","\n","> Trains a fixed-window neural language model of order *n* using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."]},{"cell_type":"markdown","metadata":{"id":"KpoQApo0zG8k"},"source":["The code in the cell below trains a bigram model."]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114007,"status":"ok","timestamp":1675399918914,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"},"user_tz":-60},"id":"sBhTCjChArU7","outputId":"7ddfee77-65d1-4d99-a7ef-c15d76b25529"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch1:   0%|          | 0/3 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Running loss 13029441.862940788\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\David Ångström\\AppData\\Local\\Temp\\ipykernel_21180\\3256191725.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  perplexity.append(torch.exp(torch.tensor(avg_loss)).item())\n","Epoch2:  33%|███▎      | 1/3 [00:15<00:31, 15.92s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [321.0711364746094]\n","Running loss 11595817.076814175\n"]},{"name":"stderr","output_type":"stream","text":["Epoch3:  67%|██████▋   | 2/3 [00:28<00:14, 14.82s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [321.0711364746094, 311.3558044433594]\n","Running loss 11097026.82826519\n"]},{"name":"stderr","output_type":"stream","text":["Epoch3: 100%|██████████| 3/3 [00:41<00:00, 13.71s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [321.0711364746094, 311.3558044433594, 311.18109130859375]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model_fixed_window = train_fixed_window(3, n_epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"5obcWk-uzG8k"},"source":["**⚠️ Your submitted notebook must contain output demonstrating a validation perplexity of at most 350.**\n","\n","**Hint:** Computing the validation perplexity in one go may exhaust your computer&rsquo;s memory and/or take a lot of time. If you run into this problem, break the computation down into minibatches and take the average perplexity."]},{"cell_type":"markdown","metadata":{"id":"TlSjQQNmzG8l"},"source":["#### 🤞 Test your code\n","\n","To see whether your network is learning something, print the loss and/or the perplexity on the training data. If the two values are not decreasing over time, try to find the problem before wasting time (and energy) on useless training.\n","\n","Training and even evaluation will take some time – on a CPU, you should expect several minutes per epoch, depending on hardware. To speed things up, you can train using a GPU; our reference implementation runs in less than 30 seconds per epoch on [Colab](http://colab.research.google.com)."]},{"cell_type":"markdown","metadata":{"id":"-d8PFZucArU8"},"source":["## Problem 2: Recurrent neural network language model"]},{"cell_type":"markdown","metadata":{"id":"3iVFQf17zG8l"},"source":["In this section you will implement the recurrent neural network language model that was presented in Lecture&nbsp;2.5. Recall that an input to the network is a vector of word ids. Each integer is mapped to an embedding vector. The sequence of embedded vectors is then fed into an unrolled LSTM. At each position $i$ in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation into a final softmax layer, from which we read off the index of the word at position $i+1$. In theory, the input vector could represent the complete training data or at least a complete sentence; for practical reasons, however, we will truncate the input to some fixed value *bptt_len*, the **backpropagation-through-time horizon**."]},{"cell_type":"markdown","metadata":{"id":"2SkKudBZzG8m"},"source":["### Problem 2.1: Vectorize the data"]},{"cell_type":"markdown","metadata":{"id":"V6LoeXeQzG8m"},"source":["As in the previous problem, your first task is to transform the data in the WikiText container into a vectorized form that can be fed to the model."]},{"cell_type":"code","execution_count":99,"metadata":{"id":"w-03flJYzG8n"},"outputs":[],"source":["def vectorize_rnn(wikitext_data, bptt_len):\n","    # TODO: Replace the next line with your own code\n","    X = []\n","    Y = []\n","\n","    once = 0\n","\n","    for idx, word in enumerate(wikitext_data):\n","      if idx < len(wikitext_data) - bptt_len - 1:\n","        if not (idx % bptt_len) == 0:\n","          continue\n","        X.append(wikitext_data[idx:idx + bptt_len])\n","        Y.append(wikitext_data[idx + 1:idx + bptt_len + 1])\n","      else:\n","        break\n","    \n","    return torch.LongTensor(X), torch.LongTensor(Y)\n"]},{"cell_type":"markdown","metadata":{"id":"fcU4YwOozG8n"},"source":["Your function should meet the following specification:\n","\n","**vectorize_rnn** (*wikitext_data*, *bptt_len*)\n","\n","> Transforms a list of token indexes into a pair of tensors $\\mathbf{X}$, $\\mathbf{Y}$ that can be used to train the recurrent neural language model. The rows of both tensors represent contiguous subsequences of token indexes of length *bptt_len*. Compared to the sequences in $\\mathbf{X}$, the corresponding sequences in $\\mathbf{Y}$ are shifted one position to the right. More precisely, if the $i$th row of $\\mathbf{X}$ is the sequence that starts at token position $j$, then the same row of $\\mathbf{Y}$ is the sequence that starts at position $j+1$."]},{"cell_type":"markdown","metadata":{"id":"lOXnlRr1zG8n"},"source":["#### 🤞 Test your code\n","\n","Test your implementation by running the following code:"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14026,"status":"ok","timestamp":1675399932938,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"},"user_tz":-60},"id":"FF1TApJPzG8p","outputId":"8bd8c170-d663-4829-c96d-60b9464e26c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6801, 32])\n"]}],"source":["valid_x, valid_y = vectorize_rnn(wikitext.valid, 32)\n","#valid_x_t, valid_y_t = vectorize_rnn_test(wikitext.valid, 32)\n","\n","train_x, train_y = vectorize_rnn(wikitext.train, 32)\n","\n","print(valid_x.size())"]},{"cell_type":"markdown","metadata":{"id":"MVPsoGYnzG8p"},"source":["### Problem 2.2: Implement the model"]},{"cell_type":"markdown","metadata":{"id":"Ea4TrtnnzG8p"},"source":["Your next task is to implement the recurrent neural network model based on the graphical specification given in the lecture."]},{"cell_type":"code","execution_count":101,"metadata":{"id":"YaInmKBczG8p"},"outputs":[],"source":["import torch.nn as nn\n","\n","class RNNModel(nn.Module):\n","    \n","    def __init__(self, n_words, embedding_dim=50, hidden_dim=50):\n","        super().__init__()\n","        self.embedding = nn.Embedding(n_words,embedding_dim,padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim,hidden_dim, batch_first = True)\n","        self.linear = nn.Linear(embedding_dim, n_words)\n","        # TODO: Add your own code\n","\n","        # For the last task\n","        #self.embedding.weight.data.uniform_(-1, 1)\n","\n","    def forward(self, x):\n","        # TODO: Replace the next line with your own code\n","        embed = self.embedding(x)\n","        output, (h_n, c_n) = self.lstm(embed)\n","\n","        last_hidden_state = h_n[-1]\n","        \n","        output = self.linear(output)\n","        return output, last_hidden_state"]},{"cell_type":"markdown","metadata":{"id":"KsBk505UzG8q"},"source":["Your implementation should follow this specification:\n","\n","**__init__** (*self*, *n_words*, *embedding_dim* = 50, *hidden_dim* = 50)\n","\n","> Creates a new recurrent neural network language model. The argument *n_words* is the number of words in the vocabulary. The arguments *embedding_dim* and *hidden_dim* specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 50.\n","\n","**forward** (*self*, *x*)\n","\n","> Computes the network output on an input batch *x*. The shape of *x* is $(B, H)$, where $B$ is the batch size and $H$ is the length of each input sequence. The shape of the output tensor is $(B, H, V)$, where $V$ is the size of the vocabulary."]},{"cell_type":"markdown","metadata":{"id":"BUGvIKfEzG8q"},"source":["#### 🤞 Test your code\n","\n","Test your code by instantiating the model and feeding it a batch of examples from the training data."]},{"cell_type":"markdown","metadata":{"id":"fUHt40BvzG8q"},"source":["### Problem 2.3: Train the model"]},{"cell_type":"markdown","metadata":{"id":"d0Dayb1GzG8r"},"source":["The training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the [`view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) method."]},{"cell_type":"code","execution_count":102,"metadata":{"id":"XdX4YfgPzG8r"},"outputs":[],"source":["def train_rnn(n_epochs=1, batch_size=100, bptt_len=32, lr=1e-2):\n","    # TODO: Replace the next line with your own code\n","    model = RNNModel(len(wikitext.vocab))\n","    model.to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    train_losses = []\n","    dev_losses = []\n","    dev_accuracies = []\n","    perplexity = []\n","\n","    #print(valid_x[0:batch_size])\n","\n","    with tqdm.tqdm(total = n_epochs) as pbar:\n","      for t in range (n_epochs):\n","        pbar.set_description(f'Epoch{t+1}')\n","        model.train()\n","        running_loss = 0\n","        it = 0\n","        for x, y in batchify(train_x, train_y, batch_size):\n","          optimizer.zero_grad()\n","\n","          output, h_c = model.forward(x)\n","\n","          loss = F.cross_entropy(output.view(x.shape[0]*x.shape[1], -1), y.view(-1))\n","          loss.backward()\n","          optimizer.step()\n","          running_loss += loss.item()* len(x)\n","          it += 1\n","\n","        model.eval()\n","        for x, y in batchify(valid_x, valid_y, batch_size):\n","          with torch.no_grad():\n","                output_valid,h_C = model.forward(x)\n","                loss = F.cross_entropy(output_valid.view(x.shape[0]*x.shape[1], -1), y.view(-1))\n","                dev_losses.append(loss)\n","                \n","        avg_loss = sum(dev_losses)/len(dev_losses)\n","        train_losses.append(running_loss / len(valid_x))\n","        perplexity.append(torch.exp(torch.tensor(avg_loss)).item())\n","        print(\"perp \", perplexity)\n","        print('Avg loss', running_loss/it) \n","        pbar.update()        \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"kI4BGzZvzG8r"},"source":["Here is the specification of the training function:\n","\n","**train_rnn** (*n_epochs* = 1, *batch_size* = 100, *bptt_len* = 32, *lr* = 0.01)\n","\n","> Trains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. The parameter *bptt_len* specifies the length of the backpropagation-through-time horizon, that is, the length of the input and output sequences. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*. After each epoch, prints the perplexity of the model on the validation data."]},{"cell_type":"markdown","metadata":{"id":"y0UvzUyxzG8s"},"source":["Evaluate your model by running the following code cell:"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfhQ7Uy6ArVD","outputId":"4656a837-4adf-4aa2-eb98-a7243270a284"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch1:   0%|          | 0/10 [00:00<?, ?it/s]C:\\Users\\David Ångström\\AppData\\Local\\Temp\\ipykernel_21180\\3041619251.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  perplexity.append(torch.exp(torch.tensor(avg_loss)).item())\n","Epoch2:  10%|█         | 1/10 [00:14<02:10, 14.54s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125]\n","Avg loss 634.7905086477901\n"]},{"name":"stderr","output_type":"stream","text":["Epoch3:  20%|██        | 2/10 [00:31<02:01, 15.14s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875]\n","Avg loss 558.6574141676174\n"]},{"name":"stderr","output_type":"stream","text":["Epoch4:  30%|███       | 3/10 [00:45<01:43, 14.81s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875]\n","Avg loss 533.1300615278539\n"]},{"name":"stderr","output_type":"stream","text":["Epoch5:  40%|████      | 4/10 [00:57<01:24, 14.03s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875]\n","Avg loss 517.1147181451047\n"]},{"name":"stderr","output_type":"stream","text":["Epoch6:  50%|█████     | 5/10 [01:09<01:07, 13.49s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875, 261.8218078613281]\n","Avg loss 505.8182117095585\n"]},{"name":"stderr","output_type":"stream","text":["Epoch7:  60%|██████    | 6/10 [01:22<00:52, 13.25s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875, 261.8218078613281, 261.469482421875]\n","Avg loss 497.1603724076591\n"]},{"name":"stderr","output_type":"stream","text":["Epoch8:  70%|███████   | 7/10 [01:39<00:42, 14.30s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875, 261.8218078613281, 261.469482421875, 262.47869873046875]\n","Avg loss 490.283192257056\n"]},{"name":"stderr","output_type":"stream","text":["Epoch9:  80%|████████  | 8/10 [01:57<00:30, 15.44s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875, 261.8218078613281, 261.469482421875, 262.47869873046875, 264.2890930175781]\n","Avg loss 484.59937842224497\n"]},{"name":"stderr","output_type":"stream","text":["Epoch10:  90%|█████████ | 9/10 [02:12<00:15, 15.51s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875, 261.8218078613281, 261.469482421875, 262.47869873046875, 264.2890930175781, 266.2478942871094]\n","Avg loss 479.7235427735228\n"]},{"name":"stderr","output_type":"stream","text":["Epoch10: 100%|██████████| 10/10 [02:27<00:00, 14.76s/it]"]},{"name":"stdout","output_type":"stream","text":["perp  [300.1768798828125, 279.1668701171875, 268.77215576171875, 263.87908935546875, 261.8218078613281, 261.469482421875, 262.47869873046875, 264.2890930175781, 266.2478942871094, 268.9085388183594]\n","Avg loss 475.5786110299652\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model_rnn = train_rnn(n_epochs=10)"]},{"cell_type":"markdown","metadata":{"id":"vGTlLdeTzG8t"},"source":["**⚠️ Your submitted notebook must contain output demonstrating a validation perplexity of at most 310.**"]},{"cell_type":"markdown","metadata":{"id":"VuIMzlbMArTj"},"source":["## Problem 3: Parameter initialization (reflection)"]},{"cell_type":"markdown","metadata":{"id":"3sfzbdDbzG8u"},"source":["Since the error surfaces that gradient search explores when training neural networks can be very complex, it is important to choose &lsquo;good&rsquo; initial values for the parameters. In PyTorch, the weights of the embedding layer are initialized by sampling from the standard normal distribution $\\mathcal{N}(0, 1)$. Test how changing the standard deviation and/or the distribution affects the perplexity of your feed-forward language model. Write a short report about your experience (ca. 150 words). Use the following prompts:\n","\n","* What different settings for the initialization did you try? What results did you get?\n","* How can you choose a good initialization strategy?\n","* What did you learn? How, exactly, did you learn it? Why does this learning matter?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*TODO: Enter your text here*\n","\n","* What different settings for the initialization did you try? What results did you get? *\n","    We tried changing the uniform distribution to 0.3, -1 and 1 with self.embedding.weight.data.uniform_(0.3, etc.).\n","    0.3: We noted a higher intitial perplexity as well as a higher loss after the first epoch. Although The later epochs balanced this out. Perp: 332\n","    -1 : We noted a much lower perplexity after the first epoch but a pretty similar loss. perp: 301\n","    1  : -||-. Perp: 294\n","* How can you choose a good initialization strategy?\n","\n","    We could initialize the weights with the weights of a pre trained language model, preferably similar to the task we want to solve. These weights could either be frozen or further trained for our specific problem\n","\n","* What did you learn? How, exactly, did you learn it? Why does this learning matter? *\n","\n","    We have learned that vectorizing correctly is important.. We have also learned that overtraining is a thing by observing the perpexity and loss.\n","    We have also developed our understanding of NLP architectures and how to construct such models using the torch library. This knowledge alows us to more easily construct different archtectures for future labs and work in the field."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"vscode":{"interpreter":{"hash":"bc143343ca8435bba8c44b3b1f47f9edcb7f00f13cf7dc8cb9f5e5ffbd446b7a"}}},"nbformat":4,"nbformat_minor":0}
