{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). The code below defines an iterable-style dataset for parser data in the [CoNLL-U format](https://universaldependencies.org/format.html) that the project uses to distribute its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = [Dataset.ROOT]\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append((columns[1], columns[3], int(columns[6])))\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = [Dataset.ROOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('en_ewt-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('en_ewt-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<root>', '<root>', 0),\n",
       " ('They', 'PRON', 4),\n",
       " ('are', 'AUX', 4),\n",
       " ('merely', 'ADV', 4),\n",
       " ('imposters', 'NOUN', 0),\n",
       " ('.', 'PUNCT', 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = list(train_data)[1000]\n",
    "example_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger evaluation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**accuracy** (*tagger*, *gold_data*)\n",
    "\n",
    "> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tagger, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        \n",
    "        nr_words += len(words)\n",
    "\n",
    "        correct_tags = [tokens[1] for tokens in sentence]\n",
    "        predicted_tags = tagger.predict(words)\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            if predicted_tags[i] == correct_tags[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabularies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make_vocabs** (*gold_data*)\n",
    "\n",
    "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over tagged sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    tags = {PAD: 0}\n",
    "    for sentence in gold_data:\n",
    "        for pair in sentence:\n",
    "            word = pair[0]\n",
    "            tag = pair[1]\n",
    "            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            \n",
    "            if tag not in tags:\n",
    "                tags[tag] = len(tags)\n",
    "                    \n",
    "    return vocab, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19676\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "vocab, tags = make_vocabs(train_data)\n",
    "print(len(vocab))\n",
    "print(len(tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input to the network takes the form of a $k$-dimensional vector of word ids and/or tag ids. Each integer $i$ is mapped to an $e_i$-dimensional embedding vector. These vectors are concatenated to form a vector of length $e_1 + \\cdots + e_k$, and sent through a feed-forward network with a single hidden layer and a rectified linear unit (ReLU).\n",
    "\n",
    "#### Default features\n",
    "\n",
    "A fixed-window model with the following features ($k=4$):\n",
    "\n",
    "0. current word\n",
    "1. previous word\n",
    "2. next word\n",
    "3. tag predicted for the previous word\n",
    "\n",
    "Whenever the value of a feature is undefined, the special value `PAD` is used.\n",
    "\n",
    "#### Embedding specifications\n",
    "\n",
    " An embedding specification is a triple $(m, n, e)$ consisting of three integers. Such a triple specifies that the model should include $m$ instances of an embedding from $n$ items to vectors of size $e$. All of the $m$ instances are to share their weights. The embeddings are embeddings for words and tags. For example, to instantiate the default feature model:\n",
    "\n",
    "``\n",
    "[(3, num_words, word_dim), (1, num_tags, tag_dim)]\n",
    "``\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The network architecture introduces a number of hyperparameters. The following choices are reasonable defaults:\n",
    "\n",
    "* width of each word embedding: 50\n",
    "* width of each tag embedding: 10\n",
    "* size of the hidden layer: 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n",
    "\n",
    "> A fixed-window model is initialized with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n",
    "\n",
    "**forward** (*self*, *features*)\n",
    "\n",
    "> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([\n",
    "                        ['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                        ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 1 * 10,\n",
    "\n",
    "        # Create RELU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags \n",
    "        words = features[:,:-1]\n",
    "        tags = features[:,-1]\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 1 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, tag_embs], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger interface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict** (*self*, *sentence*)\n",
    "\n",
    "> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *i*, *pred_tags*)\n",
    "\n",
    "> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n",
    "\n",
    "**predict** (*self*, *words*)\n",
    "\n",
    "> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowTaggerModel(embedding_specs, hidden_dim, len(vocab_tags)).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        feature = []\n",
    "        if len(words) == 1:\n",
    "            feature = [words[i], 0, 0, 0]\n",
    "\n",
    "        elif i == 0: # first word\n",
    "            # Wi, PAD, PAD, PAD\n",
    "            feature = [words[i], words[i+1], 0, 0]\n",
    "        elif i == len(words)-1: # last word\n",
    "            # Wi, Wi+1, PAD, PAD\n",
    "            feature = [words[i], 0, words[i-1], pred_tags[i-1]]\n",
    "        else:\n",
    "            # Wi, Wi+1, Wi-1, Ti-1\n",
    "            feature = [words[i], words[i+1], words[i-1], pred_tags[i-1]]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if not word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "\n",
    "        # predict tags\n",
    "        pred_tags_idxs = [0] * len(words)\n",
    "        for i in range(0, len(words_idxs)):\n",
    "            feature = self.featurize(words_idxs, i, pred_tags_idxs)\n",
    "            pred_tags = self.model.forward(feature)\n",
    "            # Find tag index with highest probability\n",
    "            pred_tags_idxs[i] = torch.argmax(pred_tags).item()\n",
    "        \n",
    "        # convert tag indexes\n",
    "        pred_tags = []\n",
    "        for tag_idx in pred_tags_idxs:\n",
    "            tag = [k for k, v in self.vocab_tags.items() if v == tag_idx][0]\n",
    "            pred_tags.append(tag)\n",
    "        \n",
    "        return pred_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training examples for the Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training_examples_tagger** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of tagged sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_tagger(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
    "    batch = []\n",
    "    gold_label = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "\n",
    "        for word, tag, _ in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "\n",
    "        for i in range(0, len(all_words_idx)):\n",
    "            batch.append(tagger.featurize(all_words_idx, i, all_tags_idx))\n",
    "            gold_label.append(all_tags_idx[i])\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 4).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(gold_label).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                gold_label = []\n",
    "\n",
    "        # Yield remaining batch\n",
    "        if sentence_idx == len(list(gold_data))-1:\n",
    "            remainder = len(batch)\n",
    "            batch_tensor = torch.Tensor(remainder, 4).long().to(device)\n",
    "            bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "            by = torch.Tensor(gold_label).long().to(device)\n",
    "            yield bx, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_fixed_window_tagger** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over tagged sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_tagger(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "    tagger.model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        nr_iterations += len(words)\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                batch = 0\n",
    "                tagger.model.train()\n",
    "                for bx, by in training_examples_tagger(vocab_words, vocab_tags, train_data, tagger, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = tagger.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(loss.item()), batch=batch+1)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "tagger.model = torch.load('tagger_model', map_location=device)\n",
    "print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger = train_fixed_window_tagger(train_data)\n",
    "# vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "# tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "# print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predicted part-of-speech tags dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tagger to create predicted part-of-speech tags dataset for parser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedDataset():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = []\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append(columns)\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-train-projectivized-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "    for sentence in TaggedDataset('en_ewt-ud-train-projectivized.conllu'):\n",
    "        words = [columns[1] for columns in sentence]\n",
    "        for i, t in enumerate(tagger.predict(words)):\n",
    "            sentence[i][3] = t\n",
    "        for columns in sentence:\n",
    "            print('\\t'.join(c for c in columns), file=target)\n",
    "        print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-dev-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "    for sentence in TaggedDataset('en_ewt-ud-dev.conllu'):\n",
    "        words = [columns[1] for columns in sentence]\n",
    "        for i, t in enumerate(tagger.predict(words)):\n",
    "            sentence[i][3] = t\n",
    "        for columns in sentence:\n",
    "            print('\\t'.join(c for c in columns), file=target)\n",
    "        print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_retagged = Dataset('en_ewt-ud-train-projectivized-retagged.conllu')\n",
    "dev_data_retagged = Dataset('en_ewt-ud-dev-retagged.conllu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser evaluation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**uas** (*parser*, *gold_data*)\n",
    "\n",
    "> Computes the unlabelled attachment score of the specified *parser* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. The unlabelled attachment score is the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). The calculation excludes the pseudo-roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        tags = [tokens[1] for tokens in sentence]\n",
    "        # Do not include pseudo-root\n",
    "        nr_words += (len(words) - 1)\n",
    "\n",
    "        correct_head = [tokens[2] for tokens in sentence]\n",
    "        predicted_head = parser.predict(words, tags)\n",
    "\n",
    "        # skip pseudo-root\n",
    "        for i in range(1, len(words)):\n",
    "            if predicted_head[i] == correct_head[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The arc-hybrid algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcHybridParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return (0, [], [0] * num_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        valid_moves = []\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            valid_moves.append(ArcHybridParser.SH)\n",
    "        if len(stack) > 1 and buffer < len(config[2]):\n",
    "            valid_moves.append(ArcHybridParser.LA)\n",
    "        if len(stack) > 1:\n",
    "            valid_moves.append(ArcHybridParser.RA)\n",
    "        return valid_moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        buffer, stack, heads = config\n",
    "        # SHIFT\n",
    "        if move == ArcHybridParser.SH:\n",
    "            stack.append(buffer)\n",
    "            buffer += 1\n",
    "        # LEFT ARC\n",
    "        elif move == ArcHybridParser.LA:\n",
    "            heads[stack[-1]] = buffer\n",
    "            stack = stack[:-1]\n",
    "        # RIGHT ARC\n",
    "        elif move == ArcHybridParser.RA:\n",
    "            heads[stack[-1]] = stack[-2]\n",
    "            stack = stack[:-1]\n",
    "            \n",
    "        return (buffer, stack, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        buffer, stack, heads = config\n",
    "        return buffer == len(heads) and len(stack) == 1 and stack[0] == 0\n",
    "    \n",
    "    # Buffer = 0\n",
    "    # stack = 1\n",
    "    # heads = 2\n",
    "    @staticmethod\n",
    "    def zero_cost_shift(current_config, gold_config):\n",
    "        if current_config[0] == len(current_config[2]):\n",
    "            return False\n",
    "        if len(current_config[1]) == 0:\n",
    "            return True\n",
    "        item = current_config[0]\n",
    "\n",
    "        # Shift 1\n",
    "        if item in gold_config:\n",
    "            for d in current_config[1][0:-1]:\n",
    "                if item == gold_config[d]:\n",
    "                    return False\n",
    "\n",
    "        # Shift 2\n",
    "        for h in current_config[1][0:-1]:\n",
    "            if h == gold_config[item]:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    # Buffer = 0\n",
    "    # stack = 1\n",
    "    # heads = 2\n",
    "    @staticmethod\n",
    "    def zero_cost_la(current_config, gold_config):\n",
    "        s0 = current_config[1][-1]\n",
    "        if len(current_config[1]) < 2:\n",
    "            s1 = None\n",
    "        else:\n",
    "            s1 = current_config[1][-2]\n",
    "\n",
    "        # LA 1\n",
    "        for buffer_item in range(current_config[0],len(current_config[2])):\n",
    "            if s0 == gold_config[buffer_item]:\n",
    "                return False \n",
    "    \n",
    "        # LA 2\n",
    "        if s1 == gold_config[s0]:\n",
    "            return False\n",
    "         \n",
    "        # LA 3\n",
    "        for buffer_item in range(current_config[0]+1,len(current_config[2])):\n",
    "            if buffer_item == gold_config[s0]:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    # Buffer = 0\n",
    "    # stack = 1\n",
    "    # heads = 2\n",
    "    @staticmethod\n",
    "    def zero_cost_ra(current_config, gold_config):\n",
    "        s0 = current_config[1][-1]\n",
    "\n",
    "        # 1\n",
    "        for buffer_item in range(current_config[0],len(current_config[2])):\n",
    "            if s0 == gold_config[buffer_item]:\n",
    "                return False\n",
    "\n",
    "        # 2\n",
    "        for buffer_item in range(current_config[0],len(current_config[2])):\n",
    "            if buffer_item == gold_config[s0]:\n",
    "                return False\n",
    "        return True\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dynamic oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT, LA, RA = 0,1,2\n",
    "def dynamic_oracle(gold_config, current_config, legal_transition,parser):\n",
    "    moves = []\n",
    "    if SHIFT in legal_transition and parser.zero_cost_shift(current_config,gold_config):\n",
    "        moves.append(SHIFT)\n",
    "    if LA in legal_transition and parser.zero_cost_la(current_config,gold_config):\n",
    "        moves.append(LA)\n",
    "    if RA in legal_transition and parser.zero_cost_ra(current_config,gold_config):\n",
    "        moves.append(RA)\n",
    "    return moves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                                         ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 6 * 50 + 6 * 10,\n",
    "\n",
    "        # Create ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags for buffer 1, 2, 3, stack 1, 2, 3\n",
    "        words, tags = torch.split(features, 12, dim=1)\n",
    "        \n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 6 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 6 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        concat_tags = tag_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, concat_tags], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcHybridParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        num_moves = len(ArcHybridParser.MOVES)\n",
    "        embedding_specs = [(12, len(vocab_words), word_dim), (12, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowParserModel(embedding_specs, hidden_dim, num_moves)#.to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, tags, gold_heads, config):\n",
    "        buffer, stack, heads = config\n",
    "    \n",
    "        s0_w = self.vocab_words[PAD]\n",
    "        s0_t = self.vocab_tags[PAD]\n",
    "        s1_w = self.vocab_words[PAD]\n",
    "        s1_t = self.vocab_tags[PAD]\n",
    "        s2_w = self.vocab_words[PAD]\n",
    "        s2_t = self.vocab_tags[PAD]\n",
    "\n",
    "        b0_w = self.vocab_words[PAD]\n",
    "        b0_t = self.vocab_tags[PAD]\n",
    "        b1_w = self.vocab_words[PAD]\n",
    "        b1_t = self.vocab_tags[PAD]\n",
    "        b2_w = self.vocab_words[PAD]\n",
    "        b2_t = self.vocab_tags[PAD]\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            b0_w = words[buffer]\n",
    "            b0_t = tags[buffer]\n",
    "            if buffer + 1 < len(heads):\n",
    "                b1_w = words[buffer + 1]\n",
    "                b1_t = tags[buffer + 1]\n",
    "                if buffer + 2 < len(heads):\n",
    "                    b2_w = words[buffer + 2]\n",
    "                    b2_t = tags[buffer + 2]\n",
    "        \n",
    "        if len(stack) >= 1:\n",
    "            s0_w = words[stack[-1]]\n",
    "            s0_t = tags[stack[-1]]\n",
    "            if len(stack) >= 2:\n",
    "                s1_w = words[stack[-2]]\n",
    "                s1_t = tags[stack[-2]]\n",
    "                if len(stack) >= 3:\n",
    "                    s2_w = words[stack[-3]]\n",
    "                    s2_t = tags[stack[-3]]\n",
    "        \n",
    "        s0_b1_w = self.vocab_tags[PAD]\n",
    "        s0_b2_w = self.vocab_tags[PAD]\n",
    "        s0_b1_t = self.vocab_tags[PAD]\n",
    "        s0_b2_t = self.vocab_tags[PAD]\n",
    "        for idx, head in enumerate(gold_heads[0:s0_w]):\n",
    "            if head == s0_w and s0_b1_w == self.vocab_tags[PAD]:\n",
    "                s0_b1_w = words[idx]\n",
    "                s0_b1_t = tags[idx]\n",
    "            if head == s0_w and s0_b2_w == self.vocab_tags[PAD]:\n",
    "                s0_b2_w = words[idx]\n",
    "                s0_b2_t = tags[idx]\n",
    "\n",
    "\n",
    "        s0_f1_w = self.vocab_tags[PAD]\n",
    "        s0_f2_w = self.vocab_tags[PAD]\n",
    "        s0_f1_t = self.vocab_tags[PAD]\n",
    "        s0_f2_t = self.vocab_tags[PAD]\n",
    "        if len(stack) >= 1:\n",
    "            for idx, head in enumerate(gold_heads[s0_w:]):\n",
    "                if head == s0_w and s0_f1_w == self.vocab_tags[PAD]:\n",
    "                    s0_f1_w = words[idx]\n",
    "                    s0_f1_t = tags[idx]\n",
    "                if head == s0_w and s0_f2_w == self.vocab_tags[PAD]:\n",
    "                    s0_f2_w = tags[idx]\n",
    "                    s0_f2_t = tags[idx]\n",
    "\n",
    "\n",
    "        n0_b1_w = self.vocab_tags[PAD]\n",
    "        n0_b2_w = self.vocab_tags[PAD]\n",
    "        n0_b1_t = self.vocab_tags[PAD]\n",
    "        n0_b2_t = self.vocab_tags[PAD]\n",
    "        for idx, head in enumerate(gold_heads[0:b0_w]):\n",
    "            if head == b0_w and n0_b1_w == self.vocab_tags[PAD]:\n",
    "                n0_b1_w = words[idx]\n",
    "                n0_b1_t = tags[idx]\n",
    "            if head == b0_w and n0_b2_w == self.vocab_tags[PAD]:\n",
    "                n0_b2_w = words[idx]\n",
    "                n0_b2_t = tags[idx]\n",
    "\n",
    "\n",
    "        feature = [b0_w, b1_w, b2_w, s0_w, s1_w, s2_w, b0_t, b1_t, b2_t, s0_t, s1_t, s2_t,\n",
    "                   s0_b1_w, s0_b2_w, s0_f1_w, s0_f2_w, n0_b1_w, n0_b2_w,\n",
    "                   s0_b1_t, s0_b2_t, s0_f1_t, s0_f2_t, n0_b1_t, n0_b2_t]\n",
    "        return torch.tensor([feature])#.to(device)\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "\n",
    "        # find tag indexes for given tags\n",
    "        tags_idxs = []\n",
    "        for tag in tags:\n",
    "            if tag in self.vocab_tags:\n",
    "                tags_idxs.append(self.vocab_tags[tag])\n",
    "            else:\n",
    "                tags_idxs.append(self.vocab_tags[PAD])\n",
    "\n",
    "        config = self.initial_config(len(words))\n",
    "\n",
    "        while not self.is_final_config(config):\n",
    "            valid_moves = self.valid_moves(config)\n",
    "            feature = self.featurize(words_idxs, tags_idxs, config)\n",
    "            pred_moves = self.model.forward(feature)\n",
    "            _, sorted_indexes = torch.sort(pred_moves, descending=True)\n",
    "            # find valid move with highest score (SH, LA, RA)\n",
    "            if len(valid_moves) > 0:\n",
    "                sorted_move_list = sorted_indexes.tolist()[0]\n",
    "                # choose first valid move as default move\n",
    "                new_move = valid_moves[0]\n",
    "                for move in sorted_move_list:\n",
    "                    if move in valid_moves:\n",
    "                        new_move = move\n",
    "                        break\n",
    "                config = self.next_config(config, new_move)\n",
    "\n",
    "        return config[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_fixed_window_parser** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window parser from a set of training data *train_data* (an iterable over parsed sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_move(scores, legal_transitions):\n",
    "    _, sorted_indexes = torch.sort(scores, descending=True)\n",
    "    # find valid move with highest score (SH, LA, RA)\n",
    "    if len(legal_transitions) > 0:\n",
    "        sorted_move_list = sorted_indexes.tolist()[0]\n",
    "        # choose first valid move as default move\n",
    "        t_p = legal_transitions[0]\n",
    "        for move in sorted_move_list:\n",
    "            if move in legal_transitions:\n",
    "                return t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "import random\n",
    "def train_fixed_window_parser(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "    arc_parser = ArcHybridParser()\n",
    "\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        nr_iterations += 1\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                total_moves = 1\n",
    "                train_loss = 0\n",
    "                batch_loss = []\n",
    "                batch_iter = 0\n",
    "\n",
    "                parser.model.train()\n",
    "                for sentence in train_data:\n",
    "                    all_words_idx = []\n",
    "                    all_tags_idx = []\n",
    "                    all_heads = []\n",
    "\n",
    "                    for word, tag, head in sentence:\n",
    "                        all_words_idx.append(vocab_words[word])\n",
    "                        all_tags_idx.append(vocab_tags[tag])\n",
    "                        all_heads.append(head)\n",
    "                    \n",
    "                    config = arc_parser.initial_config(len(all_heads))\n",
    "\n",
    "                    while not arc_parser.is_final_config(config):\n",
    "                        # Get all legal moves\n",
    "                        legal_transitions = arc_parser.valid_moves(config)\n",
    "\n",
    "                        # Compute which move should be taken next\n",
    "                        scores = parser.model.forward(parser.featurize(all_words_idx, all_tags_idx, all_heads, config))#.to(device)\n",
    "\n",
    "                        # Get legal move with highest probability\n",
    "                        print(scores)\n",
    "                        t_p = find_highest_move(scores, legal_transitions)\n",
    "\n",
    "                        # Extract scores to list\n",
    "                        scores_list = scores.tolist()[0]\n",
    "\n",
    "                        # Compute which moves are zero cost\n",
    "                        zero_cost_moves = dynamic_oracle(all_heads, config, legal_transitions, arc_parser)\n",
    "                        \n",
    "                        # Get the best legal zero cost move\n",
    "                        t_o = max(zero_cost_moves, key=lambda p: scores_list[p])\n",
    "                        # Target vector    \n",
    "                        y = torch.tensor([t_o]).long()#.to(device)\n",
    "\n",
    "                        loss = F.cross_entropy(scores, y)\n",
    "                        batch_loss.append(loss)\n",
    "                        train_loss += loss.item()\n",
    "\n",
    "                        # If predicted transition is not in the zero cost moves, update weights.\n",
    "                        if t_p not in zero_cost_moves:\n",
    "                            # choose random transition from zero_cost. Might be bad move but such is life.\n",
    "                            config = parser.next_config(config, random.choice(zero_cost_moves))\n",
    "                        else:\n",
    "                            config = parser.next_config(config, t_p)\n",
    "\n",
    "                        pbar.set_postfix(loss=(train_loss/total_moves), configs=total_moves)\n",
    "                        total_moves += 1\n",
    "                        batch_iter += 1\n",
    "\n",
    "                        # Update the parameters\n",
    "                        if len(batch_loss) > 0 and batch_iter >= batch_size:\n",
    "                            optimizer.zero_grad()\n",
    "                            loss = sum(batch_loss)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            batch_loss = []\n",
    "                            batch_iter = 0\n",
    "                    \n",
    "                    #if batch_iter == 10:\n",
    "                    #    break\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12544 [00:00<?, ?it/s, configs=37, loss=1.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.0554,  0.1123, -0.1961]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.0604,  0.2901, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.1527,  0.0157, -0.1465]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.2377, 0.2444, 0.0611]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[-0.0296,  0.1707, -0.1292]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 5, 5]])\n",
      "tensor([[ 0.0789, -0.0037,  0.0961]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 5, 5]])\n",
      "tensor([[0.0847, 0.1524, 0.0128]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[7, 7, 0, 0, 8, 8, 5, 5, 0, 0, 4, 4]])\n",
      "tensor([[0.0762, 0.0167, 0.2200]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[8, 8, 3, 2, 0, 0, 4, 4, 2, 2, 0, 0]])\n",
      "tensor([[-0.1021,  0.2795, -0.1556]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 3, 2, 0, 0, 0, 0, 2, 2, 0, 0]])\n",
      "tensor([[ 0.1219,  0.1201, -0.1666]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[8, 8, 3, 2, 0, 0, 4, 4, 2, 2, 0, 0]])\n",
      "tensor([[ 0.1200,  0.2746, -0.2969]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[7, 7, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0]])\n",
      "tensor([[ 0.1061,  0.1367, -0.0426]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.1073,  0.0544, -0.1018]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.6042, 0.2141, 0.2032]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.1540, 0.3396, 0.0642]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.2282, -0.0149, -0.0675]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.3019,  0.2595, -0.0086]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.3353,  0.0989, -0.1781]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.2329, 0.3131, 0.0329]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.2608, 0.2378, 0.0530]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.1183, 0.3476, 0.2355]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.1370, -0.0465,  0.0929]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 0.0008, -0.0627,  0.2910]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0,  0,  0,  0, 15, 15,  0,  0,  0,  0,  7,  7]])\n",
      "tensor([[ 0.1539,  0.1048, -0.2666]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[15, 15,  5,  4,  0,  0,  7,  7,  4,  4,  0,  0]])\n",
      "tensor([[ 0.2546,  0.1709, -0.0228]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.1529, 0.2517, 0.0258]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.1828, 0.2987, 0.3008]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[0.0546, 0.0911, 0.0231]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0,  0,  0,  0, 15, 15,  0,  0,  0,  0,  7,  7]])\n",
      "tensor([[ 0.2600,  0.1309, -0.3252]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[15, 15,  5,  4, 17, 17,  7,  7,  4,  4,  8,  8]])\n",
      "tensor([[-0.1032,  0.4787,  0.0871]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0,  0,  0,  0, 17, 17,  0,  0,  0,  0,  8,  8]])\n",
      "tensor([[-0.0106,  0.4896, -0.1051]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0,  0,  0,  0, 17, 17,  0,  0,  0,  0,  8,  8]])\n",
      "tensor([[0.0772, 0.1730, 0.0577]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[17, 17,  5,  4,  0,  0,  8,  8,  4,  4,  0,  0]])\n",
      "tensor([[-0.0848, -0.0560,  0.0015]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0,  0,  0,  0, 15, 15,  0,  0,  0,  0,  7,  7]])\n",
      "tensor([[-0.0471,  0.0227,  0.0121]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[15, 15,  5,  4,  0,  0,  7,  7,  4,  4,  0,  0]])\n",
      "tensor([[-0.2700,  0.2816,  0.3255]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[-0.0009,  0.4892,  0.3539]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[17, 17,  5,  4,  0,  0,  8,  8,  4,  4,  0,  0]])\n",
      "tensor([[0.0312, 0.0911, 0.0420]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0,  0,  0,  0, 19, 19,  0,  0,  0,  0,  8,  8]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parser \u001b[39m=\u001b[39m train_fixed_window_parser(train_data_retagged, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[52], line 43\u001b[0m, in \u001b[0;36mtrain_fixed_window_parser\u001b[1;34m(train_data, n_epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     40\u001b[0m legal_transitions \u001b[39m=\u001b[39m arc_parser\u001b[39m.\u001b[39mvalid_moves(config)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Compute which move should be taken next\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m scores \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(parser\u001b[39m.\u001b[39;49mfeaturize(all_words_idx, all_tags_idx, all_heads, config))\u001b[39m#.to(device)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# Get legal move with highest probability\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mprint\u001b[39m(scores)\n",
      "Cell \u001b[1;32mIn[49], line 39\u001b[0m, in \u001b[0;36mFixedWindowParserModel.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Get the word and tag embeddings\u001b[39;00m\n\u001b[0;32m     38\u001b[0m word_embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings[\u001b[39m'\u001b[39m\u001b[39mword_embs\u001b[39m\u001b[39m'\u001b[39m](words) \u001b[39m# 6 * 50\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m tag_embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings[\u001b[39m'\u001b[39;49m\u001b[39mtag_embs\u001b[39;49m\u001b[39m'\u001b[39;49m](tags) \u001b[39m# 6 * 10\u001b[39;00m\n\u001b[0;32m     41\u001b[0m concat_words \u001b[39m=\u001b[39m word_embs\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m concat_tags \u001b[39m=\u001b[39m tag_embs\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\David ngstrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\David ngstrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\David ngstrm\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "parser = train_fixed_window_parser(train_data_retagged, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(uas(parser, dev_data_retagged)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "print('{:.4f}'.format(uas(parser, dev_data_retagged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc143343ca8435bba8c44b3b1f47f9edcb7f00f13cf7dc8cb9f5e5ffbd446b7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
