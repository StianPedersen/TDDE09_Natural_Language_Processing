{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psYARLpkfBjF"
   },
   "source": [
    "# Lab L3X: BERT for Natural Language Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXTrlu1-fIri"
   },
   "source": [
    "One of the main selling points of pre-trained language models is that they can be applied to a wide spectrum of different tasks in natural language processing. In this lab you will test this by fine-tuning a pre-trained BERT model on a benchmark task in natural language inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this lab, you will need a computer with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWurVptwtu8M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXTrlu1-fIri"
   },
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXTrlu1-fIri"
   },
   "source": [
    "The data for this lab is the [SNLI corpus](https://nlp.stanford.edu/projects/snli/), a collection of 570k human-written English image caption pairs manually labeled with the labels *Entailment*, *Contradiction*, and *Neutral*. Consider the following sentence pair as an example:\n",
    "\n",
    "* Sentence 1: A soccer game with multiple males playing.\n",
    "* Sentence 2: Some men are playing a sport.\n",
    "\n",
    "This pair is labeled with *Entailment*, because sentence&nbsp;2 is logically entailed (implied) by sentence&nbsp;1 ‚Äì if sentence&nbsp;1 is true, then sentence&nbsp;2 is true, too. The following sentence pair, on the other hand, is labeled with *Contradiction*, because both sentences cannot be true at the same time.\n",
    "\n",
    "* Sentence 1: A black race car starts up in front of a crowd of people.\n",
    "* Sentence 2: A man is driving down a lonely road.\n",
    "\n",
    "For detailed information about the corpus and how it was constructed, refer to [Bowman et al. (2015)](https://www.aclweb.org/anthology/D15-1075/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a custom [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-Ig6JpHtosH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SNLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, max_size=None):\n",
    "        super().__init__()\n",
    "        self.xs = []\n",
    "        self.ys = []\n",
    "        with open(filename) as source:\n",
    "            for i, line in enumerate(source):\n",
    "                if max_size and i >= max_size:\n",
    "                    break\n",
    "                sentence1, sentence2, gold_label = line.rstrip().split('\\t')\n",
    "                self.xs.append((sentence1, sentence2))\n",
    "                self.ys.append(['contradiction', 'entailment', 'neutral'].index(gold_label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training portion and the development portion of the dataset. For starters, we only load the first 1k sentence pairs from the training data. You will later need increase the maximal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p79Ta5QTtosI"
   },
   "outputs": [],
   "source": [
    "train_dataset = SNLIDataset('snli_1.0_train_preprocessed.txt', max_size=1000)\n",
    "test_dataset = SNLIDataset('snli_1.0_test_preprocessed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows an example from the training data. The labels *Contradiction*, *Entailment*, and *Neutral* are mapped to the integers 0‚Äì2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4h9Ep3StosJ",
    "outputId": "28c01fc7-04fe-470e-c972-e0bff22725a8"
   },
   "outputs": [],
   "source": [
    "train_dataset[120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this lab is to fine-tune a pre-trained BERT model on the SNLI training data, and evaluate the performance of the fine-tuned model on the test data. Pre-trained BERT models and standard architectures are available in the [Hugging Face Transformers library](https://huggingface.co/transformers/model_doc/bert.html). You will need to read the relevant parts of the documentation of that library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xZ3x_MQvYuK",
    "outputId": "d359707f-4f79-4192-ace1-5e740733ed1f"
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line to install the transformers library:\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need two classes from the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xZ3x_MQvYuK",
    "outputId": "d359707f-4f79-4192-ace1-5e740733ed1f"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `BertTokenizer` is in charge of preparing the inputs to a BERT model. This involves the tokenisation and encoding of the resulting word pieces into integers from the vocabulary. The `BertForSequenceClassification` architecture extends the basic BERT architecture with a linear layer on top of the pooled, token-specific output. You should instantiate both classes with the pre-trained `bert-base-uncased` model. (We have preprocessed the data for this lab by lowercasing.)\n",
    "\n",
    "Here is the basic recipe for this lab:\n",
    "\n",
    "1. Use the `BertTokenizer` to convert the data into a tensorised form.\n",
    "2. Train a `BertForSequenceClassification` model on the tensorised data.\n",
    "3. Evaluate the trained model by computing its accuracy on the test data.\n",
    "\n",
    "Submit your final notebook. Include a short (ca. 150&nbsp;words) report about your experience. Compare your results to the one by [Bowman et al. (2015)](https://www.aclweb.org/anthology/D15-1075/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è Your submitted notebook must contain output demonstrating a higher accuracy than the best model of Bowman et al. (2015).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí°Tips\n",
    "\n",
    "* You can simplify things by using a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) with a suitable `collate_fn`.\n",
    "* Train for 1&nbsp;epoch using a batch size of 32 and a learning rate of 1e-5.\n",
    "* You will need to train on approximately 40k instances to reach the performance goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Your report here*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-L5X.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
