{"cells":[{"cell_type":"markdown","metadata":{"id":"-4gR1jRrsCjr"},"source":["In this lab you will implement the **skip-gram model with negative sampling (SGNS)** from Lecture&nbsp;1.4, and use it to train word embeddings on the text of the [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page).\n","\n","⚠️ The dataset for this lab contains 18M tokens. This is very little as far as word embedding datasets are concerned – for example, the original word2vec model was pre-trained on 100B tokens. In spite of this, you will need to think about efficiency when processing the data and training your models. In particular, wherever possible you should use iterators rather than lists, and vectorize operations (using [NumPy](https://numpy.org) or [PyTorch](https://pytorch.org)) as much as possible."]},{"cell_type":"markdown","metadata":{"id":"8sFFh756sCjo"},"source":["# L1: Word representations"]},{"cell_type":"markdown","metadata":{"id":"rxhQ138ssCjr"},"source":["## Load the data"]},{"cell_type":"markdown","metadata":{"id":"N--U6B5_sCjs"},"source":["The data for this lab comes as a bz2-compressed plain text file. It consists of 1,163,769 sentences, with one sentence per line and tokens separated by spaces. The cell below contains a wrapper class `SimpleWikiDataset` that can be used to iterate over the sentences (lines) in the text file. On the Python side of things, each sentence is represented as a list of tokens (strings)."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlgoBtvTxt24","outputId":"18e93500-4966-43d3-a31b-162fa4228993","executionInfo":{"status":"ok","timestamp":1674997051366,"user_tz":-60,"elapsed":2072,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-0KRMAUxsCjt","executionInfo":{"status":"ok","timestamp":1674997051366,"user_tz":-60,"elapsed":5,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["import bz2\n","\n","class SimpleWikiDataset():\n","    \n","    def __init__(self, max_sentences=None):\n","        self.max_sentences = max_sentences\n","    \n","    def __iter__(self):\n","        with bz2.open('/content/drive/MyDrive/TDDE09/labs/l1/simplewiki.txt.bz2', 'rt', encoding='utf-8') as sentences: # unzip manually if it is not working\n","            for i, sentence in enumerate(sentences):\n","                if self.max_sentences and i >= self.max_sentences:\n","                    break\n","                yield sentence.split()"]},{"cell_type":"markdown","metadata":{"id":"7dktPj1esCju"},"source":["Using this class, we define two variants of the dataset: the full dataset and a minimal version with the first 1% of the sentences in the full dataset. The latter will be useful to test code without running it on the full dataset."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KRxWDQ9csCju","executionInfo":{"status":"ok","timestamp":1674997051367,"user_tz":-60,"elapsed":5,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["# Dataset with all sentences (N = 1,163,769)\n","full_dataset = SimpleWikiDataset()\n","\n","# Minimal dataset\n","mini_dataset = SimpleWikiDataset(max_sentences=11638)\n","\n","CURR_DATASET = full_dataset"]},{"cell_type":"markdown","metadata":{"id":"8EvEVP5isCju"},"source":["The next code cell defines a generator function that allows you to iterate over all tokens in a dataset:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"XGyw-_IGsCjv","executionInfo":{"status":"ok","timestamp":1674997051367,"user_tz":-60,"elapsed":5,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["def tokens(sentences):\n","    for sentence in sentences:\n","        for token in sentence:\n","            yield token"]},{"cell_type":"markdown","metadata":{"id":"yO3NIrRysCjv"},"source":["To illustrate how to use this function, here is code that prints the number of tokens in the full dataset:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNfAHvBIsCjv","outputId":"00c8bd72-d599-4728-ae2c-98e7eab3ede5","executionInfo":{"status":"ok","timestamp":1674997060123,"user_tz":-60,"elapsed":8760,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["17594885\n"]}],"source":["print(sum(1 for t in tokens(full_dataset)))"]},{"cell_type":"markdown","metadata":{"id":"D99qEce2sCjw"},"source":["## Problem 1: Build the vocabulary and frequency table"]},{"cell_type":"markdown","metadata":{"id":"XDk8E7scsCjw"},"source":["Your first task is to construct the embedding **vocabulary** – the set of unique words that will receive an embedding. Because you will eventually need to map words to vector dimensions, you will represent the vocabulary as a dictionary that maps words (strings) to a contiguous range of integers.\n","\n","Along with the vocabulary, you will also construct the **frequency table**, that is, the table that holds the absolute frequencies (counts) in the data, for all words in your vocabulary. This will simply be an array of integers, indexed by the word ids in the vocabulary."]},{"cell_type":"markdown","metadata":{"id":"f9EiN6JxsCjw"},"source":["To construct the vocabulary and the frequency table, complete the skeleton code in the cell below:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"aC5cSjSAsCjw","executionInfo":{"status":"ok","timestamp":1674997060123,"user_tz":-60,"elapsed":11,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["import numpy as np\n","\n","def make_vocab_and_counts(sentences, min_count=5):\n","    vocab = {}        # The vocabulary to be returned.\n","    counts = {}       # For simple count of all words without index.\n","    freq_table = []   # The frequenct table to be returned. Converted to np array later.\n","\n","    for sentence in sentences:\n","      for token in sentence:\n","        if token not in counts:\n","          counts[token] = 1\n","        else:\n","          counts[token] += 1\n","\n","    for sentence in sentences: \n","      for token in sentence:\n","        idx = len(vocab)\n","        if token not in vocab and counts[token] >= min_count:\n","          vocab[token] = idx\n","          freq_table.append(counts[token])\n","\n","    return vocab, np.array(freq_table)"]},{"cell_type":"markdown","metadata":{"id":"rjHi9NznsCjx"},"source":["Your code should comply with the following specification:\n","\n","**make_vocab_and_counts** (*sentences*, *min_count* = 5)\n","\n","> Reads from an iterable of *sentences* (lists of string tokens) and returns a pair *vocab*, *counts* where *vocab* is a dictionary representing the vocabulary and *counts* is a 1D-[ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) with the absolute frequencies (counts) of the words in the vocabulary. The dictionary *vocab* maps words to a contiguous range of integers starting at&nbsp;0. In the *counts* array, the entry at index $i$ is the count of that word in *vocab* which maps to $i$. Words that occur less than *min_count* times are excluded from the vocabulary."]},{"cell_type":"markdown","metadata":{"id":"9n-s9GwzsCjx"},"source":["### 🤞 Test your code\n","\n","To test your code, print the sizes of the vocabularies constructed from the two datasets, as well as the count totals. The correct vocabulary size for the minimal dataset is 3,231; for the full dataset, the correct vocabulary size is 73,339. The correct totals are 155,818 for the minimal dataset and 17,297,355 for the full dataset."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsxIdhgNsCjx","outputId":"7aabc4fe-82b2-483b-fb2d-fd64195c8d7c","executionInfo":{"status":"ok","timestamp":1674997081649,"user_tz":-60,"elapsed":21536,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","73339\n","73339\n","17297355\n"]}],"source":["# TODO: Test your code here\n","vocab, freq_table = make_vocab_and_counts(CURR_DATASET)\n","print(vocab['usually'])\n","print(len(freq_table))\n","print(len(vocab))\n","#print(vocab)\n","print(sum(freq_table)) # Correct!! :)"]},{"cell_type":"markdown","metadata":{"id":"vRHAJYp4sCjx"},"source":["## Problem 2: Preprocess the data"]},{"cell_type":"markdown","metadata":{"id":"zkNikQSVsCjx"},"source":["Your next task is to preprocess the training data. This involves the following:\n","\n","* Discard words that are not in the vocabulary\n","* Map each word to its vocabulary id\n","* Randomly discard words according to the subsampling strategy covered in Lecture&nbsp;1.4\n","* Discard sentences that have become empty\n","\n","As a reminder, the subsampling strategy involves discarding tokens $w$ with probability\n","\n","$$\n","P(w) = \\max (0, 1-\\sqrt{tN/\\#(w)})\n","$$\n","\n","where $\\#(w)$ is the count of $w$, $N$ is the total number of counts, and $t$ is the chosen threshold (default value: 0.001)."]},{"cell_type":"markdown","metadata":{"id":"09_qobvSsCjx"},"source":["The cell below contains skeleton code for a generator function `preprocess`:"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"PToHfHMYsCjy","executionInfo":{"status":"ok","timestamp":1674997438177,"user_tz":-60,"elapsed":254,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["import torch\n","import math\n","\n","def discard(N, dist, w, t = 0.001):\n","  pw = max(0, 1 - math.sqrt(t*N / w))\n","  \n","  if pw >= np.random.rand(1):\n","    return True\n","\n","def preprocess(vocab, counts, sentences, threshold=0.001):\n","    # TODO: Replace the next line with your own code\n","    N = sum(counts)\n","    cumulative_sums = torch.cumsum(torch.from_numpy(counts), dim=0)\n","\n","    for sentence in sentences:\n","      list_of_ids = []\n","      for word in sentence:\n","        if word in vocab and not discard(N, cumulative_sums, counts[vocab[word]], threshold):          \n","          list_of_ids.append(vocab[word])\n","\n","      if list_of_ids:\n","        yield list_of_ids\n","# Worst case for discarding randomly is that the whole sentence is gone, need to be implemented?"]},{"cell_type":"markdown","metadata":{"id":"dcstqclUsCjy"},"source":["Extend this skeleton code into a function that implements the preprocessing. Your code should comply with the following specification:\n","\n","**preprocess** (*vocab*, *counts*, *sentences*, *threshold* = 0.001)\n","\n","> Reads from an iterable of *sentences* (lists of string tokens) and yields the preprocessed sentences as non-empty lists of word ids (integers). Words not in *vocab* are discarded. The remaining words are randomly discarded according to the subsampling strategy with the given *threshold*. In the non-empty sentences, each token is replaced by its id in the vocabulary.\n","\n","**⚠️ Please observe** that your function should *yield* the preprocessed sentences, not return a list with all of them. That is, we ask you to write a *generator function*. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"]},{"cell_type":"markdown","metadata":{"id":"0YBvapfxsCjy"},"source":["### 🤞 Test your code\n","\n","Test your code by comparing the total number of tokens in the preprocessed version of each dataset with the corresponding number for the original data. The former should be ca. 59% of the latter for the minimal dataset, and ca. 69% for the full dataset. The exact percentage will vary slightly because of the randomness in the sampling. You may want to repeat your computation several times."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDeH47VYsCjy","outputId":"2c313660-f1ef-4461-ae61-c408621c8048","executionInfo":{"status":"ok","timestamp":1675006983474,"user_tz":-60,"elapsed":138552,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["12221369\n","17594885\n","0.694597833404424\n"]}],"source":["processed = [sentence_list for sentence_list in preprocess(vocab, freq_table, CURR_DATASET)]\n","print(sum(1 for t in tokens(processed)))\n","print(sum(1 for t in tokens(CURR_DATASET)))\n","\n","print(sum(1 for t in tokens(processed)) / sum(1 for t in tokens(CURR_DATASET)))\n"]},{"cell_type":"markdown","metadata":{"id":"tTmjcm2fsCjy"},"source":["## Problem 3: Generate the training examples"]},{"cell_type":"markdown","metadata":{"id":"IUdQBWHPsCjy"},"source":["Your next task is to translate the preprocessed sentences into training examples for the skip-gram model: both *positive examples* (target word–context word pairs actually observed in the data) and *negative examples* (pairs randomly sampled from a noise distribution).\n","\n","**⚠️ We expect that solving this problem will take you the longest time in this lab.**\n","\n","### General strategy\n","\n","The general plan for solving this problem is to implement a generator function that traverses the preprocessed sentences, at each position of the text samples a window, and then extracts all positive examples from it. For each positive example, the function also generates $k$ negative examples, where $k$ is a hyperparameter. Finally, all examples (positive and negative) are combined into the tensor representation described below.\n","\n","### Representation\n","\n","How should you represent a batch of training examples? Writing $B$ for the batch size, the obvious choice would be to represent the inputs as a matrix of shape $[B, 2]$ and the output labels (positive/negative) as a vector of length $B$. This representation would be quite wasteful on the input side, however, as each target word (index) from a positive example would have to be repeated in all negative samples. For example ($k=3$):"]},{"cell_type":"raw","metadata":{"id":"Sj61VKYQsCjy"},"source":["tensor([[34,  237],    # positive example 1 # LEFT POSITIVE WORD\n","\n","*   List item\n","*   List item\n","\n","\n","        [34, 2561],    # negative example 1.1 K = 1\n","        [34,   39],    # negative example 1.2 K = 2\n","        [34,  903],    # negative example 1.3 K = 3\n","        [34, 2036],    # positive example 2 # RIGHT POSITIVE WORD\n","        [34, 2132],    # negative example 2.1 \n","        [34,  576],    # negative example 2.2 \n","        [34, 2437]])   # negative example 2.3"]},{"cell_type":"markdown","metadata":{"id":"R2AcRXeasCjz"},"source":["Here you will use a different representation: First, instead of a single input batch, there will be a *pair* of input batches – a vector for the target words and a matrix for the context words. If the target word vector has length $B$, the context word matrix has shape $[B, 1+k]$. The $i$th element of the target word vector is the target word for *all* context words in the $i$th row of the context word matrix: the first column of that row comes from a positive example, the remaining columns come from the $k$ negative samples. Accordingly, the batch with the output labels will be a matrix of the same shape as the context word matrix, with its first column set to&nbsp;1 and its remaining columns set to&nbsp;0. Corresponding to the example above:"]},{"cell_type":"raw","metadata":{"id":"miQcGxNxsCjz"},"source":["# input batch component 1: target word vector\n","tensor([34, 34])\n","\n","# input batch component 2: context word matrix\n","tensor([[237, 2561, 39, 903], [2036, 2132, 576, 2437]])\n","\n","# output labels\n","tensor([[1, 0, 0, 0], [1, 0, 0, 0]])"]},{"cell_type":"markdown","metadata":{"id":"J8LpecsQsCjz"},"source":["For the present problem, you will only be concerned with the two input batches; the output batch will be constructed in the training procedure. In fact, for a fixed batch size $B$, that batch is always exactly the same, so you will only have to build it once.\n","\n","### Negative sampling\n","\n","Recall from Lecture&nbsp;1.4 that the probability of a word $c$ to be selected as the context word in a negative sample is proportional to its exponentiated count $\\#(c)^\\alpha$, where $\\alpha$ is a hyperparameter (default value: 0.75).\n","\n","To implement negative sampling from this distribution, you can follow a standard recipe: Start by pre-computing an array containing the *cumulative sums* of the exponentiated counts. Then, generate a random cumulative count $n$, and find that index in the pre-computed array at which $n$ should be inserted to keep the array sorted. That index identifies the sampled context word.\n","\n","All operations in this recipe can be implemented efficiently in PyTorch; the relevant functions are [`torch.cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html) and [`torch.searchsorted`](https://pytorch.org/docs/stable/generated/torch.searchsorted.html). For optimal efficiency, you should sample all $B \\times k$ negative examples in a batch at once."]},{"cell_type":"markdown","metadata":{"id":"do_bu1CLsCjz"},"source":["Here is skeleton code for this problem:"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"_13vpICRrqu_","executionInfo":{"status":"ok","timestamp":1675003272616,"user_tz":-60,"elapsed":3,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["from torch import LongTensor\n","import math as m\n","\n","def get_c1_c2(sent, c_w, t_id):\n","  for context_id in sent[c_w]: # 3\n","    if t_id != context_id:\n","      yield t_id, [context_id]\n","\n","\n","def training_examples(vocab, counts, sentences, window=5, num_ns=5, batch_size=1<<19, ns_exponent=0.75):\n","    # TODO: Replace the next line with your own code\n","    cumulative_sum = torch.cumsum(torch.from_numpy(counts**ns_exponent), dim=0)\n","    rand = torch.rand(batch_size, num_ns)*cumulative_sum[-1]\n","    negative_samples = torch.searchsorted(cumulative_sum, rand)\n","\n","\n","    preprocessed = [sentence for sentence in preprocess(vocab, freq_table, sentences)]\n","    comp1 = []\n","    comp2 = []\n","\n","    for sentence in preprocessed: # 1\n","      window_size =  int(np.random.uniform(2, window))\n","      \n","      for index, target_id in enumerate(sentence): # 2\n","        context_window = slice(max(0, -window_size+index), min(window_size+index+1, len(sentence)),1)\n","\n","        for f, s in get_c1_c2(sentence, context_window, target_id):\n","          comp1.append(f)\n","          comp2.append(s)\n","          if len(comp1) == batch_size:\n","            yield torch.tensor(comp1), torch.cat((torch.tensor(comp2), negative_samples), 1)\n","            comp1 = []\n","            comp2 = []\n","            negative_samples = torch.searchsorted(cumulative_sum, torch.rand(batch_size, num_ns)*cumulative_sum[-1])\n","\n","    yield torch.tensor(comp1), torch.cat((torch.tensor(comp2), negative_samples[:len(torch.tensor(comp1))]), 1)"]},{"cell_type":"markdown","metadata":{"id":"4bDhBjZysCjz"},"source":["Your code should comply with the following specification:\n","\n","**training_examples** (*vocab*, *counts*, *sentences*, *window* = 5, *num_ns* = 5, *batch_size* = 524,288, *ns_exponent*=0.75)\n","\n","> Reads from an iterable of *sentences* (lists of string tokens), preprocesses them using the function implemented in Problem&nbsp;2, and then yields pairs of input batches for gradient-based training, represented as described above. Each batch contains *batch_size* positive examples. The parameter *window* specifies the maximal distance between a target word and a context word in a positive example; the actual window size around any given target word is sampled uniformly at random. The parameter *num_ns* specifies the number of negative samples per positive sample. The parameter *ns_exponent* specifies the exponent in the negative sampling (called $\\alpha$ above)."]},{"cell_type":"markdown","metadata":{"id":"c88z_yn4sCj0"},"source":["### 🤞 Test your code\n","\n","To test your code, compare the total number of positive samples (across all batches) to the total number of tokens in the (un-preprocessed) minimal dataset. The ratio between these two values should be ca. 2.64. If you can spare the time, you can make the same comparison on the full dataset; here, the expected ratio is 3.25. As before, the numbers may vary slightly because of randomness, so you may want to run the comparison more than once."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iPe3w5JrsCj0","outputId":"a68c0812-f21c-48f5-8912-054703c92517","executionInfo":{"status":"ok","timestamp":1675003579945,"user_tz":-60,"elapsed":303806,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Original length:  17594885\n","Batch:  0\n","Batch:  1\n","Batch:  2\n","Batch:  3\n","Batch:  4\n","Batch:  5\n","Batch:  6\n","Batch:  7\n","Batch:  8\n","Batch:  9\n","Batch:  10\n","Batch:  11\n","Batch:  12\n","Batch:  13\n","Batch:  14\n","Batch:  15\n","Batch:  16\n","Batch:  17\n","Batch:  18\n","Batch:  19\n","Batch:  20\n","Batch:  21\n","Batch:  22\n","Batch:  23\n","Batch:  24\n","Batch:  25\n","Batch:  26\n","Batch:  27\n","Batch:  28\n","Batch:  29\n","Batch:  30\n","Batch:  31\n","Batch:  32\n","Batch:  33\n","Batch:  34\n","Batch:  35\n","Batch:  36\n","Batch:  37\n","Batch:  38\n","Batch:  39\n","Batch:  40\n","Batch:  41\n","Batch:  42\n","Batch:  43\n","Batch:  44\n","Batch:  45\n","Batch:  46\n","Batch:  47\n","Batch:  48\n","Batch:  49\n","Batch:  50\n","Batch:  51\n","Batch:  52\n","Batch:  53\n","Batch:  54\n","Batch:  55\n","Batch:  56\n","Batch:  57\n","Batch:  58\n","Batch:  59\n","Batch:  60\n","Batch:  61\n","Batch:  62\n","Batch:  63\n","Batch:  64\n","Batch:  65\n","Batch:  66\n","Batch:  67\n","Batch:  68\n","Batch:  69\n","Batch:  70\n","Batch:  71\n","Batch:  72\n","Batch:  73\n","Batch:  74\n","Batch:  75\n","Batch:  76\n","Batch:  77\n","Batch:  78\n","Batch:  79\n","Batch:  80\n","Batch:  81\n","Batch:  82\n","Batch:  83\n","Batch:  84\n","Batch:  85\n","Batch:  86\n","Batch:  87\n","Batch:  88\n","Batch:  89\n","Batch:  90\n","Batch:  91\n","Batch:  92\n","Batch:  93\n","Batch:  94\n","Batch:  95\n","Batch:  96\n","Batch:  97\n","Batch:  98\n","Batch:  99\n","Batch:  100\n","Batch:  101\n","Batch:  102\n","Batch:  103\n","Batch:  104\n","Batch:  105\n","Batch:  106\n","Batch:  107\n","Batch:  108\n","Batch:  109\n","Batch:  110\n","Batch:  111\n","n_examples:  58444202\n","Ratio:  3.3216586525004286\n"]}],"source":["# TODO: Test your code here\n","original_length = sum([len(token) for token in CURR_DATASET])\n","print(\"Original length: \", original_length)\n","\n","n_batches = 0\n","examples = 0\n","for comp1, comp2 in training_examples(vocab, freq_table, list(CURR_DATASET)):\n","  print(\"Batch: \", n_batches)\n","  n_batches += 1\n","  examples += len(comp1)\n","\n","print(\"n_examples: \", examples)\n","\n","print(\"Ratio: \", str(examples / original_length))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCXBlFvwlv4-","executionInfo":{"status":"aborted","timestamp":1674997240718,"user_tz":-60,"elapsed":9,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["sum(1 for t in tokens(CURR_DATASET))"]},{"cell_type":"markdown","metadata":{"id":"jFc5SfsesCj0"},"source":["## Problem 4: Implement the model"]},{"cell_type":"markdown","metadata":{"id":"lM-0YJ1AsCj1"},"source":["Now it is time to implement the skip-gram model as such. The cell below contains skeleton code for this. As you will recall from Lecture&nbsp;1.4, the core of the implementation is formed by two embedding layers: one for the target word representations, and one for the context word representations. Your task is to implement the missing `forward()` method."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"aWjKJzqxsCj1","executionInfo":{"status":"ok","timestamp":1675003603242,"user_tz":-60,"elapsed":431,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SGNSModel(nn.Module):\n","    \n","    def __init__(self, vocab, embedding_dim):\n","        super().__init__()\n","        self.vocab = vocab\n","        self.w = nn.Embedding(len(vocab), embedding_dim)\n","        self.c = nn.Embedding(len(vocab), embedding_dim)\n","    \n","    def forward(self, w, c):\n","        # TODO: Replace the next line with your own code\n","        return torch.bmm(torch.unsqueeze(self.w(w), 1), torch.transpose(self.c(c), 1, 2)) # can be one line of code. FOUND IT!! :))))"]},{"cell_type":"markdown","metadata":{"id":"mQ_2rhynsCj1"},"source":["Your implementation of the `forward()` method should comply with the following specification:\n","\n","**forward** (*self*, *w*, *c*)\n","\n","> The input to this methods is a tensor *w* with target words of shape $[B]$ and a tensor *c* with context words of shape $[B, 1+k]$, where $B$ is the batch size and $k$ is the number of negative samples. The two tensors are structured as explained for Problem&nbsp;3. The output of the method is a tensor $D$ of shape $[B, k+1]$ where entry $D_{ij}$ is the dot product between the embedding vector for the $i$th target word and the embedding vector for the context word in row $i$, column $j$.\n","\n","**💡 Hint:** To compute a dot product $x^\\top y$, you can first compute the Hadamard product $z = x \\odot y$ and then sum up the elements of $z$."]},{"cell_type":"markdown","metadata":{"id":"7dIcPYFwsCj1"},"source":["### 🤞 Test your code\n","\n","Test your code by creating an instance of the model, and check that `forward` returns the expected result on random input tensors *w* and *c*. To help you, the following function will return a random example from the first 100 examples produced by `training_examples`."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"zxpwj0WnsCj1","executionInfo":{"status":"ok","timestamp":1675003605997,"user_tz":-60,"elapsed":593,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["import numpy as np\n","\n","def random_example(vocab, counts, sentences):\n","    skip = np.random.randint(100)\n","    for i, example in enumerate(training_examples(vocab, counts, sentences, num_ns=1, batch_size=5)):\n","        if i >= skip:\n","            break\n","    return example\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gjvr3L78mA-c","executionInfo":{"status":"ok","timestamp":1674998029751,"user_tz":-60,"elapsed":115399,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}},"outputId":"53cd23aa-e170-438d-bf46-23360f62087d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-6.5494, -5.7961]],\n","\n","        [[-7.5001,  7.7565]],\n","\n","        [[ 5.2542,  4.6341]],\n","\n","        [[ 0.7494,  7.5003]],\n","\n","        [[-1.5652,  7.2306]]], grad_fn=<BmmBackward0>)"]},"metadata":{},"execution_count":22}],"source":["w, c = random_example(vocab, freq_table, CURR_DATASET)\n","SGNS = SGNSModel(vocab, 50)\n","SGNS.forward(w,c)"]},{"cell_type":"markdown","metadata":{"id":"fNpG4ty9sCj1"},"source":["## Problem 5: Train the model"]},{"cell_type":"markdown","metadata":{"id":"Rn-CxkVnsCj1"},"source":["Once you have a working model, it is time to train it. The training loop for the skip-gram model will be very similar to the prototypical training loop that you saw in Lecture&nbsp;0.6, with two things to note:\n","\n","First, instead of categorical cross entropy, you will use binary cross entropy. Just like the standard implementation of the softmax classifier, the skip-gram model does not include a final non-linearity, so you should use [`binary_cross_entropy_with_logits()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.functional.binary_cross_entropy_with_logits.html).\n","\n","The second thing to note is that you will have to create the tensor with the output labels, as explained already in Problem&nbsp;3. This should be a matrix of size $[B, 1+k]$ whose first column contains $1$s and whose remaining columns contains $0$s."]},{"cell_type":"markdown","metadata":{"id":"FPJ9UUe5sCj2"},"source":["Here is skeleton code for the training loop, including default values for the most important hyperparameters:"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"vAE_khDgsCj2","executionInfo":{"status":"ok","timestamp":1675003611131,"user_tz":-60,"elapsed":439,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import tqdm\n","\n","def output_labels(batch, num_ns):\n","    tensor = torch.LongTensor(batch, num_ns + 1)\n","    tensor[:,0] = 1\n","    tensor[:, 1:] = 0\n","    return tensor\n","\n","def train(sentences, embedding_dim=50, window=5, num_ns=5, batch_size=1<<19, n_epochs=1, lr=1e-1):\n","    # Create the vocabulary and the counts\n","    vocab, counts = make_vocab_and_counts(sentences)\n","    \n","    # Initialize the model\n","    model = SGNSModel(vocab, embedding_dim)\n","    \n","    # Initialize the optimizer. Here we use Adam rather than plain SGD\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    \n","    # TODO: Add your code here\n","    train_losses = []\n","\n","\n","    # Inspired by Lab1_A example notebook\n","    with tqdm.tqdm(total=n_epochs) as pbar:\n","        for t in range(n_epochs):\n","            pbar.set_description(f'Epoch {t+1}')\n","            iteration = 0\n","            running_loss = 0\n","\n","            # Start training\n","            model.train()\n","            running_loss = 0\n","            for bx, by in training_examples(vocab, counts, sentences):\n","                optimizer.zero_grad()\n","                output = model.forward(bx, by)\n","                by = torch.unsqueeze(output_labels(by.shape[0], num_ns), 1)\n","                loss = F.binary_cross_entropy_with_logits(output.to(torch.float32), by.to(torch.float32))\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item()\n","                iteration += 1\n","                \n","            pbar.update()\n","            train_losses.append(running_loss / len(vocab))\n","            print(\"\\n[Epoch \" + str(t+1) +  \" avg loss]: \" + str(running_loss/iteration))\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"6G1M0bcasCj2"},"source":["To show you how `train` is meant to be used, the code in the next cell trains a model on the minimal dataset."]},{"cell_type":"markdown","metadata":{"id":"W6tZ70HhsCj2"},"source":["### 🤞 Test your code\n","\n","Test your implementation of the training loop by training a model on the minimal dataset. This should only take a few seconds. You will not get useful word vectors, but you will be able to see whether your code runs without errors.\n","\n","Once you have passed this test, you can train a model on the full dataset. Print the loss to check that the model is actually learning; if the loss is not decreasing, try to find the problem before wasting time (and energy) on useless training.\n","\n","Training on the full dataset will take some time – on a CPU, you should expect 10–40 minutes per epoch, depending on hardware. To give you some guidance: The total number of positive examples is approximately 58M, and the batch size is chosen so that each batch contains roughly 10% of these examples. To speed things up, you can train using a GPU; our reference implementation runs in less than 2 minutes per epoch on [Colab](http://colab.research.google.com)."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"a2tlrdYssCj2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675006841809,"user_tz":-60,"elapsed":2787081,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}},"outputId":"bb585df6-225d-415a-bf53-f502191d0a52"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2:  10%|█         | 1/10 [05:24<48:42, 324.76s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","[Epoch 1 avg loss]: 0.5989209025033883\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3:  20%|██        | 2/10 [10:45<43:00, 322.56s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 2 avg loss]: 0.40541314147412777\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4:  30%|███       | 3/10 [16:04<37:25, 320.82s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 3 avg loss]: 0.39662581788642065\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5:  40%|████      | 4/10 [21:23<32:01, 320.22s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 4 avg loss]: 0.39356750303081106\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6:  50%|█████     | 5/10 [26:43<26:40, 320.12s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 5 avg loss]: 0.392048804089427\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7:  60%|██████    | 6/10 [32:02<21:18, 319.55s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 6 avg loss]: 0.391881611464279\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8:  70%|███████   | 7/10 [37:20<15:57, 319.14s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 7 avg loss]: 0.39144144233848366\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9:  80%|████████  | 8/10 [42:40<10:38, 319.28s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 8 avg loss]: 0.3914105822997434\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10:  90%|█████████ | 9/10 [47:56<05:18, 318.34s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 9 avg loss]: 0.3909832313656807\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|██████████| 10/10 [53:12<00:00, 319.25s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","[Epoch 10 avg loss]: 0.3913383702082293\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# TODO: Train your model on the full dataset here\n","\n","model_big = train(CURR_DATASET, n_epochs = 10)"]},{"cell_type":"markdown","metadata":{"id":"u5lnYhZ_sCj2"},"source":["## Problem 6: Analyse the embeddings (reflection)"]},{"cell_type":"markdown","metadata":{"id":"1-gT41WCsCj2"},"source":["Now that you have a trained model, you will probably be curious to see what it has learned. You can inspect your embeddings using the [Embedding Projector](http://projector.tensorflow.org). To that end, click on the ‘Load’ button, which will open up a dialogue with instructions for how to upload embeddings from your computer.\n","\n","You will need to upload two tab-separated files. To create them, you can use the following code:"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"iaHL8LGSsCj2","executionInfo":{"status":"ok","timestamp":1675003156294,"user_tz":-60,"elapsed":386,"user":{"displayName":"TheQkk LP","userId":"05924923103342954592"}}},"outputs":[],"source":["def save_model(model):\n","    # Extract the embedding vectors as a NumPy array\n","    embeddings = model.w.weight.detach().numpy()\n","    \n","    # Create the word–vector pairs\n","    items = sorted((i, w) for w, i in model.vocab.items())\n","    items = [(w, e) for (i, w), e in zip(items, embeddings)]\n","    \n","    # Write the embeddings and the word labels to files\n","    with open('vectors.tsv', 'wt') as fp1, open('metadata.tsv', 'wt') as fp2:\n","        for w, e in items:\n","            print('\\t'.join('{:.5f}'.format(x) for x in e), file=fp1)\n","            print(w, file=fp2)"]},{"cell_type":"markdown","metadata":{"id":"7peUYwWHsCj3"},"source":["Take some time to explore the embedding space. In particular, inspect the local neighbourhoods of words that you are curious about, say the 10 closest neighbours. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n","\n","* Which words did you try? Which results did you get? Did you do anything else than inspecting local neighbourhoods?\n","* Based on what you know about word embeddings, did you expect your results? How do you explain them?\n","* What did you learn? How, exactly, did you learn it? Why does this learning matter?"]},{"cell_type":"markdown","metadata":{"id":"9nxkoMHRsCj3"},"source":["*TODO: Enter your text here*\n"]},{"cell_type":"markdown","source":["Words tried:\n","\n","Woman:landon 0.414\n","airoldi 0.445\n","sudeep 0.454\n","westbound 0.474\n","cambodian 0.481\n","streeterville 0.482\n","griggs 0.483\n","tur 0.486\n","frankweiler 0.490\n","egocentric 0.496\n","\n","Sister:\n","rubingh 0.455\n","fremantle 0.467\n","pulling 0.485\n","hindered 0.485\n","muller 0.494\n","defencemen 0.500\n","peanut 0.500\n","numbness 0.504\n","lyrebird 0.507\n","nie 0.513\n","\n","Dog: \n","explosions 0.387\n","telephones 0.467\n","motorized 0.470\n","carthaginian 0.476\n","shikasta 0.485\n","beveren 0.509\n","spirituality 0.510\n","destroys 0.522\n","napoléon 0.523\n","amplification 0.523\n","\n","Question1\n","The results show poor coorelation. Sister should be neighbours with something semantically similar. The words given could never be guessed as an relation to the word checked in the word-space. I.e., sister has the neighbours peanut and pulling, dog has the neighbours explosions and telephones. \n","We also tried with the small dataset and this gave the same results but with different neighbours.\n","\n","Besides inspecting the local neighbours we inspected the 100 closests neighbours. For the word \"football\" one can see that the training does not yield the results expected, out of all the words 1 single word \"coach\" could be related to the word football. The rest of the words seems more or less random\n","\n","Question2\n","From our knowledge the embeddings should correlate words that are close to eachother and dissimilate words that are not the same as the word. Fully training the model should yield neighbours that are semantically related. There is something wrong when correlating the words (the context words). This can be seen because the words in the space that are neighbouring are not at all similar.\n","\n","Question3\n","We have learned the skipgram model for learning word embeddings. This learning is done by applying a context window to the sentences in the corpus and adding negative samples that have no relation to the word. The main learning point for this lab for us was handling torch and manipulating tensors/vectors/lists etc. \n","The model-parts such as subsampling and negative sampling was mainly learned by deeply examining the lectures and models, whilst the torch learning part was done by examining torch sizes and understanding the error prompts given when wrongly coded. This learning matters to get a deep knowledge of the models used. For instance, the word2vec models can easily be used from libaries such as sklearn, but what negative sampling and randomly discarding words based on frequency is does not really show when using these pre-done models. \n","\n","\n"],"metadata":{"id":"BeU0lHuiD9Kb"}},{"cell_type":"markdown","metadata":{"id":"shvUl6lqsCj3"},"source":["👍 Well done!"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"},"vscode":{"interpreter":{"hash":"bc143343ca8435bba8c44b3b1f47f9edcb7f00f13cf7dc8cb9f5e5ffbd446b7a"}},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}