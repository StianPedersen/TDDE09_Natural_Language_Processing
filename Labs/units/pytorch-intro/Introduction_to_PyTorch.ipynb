{"cells":[{"cell_type":"markdown","id":"4d272c02","metadata":{"id":"4d272c02"},"source":["# Introduction to PyTorch"]},{"cell_type":"markdown","id":"2db260ca","metadata":{"id":"2db260ca"},"source":["The purpose of this notebook is to introduce you to the basics of [PyTorch](https://pytorch.org), the deep learning framework we will be using for the labs. Many good introductions to PyTorch are available online. This notebook focuses on those basics that you will encounter in the labs. Beyond it, you will also need to get comfortable with the [PyTorch documentation](https://pytorch.org/docs/stable/)."]},{"cell_type":"markdown","id":"349afbb4","metadata":{"id":"349afbb4"},"source":["We start by importing the PyTorch module:"]},{"cell_type":"code","execution_count":3,"id":"97b4ce01","metadata":{"id":"97b4ce01","executionInfo":{"status":"ok","timestamp":1674044112113,"user_tz":-60,"elapsed":625,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","id":"16747627","metadata":{"id":"16747627"},"source":["The following code prints the current version of the module:"]},{"cell_type":"code","execution_count":4,"id":"8bd1443b","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8bd1443b","executionInfo":{"status":"ok","timestamp":1674044112935,"user_tz":-60,"elapsed":447,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"de84ace8-214a-4f6b-b72b-6f98df1996b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.13.1+cu116\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"markdown","id":"59a301db","metadata":{"id":"59a301db"},"source":["The version of PyTorch at the time of writing this notebook was 1.12."]},{"cell_type":"markdown","id":"9e2c610b","metadata":{"id":"9e2c610b"},"source":["## Tensors"]},{"cell_type":"markdown","id":"4de64ccf","metadata":{"id":"4de64ccf"},"source":["The fundamental data structure in PyTorch is the **tensor**, a multi-dimensional matrix containing elements of a single numerical data type. Tensors are similar to *arrays* as you may know them from NumPy or MATLAB."]},{"cell_type":"markdown","id":"68ad407e","metadata":{"id":"68ad407e"},"source":["### Creating tensors"]},{"cell_type":"markdown","id":"0a090537","metadata":{"id":"0a090537"},"source":["One way to create a tensor is to call the function `torch.tensor()` on a Python list or NumPy array.\n","\n","The code in the following cell creates a 2-dimensional tensor with 4 elements."]},{"cell_type":"code","execution_count":5,"id":"18445c93","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18445c93","executionInfo":{"status":"ok","timestamp":1674044112936,"user_tz":-60,"elapsed":61,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"4ea0350c-2db2-44a2-d005-e4d777208d97"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3]])"]},"metadata":{},"execution_count":5}],"source":["x = torch.tensor([[0, 1], [2, 3]])\n","x"]},{"cell_type":"code","source":["test = torch.tensor([123,321,213], dtype=torch.float32)"],"metadata":{"id":"cR6x5D152sk1","executionInfo":{"status":"ok","timestamp":1674045127074,"user_tz":-60,"elapsed":233,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}}},"id":"cR6x5D152sk1","execution_count":57,"outputs":[]},{"cell_type":"markdown","id":"6d71a273","metadata":{"id":"6d71a273"},"source":["Each tensor has a *shape*, which specifies the number and sizes of its dimensions:"]},{"cell_type":"code","execution_count":6,"id":"1a08d491","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1a08d491","executionInfo":{"status":"ok","timestamp":1674044112936,"user_tz":-60,"elapsed":50,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"b52bd636-fcd6-4c8d-970b-de911ec2b099"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":6}],"source":["x.shape"]},{"cell_type":"markdown","id":"7ae63de4","metadata":{"id":"7ae63de4"},"source":["Each tensor also has a *data type* for its elements. [More information about data types](https://pytorch.org/docs/stable/tensors.html#data-types)"]},{"cell_type":"code","execution_count":7,"id":"a00cceb9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a00cceb9","executionInfo":{"status":"ok","timestamp":1674044112937,"user_tz":-60,"elapsed":40,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"a60a29cf-3ba5-4000-b10f-9e0626f41d02"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.int64"]},"metadata":{},"execution_count":7}],"source":["x.dtype"]},{"cell_type":"markdown","id":"7434e642","metadata":{"id":"7434e642"},"source":["When creating a tensor, you can explicitly pass the intended data type as a keyword argument:"]},{"cell_type":"code","execution_count":8,"id":"fc67aac7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fc67aac7","executionInfo":{"status":"ok","timestamp":1674044112938,"user_tz":-60,"elapsed":35,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"b74fbf28-31f4-4c44-fea4-15bb2152428f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":8}],"source":["y = torch.tensor([[0, 1], [2, 3]], dtype=torch.float)\n","y.dtype"]},{"cell_type":"markdown","id":"2335acc0","metadata":{"id":"2335acc0"},"source":["For many data types, there also exists a specialised constructor:"]},{"cell_type":"code","execution_count":9,"id":"271dffa2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"271dffa2","executionInfo":{"status":"ok","timestamp":1674044112938,"user_tz":-60,"elapsed":27,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"82b5d65e-3c56-4b51-f01c-e1d1f2d45f07"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":9}],"source":["z = torch.FloatTensor([[0, 1], [2, 3]])\n","z.dtype"]},{"cell_type":"markdown","id":"0859d7b0","metadata":{"id":"0859d7b0"},"source":["### More creation operations"]},{"cell_type":"markdown","id":"4c479202","metadata":{"id":"4c479202"},"source":["Create a 3D-tensor of the specified shape and filled with the scalar value zero:"]},{"cell_type":"code","execution_count":10,"id":"408406a7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"408406a7","executionInfo":{"status":"ok","timestamp":1674044112939,"user_tz":-60,"elapsed":24,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"cd0ceda8-ee1b-4718-9758-2aa088d0181f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":10}],"source":["x = torch.zeros(2, 3, 5)\n","x"]},{"cell_type":"markdown","id":"96c1bb22","metadata":{"id":"96c1bb22"},"source":["Create a 3D-tensor filled with random values:"]},{"cell_type":"code","execution_count":11,"id":"edcf74ed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edcf74ed","executionInfo":{"status":"ok","timestamp":1674044112939,"user_tz":-60,"elapsed":20,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"91c509ae-3e1c-4211-91ff-a584fe85b92e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.9584, 0.3020, 0.6729, 0.0888, 0.3039],\n","         [0.5378, 0.7147, 0.0199, 0.5545, 0.8892],\n","         [0.2479, 0.3087, 0.9475, 0.7039, 0.1018]],\n","\n","        [[0.4027, 0.8297, 0.0730, 0.9342, 0.4323],\n","         [0.6516, 0.0888, 0.6946, 0.3707, 0.5131],\n","         [0.4446, 0.9958, 0.3233, 0.5693, 0.8131]]])"]},"metadata":{},"execution_count":11}],"source":["x = torch.rand(2, 3, 5)\n","x"]},{"cell_type":"markdown","id":"5bd69703","metadata":{"id":"5bd69703"},"source":["Create a tensor with the same shape as another one, but filled with ones:"]},{"cell_type":"code","execution_count":12,"id":"df69e2aa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df69e2aa","executionInfo":{"status":"ok","timestamp":1674044112940,"user_tz":-60,"elapsed":18,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"87e6e813-dd9b-4cf6-8ce9-e650486cc743"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1.]],\n","\n","        [[1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1.]]])"]},"metadata":{},"execution_count":12}],"source":["y = torch.ones_like(x)\n","y    # shape: [2, 3, 5]"]},{"cell_type":"markdown","id":"1f402305","metadata":{"id":"1f402305"},"source":["For a complete list of tensor-creating operations, see [Creation ops](https://pytorch.org/docs/stable/torch.html#creation-ops)."]},{"cell_type":"markdown","id":"f6cd8e89","metadata":{"id":"f6cd8e89"},"source":["### Embrace vectorisation!"]},{"cell_type":"markdown","id":"4a2fd635","metadata":{"id":"4a2fd635"},"source":["Iteration is of one the most useful techniques for processing data in Python. However, you should **not loop over tensors**. Instead, try to *vectorise* any operations. Looping over tensors is slow, while vectorised operations on tensors are fast (and can be made even faster when the code runs on a GPU). To illustrate this point, let us create a 1D-tensor containing the first 1M integers:"]},{"cell_type":"code","execution_count":64,"id":"64607413","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64607413","executionInfo":{"status":"ok","timestamp":1674045497329,"user_tz":-60,"elapsed":238,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"52a0a24e-a2af-43a2-bfea-098d14f3c63e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([     0,      1,      2,  ..., 999997, 999998, 999999])"]},"metadata":{},"execution_count":64}],"source":["x = torch.arange(1000000)\n","x"]},{"cell_type":"markdown","id":"3ac363b4","metadata":{"id":"3ac363b4"},"source":["Summing up the elements of the tensor using a loop is relatively slow:"]},{"cell_type":"code","execution_count":65,"id":"e0f7446e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0f7446e","executionInfo":{"status":"ok","timestamp":1674045502406,"user_tz":-60,"elapsed":4037,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"d5f566fd-b7f6-4cd7-dcb1-fbb7789169ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(499999500000)"]},"metadata":{},"execution_count":65}],"source":["sum(i for i in x)"]},{"cell_type":"markdown","id":"4bfd223e","metadata":{"id":"4bfd223e"},"source":["Doing the same thing using a tensor operation is much faster:"]},{"cell_type":"code","execution_count":66,"id":"2c969127","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c969127","executionInfo":{"status":"ok","timestamp":1674045504303,"user_tz":-60,"elapsed":337,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"08ec961b-9de6-4a3b-9cd7-a5ef2024c0b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(499999500000)"]},"metadata":{},"execution_count":66}],"source":["x.sum()"]},{"cell_type":"markdown","id":"28eb92c5","metadata":{"id":"28eb92c5"},"source":["### Indexing and slicing"]},{"cell_type":"markdown","id":"5b09b2ca","metadata":{"id":"5b09b2ca"},"source":["To access the contents of a tensor, you can use an extended version of Python’s syntax for indexing and slicing. Essentially the same syntax is used by NumPy. For more information, see [Indexing on ndarrays](https://numpy.org/doc/stable/user/basics.indexing.html).\n","\n","To illustrate indexing and slicing, we create a 3D-tensor with random numbers:"]},{"cell_type":"code","execution_count":74,"id":"4cda991f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4cda991f","executionInfo":{"status":"ok","timestamp":1674045880921,"user_tz":-60,"elapsed":249,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"d7a2f081-921d-4887-d35c-bbac7341feb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.0325, 0.7438, 0.1388, 0.4147, 0.8045],\n","         [0.6134, 0.3788, 0.4701, 0.0094, 0.6173],\n","         [0.6834, 0.6281, 0.5560, 0.6190, 0.7028]],\n","\n","        [[0.4232, 0.1777, 0.2173, 0.9317, 0.3565],\n","         [0.4882, 0.6447, 0.3883, 0.7142, 0.0984],\n","         [0.1446, 0.2337, 0.7586, 0.5953, 0.1186]]])"]},"metadata":{},"execution_count":74}],"source":["x = torch.rand(2, 3, 5)\n","x"]},{"cell_type":"markdown","id":"8158d193","metadata":{"id":"8158d193"},"source":["Index an element by a 3D-coordinate; this gives a 0D-tensor:"]},{"cell_type":"code","execution_count":75,"id":"6ead7098","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ead7098","executionInfo":{"status":"ok","timestamp":1674045884927,"user_tz":-60,"elapsed":261,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"ad0d2472-a246-47fc-fb9b-0f993b30dff9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.7142)"]},"metadata":{},"execution_count":75}],"source":["x[1,1,3]"]},{"cell_type":"markdown","id":"b828b34e","metadata":{"id":"b828b34e"},"source":["(If you want the result as a non-tensor, use the method [`item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item).)"]},{"cell_type":"markdown","id":"b4ad7eb6","metadata":{"id":"b4ad7eb6"},"source":["Index the second element; this gives a 2D-tensor:"]},{"cell_type":"code","execution_count":76,"id":"e664be64","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e664be64","executionInfo":{"status":"ok","timestamp":1674045888813,"user_tz":-60,"elapsed":11,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"f1519c25-e9f7-45ea-a1b3-4af04cb538c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4232, 0.1777, 0.2173, 0.9317, 0.3565],\n","        [0.4882, 0.6447, 0.3883, 0.7142, 0.0984],\n","        [0.1446, 0.2337, 0.7586, 0.5953, 0.1186]])"]},"metadata":{},"execution_count":76}],"source":["x[1]"]},{"cell_type":"markdown","id":"7d5cc13e","metadata":{"id":"7d5cc13e"},"source":["Index the second-to-last element:"]},{"cell_type":"code","execution_count":77,"id":"c682a041","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c682a041","executionInfo":{"status":"ok","timestamp":1674045891219,"user_tz":-60,"elapsed":243,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"7a22867c-6a71-477b-ddb9-b82311c274a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0325, 0.7438, 0.1388, 0.4147, 0.8045],\n","        [0.6134, 0.3788, 0.4701, 0.0094, 0.6173],\n","        [0.6834, 0.6281, 0.5560, 0.6190, 0.7028]])"]},"metadata":{},"execution_count":77}],"source":["x[-2]"]},{"cell_type":"markdown","id":"e28d37e5","metadata":{"id":"e28d37e5"},"source":["Slice out the sub-tensor with elements from index 1 onwards; this gives a 3D-tensor:"]},{"cell_type":"code","execution_count":78,"id":"d3ac176c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3ac176c","executionInfo":{"status":"ok","timestamp":1674045893658,"user_tz":-60,"elapsed":8,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"dce0cada-49ea-4cb4-eb1b-f59049b1b4a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4232, 0.1777, 0.2173, 0.9317, 0.3565],\n","         [0.4882, 0.6447, 0.3883, 0.7142, 0.0984],\n","         [0.1446, 0.2337, 0.7586, 0.5953, 0.1186]]])"]},"metadata":{},"execution_count":78}],"source":["x[1:]"]},{"cell_type":"markdown","id":"c2a9938c","metadata":{"id":"c2a9938c"},"source":["Here is a more complex example of slicing. As in Python, the colon `:` selects all indices of a dimension."]},{"cell_type":"code","execution_count":79,"id":"073cd642","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"073cd642","executionInfo":{"status":"ok","timestamp":1674046025236,"user_tz":-60,"elapsed":231,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"44aa8e5a-1ca2-4841-f6ce-0becd9c4997e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.1388, 0.4147],\n","         [0.4701, 0.0094],\n","         [0.5560, 0.6190]],\n","\n","        [[0.2173, 0.9317],\n","         [0.3883, 0.7142],\n","         [0.7586, 0.5953]]])"]},"metadata":{},"execution_count":79}],"source":["x[:,:,2:4]"]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w82hm64z71FA","executionInfo":{"status":"ok","timestamp":1674046299855,"user_tz":-60,"elapsed":234,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"14201921-0078-405f-ad4a-c3d03be1195d"},"id":"w82hm64z71FA","execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.0152, 0.5878],\n","         [0.0069, 0.1900],\n","         [0.4153, 0.1454]],\n","\n","        [[0.1117, 0.0599],\n","         [0.4483, 0.2859],\n","         [0.6726, 0.6849]]])"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","id":"561a6fa7","metadata":{"id":"561a6fa7"},"source":["The syntax for indexing and slicing is very powerful. For example, the same effect as in the previous cell can be obtained with the following code, which uses the ellipsis (`...`) to match all dimensions but the ones explicitly mentioned:"]},{"cell_type":"code","execution_count":93,"id":"33b4576a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33b4576a","executionInfo":{"status":"ok","timestamp":1674046530278,"user_tz":-60,"elapsed":245,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"2dfbc7e1-1a9a-4f1f-db1e-c1b18a36f9fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.0152],\n","         [0.0069],\n","         [0.4153]],\n","\n","        [[0.1117],\n","         [0.4483],\n","         [0.6726]]])"]},"metadata":{},"execution_count":93}],"source":["x[...,2:4]\n","x[1,...,:]\n","x[...,0:-1,...]"]},{"cell_type":"markdown","id":"02bb92b3","metadata":{"id":"02bb92b3"},"source":["### Creating views"]},{"cell_type":"markdown","id":"027a61b1","metadata":{"id":"027a61b1"},"source":["You will sometimes want to use a tensor with a different shape than its initial shape. In these situations, you can **re-shape** the tensor or create a **view** of the tensor. The latter is preferable because views can share the same data as their base tensors and thus do not require copying."]},{"cell_type":"markdown","id":"61dbee10","metadata":{"id":"61dbee10"},"source":["We create a 3D-tensor of 12 random values:"]},{"cell_type":"code","execution_count":94,"id":"b278dd80","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b278dd80","executionInfo":{"status":"ok","timestamp":1674046705089,"user_tz":-60,"elapsed":260,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"67a8b4f9-e6cc-4760-acf7-3df4a7e14818"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.4451, 0.9173],\n","         [0.8155, 0.1258],\n","         [0.1015, 0.4029]],\n","\n","        [[0.8112, 0.9084],\n","         [0.3612, 0.3159],\n","         [0.2797, 0.8303]]])"]},"metadata":{},"execution_count":94}],"source":["x = torch.rand(2, 3, 2)\n","x"]},{"cell_type":"markdown","id":"76d13169","metadata":{"id":"76d13169"},"source":["Create a view of this tensor as a 2D-tensor:"]},{"cell_type":"code","execution_count":102,"id":"1ce8e478","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ce8e478","executionInfo":{"status":"ok","timestamp":1674046981809,"user_tz":-60,"elapsed":237,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"e4b240e8-a957-4a16-85e2-cefe2c671db5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[0.4451, 0.9173],\n","          [0.8155, 0.1258],\n","          [0.1015, 0.4029]],\n","\n","         [[0.8112, 0.9084],\n","          [0.3612, 0.3159],\n","          [0.2797, 0.8303]]]])"]},"metadata":{},"execution_count":102}],"source":["x.view(3, 4)\n","x.view(1, 2, 3, 2)"]},{"cell_type":"markdown","id":"4759152f","metadata":{"id":"4759152f"},"source":["When creating a view, the special size `-1` is inferred from the other sizes:"]},{"cell_type":"code","execution_count":96,"id":"3cde21b5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cde21b5","executionInfo":{"status":"ok","timestamp":1674046851806,"user_tz":-60,"elapsed":244,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"7df3f94e-3430-451c-9140-f6f28762945d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4451, 0.9173, 0.8155, 0.1258],\n","        [0.1015, 0.4029, 0.8112, 0.9084],\n","        [0.3612, 0.3159, 0.2797, 0.8303]])"]},"metadata":{},"execution_count":96}],"source":["x.view(3, -1)"]},{"cell_type":"markdown","id":"4182c249","metadata":{"id":"4182c249"},"source":["Modifying a view affects the data in the base tensor:"]},{"cell_type":"code","execution_count":97,"id":"ce0d2f2d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce0d2f2d","executionInfo":{"status":"ok","timestamp":1674046861935,"user_tz":-60,"elapsed":241,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"3f538553-285f-4c9a-a89a-b4e4986f0676"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.8685,  0.8109],\n","         [ 0.2828,  0.2762],\n","         [ 0.6815,  0.1819]],\n","\n","        [[ 0.1516,  0.9110],\n","         [ 0.9905,  0.6394],\n","         [ 0.9934, 42.0000]]])"]},"metadata":{},"execution_count":97}],"source":["y = torch.rand(2, 3, 2)\n","z = y.view(3, 4)\n","z[2, 3] = 42\n","y"]},{"cell_type":"markdown","id":"8b180762","metadata":{"id":"8b180762"},"source":["### More viewing operations"]},{"cell_type":"markdown","id":"cd46f63e","metadata":{"id":"cd46f63e"},"source":["There are a few other useful methods that create views. [More information about views](https://pytorch.org/docs/stable/tensor_view.html)"]},{"cell_type":"code","execution_count":27,"id":"a2e0c000","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2e0c000","executionInfo":{"status":"ok","timestamp":1674044117957,"user_tz":-60,"elapsed":60,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"75bdadc8-eb3b-4d6d-8382-d3c9a16a2689"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.8890, 0.9267, 0.0784, 0.2014, 0.6602],\n","         [0.4652, 0.6668, 0.9704, 0.2131, 0.6133],\n","         [0.9815, 0.6617, 0.3992, 0.2479, 0.7259]],\n","\n","        [[0.7383, 0.4450, 0.1197, 0.5831, 0.5762],\n","         [0.2489, 0.6545, 0.8384, 0.5898, 0.6237],\n","         [0.8914, 0.7219, 0.7240, 0.6899, 0.8533]]])"]},"metadata":{},"execution_count":27}],"source":["x = torch.rand(2, 3, 5)\n","x"]},{"cell_type":"markdown","id":"91cdb2d6","metadata":{"id":"91cdb2d6"},"source":["The [`permute()`](https://pytorch.org/docs/stable/generated/torch.permute.html) method returns a view of the base tensor with some of its dimensions permuted. In the example, we maintain the first dimension but swap the second and the third dimension:"]},{"cell_type":"code","execution_count":28,"id":"b5e672ba","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5e672ba","executionInfo":{"status":"ok","timestamp":1674044117957,"user_tz":-60,"elapsed":57,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"2fa50b59-81e7-4ac4-c82c-43731c8ecb7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.8890, 0.4652, 0.9815],\n","         [0.9267, 0.6668, 0.6617],\n","         [0.0784, 0.9704, 0.3992],\n","         [0.2014, 0.2131, 0.2479],\n","         [0.6602, 0.6133, 0.7259]],\n","\n","        [[0.7383, 0.2489, 0.8914],\n","         [0.4450, 0.6545, 0.7219],\n","         [0.1197, 0.8384, 0.7240],\n","         [0.5831, 0.5898, 0.6899],\n","         [0.5762, 0.6237, 0.8533]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 5, 3])"]},"metadata":{},"execution_count":28}],"source":["y = x.permute(0, 2, 1)\n","print(y)\n","y.shape"]},{"cell_type":"markdown","id":"51068591","metadata":{"id":"51068591"},"source":["The [`unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) method returns a tensor with a dimension of size one inserted at the specified position. This is useful e.g. in the training of neural networks when you want to create a batch with just one example."]},{"cell_type":"code","execution_count":117,"id":"fe2fe5fd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fe2fe5fd","executionInfo":{"status":"ok","timestamp":1674047407833,"user_tz":-60,"elapsed":244,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"9bbb357a-9871-431c-cefa-e6bb9d474556"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[[[0.4451, 0.9173],\n","            [0.8155, 0.1258],\n","            [0.1015, 0.4029]],\n","\n","           [[0.8112, 0.9084],\n","            [0.3612, 0.3159],\n","            [0.2797, 0.8303]]]]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 1, 2, 3, 2])"]},"metadata":{},"execution_count":117}],"source":["y = x.unsqueeze(0).unsqueeze(0).unsqueeze(0)\n","print(y)\n","y.shape"]},{"cell_type":"markdown","id":"bb4c6449","metadata":{"id":"bb4c6449"},"source":["The inverse operation to [`unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) is [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.squeeze.html):"]},{"cell_type":"code","execution_count":115,"id":"d88aa622","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d88aa622","executionInfo":{"status":"ok","timestamp":1674047384012,"user_tz":-60,"elapsed":7,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"2f585a13-225f-477f-ffd0-615678069eb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.4451, 0.9173],\n","         [0.8155, 0.1258],\n","         [0.1015, 0.4029]],\n","\n","        [[0.8112, 0.9084],\n","         [0.3612, 0.3159],\n","         [0.2797, 0.8303]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 2])"]},"metadata":{},"execution_count":115}],"source":["y = y.squeeze(1)\n","print(y)\n","y.shape"]},{"cell_type":"markdown","id":"73239829","metadata":{"id":"73239829"},"source":["### Re-shaping tensors"]},{"cell_type":"markdown","id":"daa2b33c","metadata":{"id":"daa2b33c"},"source":["In some cases, you cannot create a view and need to explicitly re-shape a tensor. In particular, this happens when the data in the base tensor and the view are not in contiguous memory regions."]},{"cell_type":"code","execution_count":31,"id":"69762d70","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69762d70","executionInfo":{"status":"ok","timestamp":1674044117958,"user_tz":-60,"elapsed":46,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"7e24c4fe-0a10-4899-8b67-9573bdbc2a3f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.1722, 0.8610, 0.2614, 0.5606, 0.8346],\n","         [0.5104, 0.0873, 0.0536, 0.5891, 0.2583],\n","         [0.8151, 0.9474, 0.5489, 0.9741, 0.8484]],\n","\n","        [[0.7658, 0.2500, 0.5732, 0.4467, 0.0253],\n","         [0.6864, 0.7274, 0.5045, 0.1537, 0.7988],\n","         [0.8439, 0.2009, 0.3046, 0.5521, 0.8255]]])"]},"metadata":{},"execution_count":31}],"source":["x = torch.rand(2, 3, 5)\n","x"]},{"cell_type":"markdown","id":"d5add2f3","metadata":{"id":"d5add2f3"},"source":["We permute the tensor `x` to create a new tensor `y` in which the data is no longer consecutive in memory:"]},{"cell_type":"code","execution_count":32,"id":"665d43f6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"665d43f6","executionInfo":{"status":"ok","timestamp":1674044117959,"user_tz":-60,"elapsed":44,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"6914e19e-6b0d-408a-9fcc-bc4da3f5bee9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.1722, 0.5104, 0.8151],\n","         [0.8610, 0.0873, 0.9474],\n","         [0.2614, 0.0536, 0.5489],\n","         [0.5606, 0.5891, 0.9741],\n","         [0.8346, 0.2583, 0.8484]],\n","\n","        [[0.7658, 0.6864, 0.8439],\n","         [0.2500, 0.7274, 0.2009],\n","         [0.5732, 0.5045, 0.3046],\n","         [0.4467, 0.1537, 0.5521],\n","         [0.0253, 0.7988, 0.8255]]])"]},"metadata":{},"execution_count":32}],"source":["y = x.permute(0, 2, 1)\n","# y = y.view(-1)    # raises a runtime error\n","y"]},{"cell_type":"markdown","id":"4716d06e","metadata":{"id":"4716d06e"},"source":["When it is not possible to create a view of a tensor, you can explicitly re-shape it, which will *copy* the data if necessary:"]},{"cell_type":"code","execution_count":33,"id":"3faaca7f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3faaca7f","executionInfo":{"status":"ok","timestamp":1674044117959,"user_tz":-60,"elapsed":41,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"a0e05f8d-9245-4c0a-ea5e-9fa8f451b601"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1722, 0.5104, 0.8151, 0.8610, 0.0873, 0.9474, 0.2614, 0.0536, 0.5489,\n","        0.5606, 0.5891, 0.9741, 0.8346, 0.2583, 0.8484, 0.7658, 0.6864, 0.8439,\n","        0.2500, 0.7274, 0.2009, 0.5732, 0.5045, 0.3046, 0.4467, 0.1537, 0.5521,\n","        0.0253, 0.7988, 0.8255])"]},"metadata":{},"execution_count":33}],"source":["y = x.permute(0, 2, 1)\n","y = y.reshape(-1)\n","y"]},{"cell_type":"markdown","id":"c057aa8c","metadata":{"id":"c057aa8c"},"source":["Modifying a reshaped tensor *will not necessarily* change the data in the base tensor. This depends on whether the reshaped tensor shares the data with the base tensor."]},{"cell_type":"code","execution_count":34,"id":"0b135ddb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0b135ddb","executionInfo":{"status":"ok","timestamp":1674044117959,"user_tz":-60,"elapsed":38,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"d53ba0bb-1849-41fe-aeb7-3d60de2117e8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.8180, 0.6934, 0.3533],\n","         [0.9551, 0.8811, 0.0962]],\n","\n","        [[0.3957, 0.5498, 0.2417],\n","         [0.1203, 0.4942, 0.9315]]])"]},"metadata":{},"execution_count":34}],"source":["y = torch.rand(2, 3, 2)\n","y = y.permute(0, 2, 1)    # if commented out, data can be shared\n","z = y.reshape(-1)\n","z[0] = 42\n","y"]},{"cell_type":"markdown","id":"0f756729","metadata":{"id":"0f756729"},"source":["## Computing with tensors"]},{"cell_type":"markdown","id":"3fd2d2ee","metadata":{"id":"3fd2d2ee"},"source":["Now that you know how to create tensors and extract data from them, we can turn to actual computations on tensors."]},{"cell_type":"markdown","id":"10f823be","metadata":{"id":"10f823be"},"source":["### Element-wise operations"]},{"cell_type":"markdown","id":"1c9e6a54","metadata":{"id":"1c9e6a54"},"source":["Unary mathematical operations defined on numbers can be ‘lifted’ to tensors by applying them element-wise. This includes multiplication by a constant, exponentiation (`**`), taking roots ([`torch.sqrt()`](https://pytorch.org/docs/stable/generated/torch.sqrt.html)), and the logarithm ([`torch.log()`](https://pytorch.org/docs/stable/generated/torch.sqrt.html))."]},{"cell_type":"code","execution_count":118,"id":"82af5d33","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82af5d33","executionInfo":{"status":"ok","timestamp":1674047940886,"user_tz":-60,"elapsed":251,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"652d9f46-b9c5-4124-9615-0ce05498f94f"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.6362, 0.8731, 0.0589],\n","        [0.9945, 0.7217, 0.1058]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.2724, 1.7462, 0.1178],\n","        [1.9890, 1.4435, 0.2117]])"]},"metadata":{},"execution_count":118}],"source":["x = torch.rand(2, 3)\n","print(x)\n","x * 2    # element-wise multiplication with 2"]},{"cell_type":"markdown","id":"f678ef81","metadata":{"id":"f678ef81"},"source":["Similarly, we can apply binary mathematical operations to tensors with the same shape. For example, the Hadamard product of two tensors $X$ and $Y$ is the tensor $X \\odot Y$ obtained by the element-wise multiplication of the elements of $X$ and $Y$."]},{"cell_type":"code","execution_count":137,"id":"0ec606e1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ec606e1","executionInfo":{"status":"ok","timestamp":1674048762027,"user_tz":-60,"elapsed":238,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"28cec929-e485-4fe2-fcf1-ff89e14ab754"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.6173, 0.1897, 0.2763],\n","        [0.2575, 0.0009, 0.0503]])"]},"metadata":{},"execution_count":137}],"source":["x = torch.rand(2, 3)\n","y = torch.rand(2, 3)\n","torch.mul(x, y)    # shape: [2, 3]"]},{"cell_type":"markdown","id":"295cb664","metadata":{"id":"295cb664"},"source":["The Hadamard product can be written more succinctly as follows:"]},{"cell_type":"code","execution_count":138,"id":"fb34f28d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fb34f28d","executionInfo":{"status":"ok","timestamp":1674048766657,"user_tz":-60,"elapsed":14,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"c64adcef-db6a-4ec9-fbc6-c942a709acfc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.6173, 0.1897, 0.2763],\n","        [0.2575, 0.0009, 0.0503]])"]},"metadata":{},"execution_count":138}],"source":["x * y"]},{"cell_type":"markdown","id":"f2acb1c5","metadata":{"id":"f2acb1c5"},"source":["### Matrix product"]},{"cell_type":"markdown","id":"6b9b51a7","metadata":{"id":"6b9b51a7"},"source":["When computing the matrix product between two tensors $X$ and $Y$, the sizes of the last dimension of $X$ and the first dimension of $Y$ must match. The shape of the resulting tensor is the concatenation of the shapes of $X$ and $Y$, with the last dimension of $X$ and the first dimension of $Y$ removed."]},{"cell_type":"code","execution_count":139,"id":"6d73b8ff","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d73b8ff","executionInfo":{"status":"ok","timestamp":1674048770877,"user_tz":-60,"elapsed":234,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"65bd65d9-66b2-407d-b939-b5bc39f619cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.7884, 0.4074, 0.4134],\n","        [0.8549, 0.4691, 0.0414]])\n","tensor([[0.3247, 0.9132, 0.0470, 0.0237, 0.4673],\n","        [0.8385, 0.9871, 0.6214, 0.7662, 0.5722],\n","        [0.2370, 0.1416, 0.8683, 0.4408, 0.8177]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[0.6956, 1.1806, 0.6492, 0.5130, 0.9395],\n","        [0.6808, 1.2497, 0.3677, 0.3980, 0.7018]])"]},"metadata":{},"execution_count":139}],"source":["x = torch.rand(2, 3)\n","print(x)\n","y = torch.rand(3, 5)\n","print(y)\n","torch.matmul(x, y)    # shape: [2, 5]"]},{"cell_type":"markdown","id":"f3533dbe","metadata":{"id":"f3533dbe"},"source":["The matrix product can be written more succinctly as follows:"]},{"cell_type":"code","execution_count":140,"id":"3867dcbe","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3867dcbe","executionInfo":{"status":"ok","timestamp":1674048810399,"user_tz":-60,"elapsed":239,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"0ac174eb-a60d-4720-9ee3-a5b7e94b01b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.6956, 1.1806, 0.6492, 0.5130, 0.9395],\n","        [0.6808, 1.2497, 0.3677, 0.3980, 0.7018]])"]},"metadata":{},"execution_count":140}],"source":["x @ y"]},{"cell_type":"markdown","id":"1568d1da","metadata":{"id":"1568d1da"},"source":["### Sum and argmax"]},{"cell_type":"markdown","id":"1ec492a9","metadata":{"id":"1ec492a9"},"source":["Let us define a tensor of random numbers:"]},{"cell_type":"code","execution_count":141,"id":"15f4cb53","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15f4cb53","executionInfo":{"status":"ok","timestamp":1674048819513,"user_tz":-60,"elapsed":262,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"83a19226-4854-4683-c8a7-1634cd956c44"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.2217, 0.5565, 0.5715, 0.5244, 0.3022],\n","         [0.8669, 0.2880, 0.9965, 0.2966, 0.2552],\n","         [0.2670, 0.5872, 0.6361, 0.0570, 0.9830]],\n","\n","        [[0.3571, 0.2263, 0.2158, 0.4190, 0.7640],\n","         [0.5551, 0.2931, 0.7114, 0.0790, 0.1458],\n","         [0.6858, 0.6525, 0.3074, 0.3546, 0.7412]]])"]},"metadata":{},"execution_count":141}],"source":["x = torch.rand(2, 3, 5)\n","x"]},{"cell_type":"markdown","id":"e351fbeb","metadata":{"id":"e351fbeb"},"source":["You have already seen that we can compute the sum of a tensor:"]},{"cell_type":"code","execution_count":142,"id":"dcb154dc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcb154dc","executionInfo":{"status":"ok","timestamp":1674048821815,"user_tz":-60,"elapsed":229,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"8a88fde5-3a31-4b62-f06a-396cf6a14b5a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(13.9178)"]},"metadata":{},"execution_count":142}],"source":["torch.sum(x)"]},{"cell_type":"markdown","id":"08717601","metadata":{"id":"08717601"},"source":["There is a second form of the sum operation where we can specify the dimension along which the sum should be computed. This will return a tensor with the specified dimension removed."]},{"cell_type":"code","execution_count":143,"id":"4d56da64","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4d56da64","executionInfo":{"status":"ok","timestamp":1674048854042,"user_tz":-60,"elapsed":258,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"0a6bbed9-0e98-494a-b027-858881b3af58"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.5789, 0.7828, 0.7874, 0.9434, 1.0662],\n","        [1.4220, 0.5811, 1.7079, 0.3756, 0.4010],\n","        [0.9528, 1.2398, 0.9435, 0.4115, 1.7242]])"]},"metadata":{},"execution_count":143}],"source":["torch.sum(x, dim=0)    # shape: [3, 5]\n"]},{"cell_type":"code","execution_count":144,"id":"f1229dae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1229dae","executionInfo":{"status":"ok","timestamp":1674048856556,"user_tz":-60,"elapsed":264,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"ae5c79fd-0b0e-4cd6-b927-3e15d2829d6c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.3556, 1.4318, 2.2041, 0.8779, 1.5404],\n","        [1.5980, 1.1719, 1.2346, 0.8525, 1.6510]])"]},"metadata":{},"execution_count":144}],"source":["torch.sum(x, dim=1)   # shape: [2, 5]"]},{"cell_type":"markdown","id":"9508eed4","metadata":{"id":"9508eed4"},"source":["The same idea also applies to the operation [`argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html), which returns the index of the component with the maximal value along the specified dimension."]},{"cell_type":"code","execution_count":44,"id":"d587eb9a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d587eb9a","executionInfo":{"status":"ok","timestamp":1674044118248,"user_tz":-60,"elapsed":30,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"a5b316ce-f70f-4a99-e6e7-372e8624f92c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(29)"]},"metadata":{},"execution_count":44}],"source":["torch.argmax(x)    # index of the highest component, numbered in consecutive order"]},{"cell_type":"code","execution_count":45,"id":"991d2fab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"991d2fab","executionInfo":{"status":"ok","timestamp":1674044118249,"user_tz":-60,"elapsed":26,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"e708c3b4-61aa-4d6d-a271-5623b0d2ae43"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 1, 1, 0],\n","        [0, 1, 0, 1, 0],\n","        [0, 1, 0, 0, 1]])"]},"metadata":{},"execution_count":45}],"source":["torch.argmax(x, dim=0)   # index of the highest component across the first dimension"]},{"cell_type":"markdown","id":"e21a39aa","metadata":{"id":"e21a39aa"},"source":["### Concatenating tensors"]},{"cell_type":"markdown","id":"2ffa1842","metadata":{"id":"2ffa1842"},"source":["A list or tuple of tensors can be combined into one long tensor by concatenation."]},{"cell_type":"code","execution_count":46,"id":"c4757f8d","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"c4757f8d","executionInfo":{"status":"ok","timestamp":1674044118250,"user_tz":-60,"elapsed":23,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"440a0c5c-1589-444b-c76a-1ffd72c2cc6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1117, 0.6293, 0.6738],\n","        [0.1591, 0.7983, 0.5173],\n","        [0.4264, 0.9148, 0.5610],\n","        [0.0802, 0.9881, 0.0501],\n","        [0.6010, 0.5276, 0.0206]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 3])"]},"metadata":{},"execution_count":46}],"source":["x = torch.rand(2, 3)\n","y = torch.rand(3, 3)\n","z = torch.cat((x, y))\n","print(z)\n","z.shape"]},{"cell_type":"markdown","id":"edbde559","metadata":{"id":"edbde559"},"source":["You can also concatenate along a specific dimension:"]},{"cell_type":"code","execution_count":47,"id":"1222e652","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1222e652","executionInfo":{"status":"ok","timestamp":1674044118250,"user_tz":-60,"elapsed":19,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"d810a2e3-26a4-4c9b-9841-3852c6da729c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4241, 0.2421],\n","        [0.2642, 0.3017]])\n","tensor([[0.4629, 0.4811],\n","        [0.9727, 0.6590]])\n","tensor([[0.4241, 0.2421],\n","        [0.2642, 0.3017],\n","        [0.4629, 0.4811],\n","        [0.9727, 0.6590]])\n","tensor([[0.4241, 0.2421, 0.4629, 0.4811],\n","        [0.2642, 0.3017, 0.9727, 0.6590]])\n"]}],"source":["x = torch.rand(2, 2)\n","y = torch.rand(2, 2)\n","print(x)\n","print(y)\n","print(torch.cat((x, y), dim=0))    # shape: [4, 2]\n","print(torch.cat((x, y), dim=1))    # shape: [2, 4]"]},{"cell_type":"markdown","id":"6f2b90bd","metadata":{"id":"6f2b90bd"},"source":["### Broadcasting"]},{"cell_type":"markdown","id":"9a2aebcf","metadata":{"id":"9a2aebcf"},"source":["The term *broadcasting* describes how PyTorch treats tensors with different shapes. Subject to certain constraints, the ‘smaller’ tensor is ‘broadcast’ across the larger tensor so that they have compatible shapes. Broadcasting is a way to avoid looping. In short, if a PyTorch operation supports broadcasting, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data)."]},{"cell_type":"markdown","id":"9ee39878","metadata":{"id":"9ee39878"},"source":["In the simplest case, two tensors have the same shapes. This is the case for the matrix `x @ W` and the bias vector `b` in the linear model below:"]},{"cell_type":"code","execution_count":48,"id":"c411711e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c411711e","executionInfo":{"status":"ok","timestamp":1674044118250,"user_tz":-60,"elapsed":16,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"d9c844bc-8a49-4ea8-e82e-acf3ae142626"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.5328, 1.2675, 1.4049]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3])"]},"metadata":{},"execution_count":48}],"source":["x = torch.rand(1, 2)\n","W = torch.rand(2, 3)\n","b = torch.rand(1, 3)\n","z = x @ W    # shape: [1, 3]\n","z = z + b    # shape: [1, 3]\n","print(z)\n","z.shape"]},{"cell_type":"markdown","id":"84b29c1e","metadata":{"id":"84b29c1e"},"source":["Now suppose that we have a whole batch of inputs. Watch what happens when adding the bias vector `b`:"]},{"cell_type":"code","execution_count":49,"id":"7b37cac8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b37cac8","executionInfo":{"status":"ok","timestamp":1674044118251,"user_tz":-60,"elapsed":14,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"8908b31a-d211-4773-fab4-be7d22c53035"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.3858, 1.4727, 1.5623],\n","        [1.1971, 1.3609, 1.4304],\n","        [1.4887, 1.3376, 1.4597],\n","        [1.5484, 1.4789, 1.5957],\n","        [1.6989, 1.3639, 1.5192]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 3])"]},"metadata":{},"execution_count":49}],"source":["X = torch.rand(5, 2)\n","Z = X @ W    # shape: [5, 3]\n","Z = Z + b    # shape: [5, 3]    Broadcasting happens here!\n","print(Z)\n","Z.shape"]},{"cell_type":"markdown","id":"3f06a524","metadata":{"id":"3f06a524"},"source":["In the example, broadcasting expands the shape of `b` from $[1, 3]$ into $[5, 3]$. The matrix `Z` is formed by effectively adding `b` *to each row* of `X`. However, this is not implemented by a Python loop but happens implicitly through broadcasting.\n","\n","PyTorch uses the same broadcasting semantics as NumPy. [More information about broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)"]},{"cell_type":"markdown","id":"d11403a9","metadata":{"id":"d11403a9"},"source":["To be expanded!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"vscode":{"interpreter":{"hash":"9df5897e90004e26c7be1a7385fbfe3cbd8a2251f561cac0fb96b53a3dca9256"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}