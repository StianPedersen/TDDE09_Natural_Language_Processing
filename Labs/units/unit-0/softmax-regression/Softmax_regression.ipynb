{"cells":[{"cell_type":"markdown","metadata":{"id":"qyH74qn3YD2J"},"source":["# Implementing softmax regression"]},{"cell_type":"markdown","metadata":{"id":"2ZEewrx2YD2K"},"source":["In this notebook you will learn how to implement softmax regression in PyTorch, and apply it to the task of sentiment analysis."]},{"cell_type":"markdown","metadata":{"id":"sfj3-wVWYD2K"},"source":["## Loading the data"]},{"cell_type":"markdown","metadata":{"id":"_Qz954qtYD2L"},"source":["Our dataset is derived from the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/), which consists of 11,855 sentences extracted from movie reviews. Each sentence has been manually labelled with a sentiment expressed as an integer between 0 (very negative) and 4 (very positive) towards the movie at hand. For this data, sentiment analysis can be framed as a multi-class classification problem. We have pre-processed the original data by tokenization, removing non-alphabetical tokens, and lowercasing.\n","\n","The following helper function loads the sentiment-labelled sentences from a tab-separated file. It returns a list of pairs where the first component of each pair is a tokenized sentence (represented a lists of string tokens) and the second component is the corresponding sentiment (an integer in the range 0–4). We cap the maximal length of a sentence at 20 words."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"uEYGjon0YD2P","executionInfo":{"status":"ok","timestamp":1673975162647,"user_tz":-60,"elapsed":4,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}}},"outputs":[],"source":["def load_data(filename, max_length=20):\n","    items = []\n","    with open(filename, 'rt', encoding='utf-8') as fp:\n","        for line in fp:\n","            sentence, label = line.rstrip().split('\\t')\n","            items.append((sentence.split()[:max_length], int(label)))\n","    return items"]},{"cell_type":"markdown","metadata":{"id":"xYUwtLnpsbbv"},"source":["We use this function to load the training data and the development data:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"nfNuJUKksbbv","executionInfo":{"status":"error","timestamp":1673975232178,"user_tz":-60,"elapsed":218,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"fb928074-f548-474a-d382-17d72f30531d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c23b3618316a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sst-5-train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sst-5-dev.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-f513b351041e>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sst-5-train.txt'"]}],"source":["train_data = load_data('sst-5-train.txt')\n","dev_data = load_data('sst-5-dev.txt')"]},{"cell_type":"markdown","metadata":{"id":"jzvqnD4dYD2P"},"source":["The next cell prints the number of examples in the training data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR-Y-hvvYD2P"},"outputs":[],"source":["print(len(train_data))"]},{"cell_type":"markdown","metadata":{"id":"E_OW9ZfeYD2P"},"source":["## Vectorizing the data"]},{"cell_type":"markdown","metadata":{"id":"Rv7jXYaTYD2P"},"source":["To process the sentences using neural networks, we need to convert them into vectors of numerical values. A simple but common vector representation in natural language processing is the **bag-of-words**. Under this representation, we record *which* words occur in a text and *how often* they occur, but completely ignore their ordering. This is the representation that we will use in this notebook.\n","\n","We first construct a mapping from words to vector indices. The domain of this mapping will be our vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9Zw2xh-YD2Q"},"outputs":[],"source":["def make_vocab(data):\n","    vocab = {}\n","    for sentence, label in data:\n","        for t in sentence:\n","            if t not in vocab:\n","                vocab[t] = len(vocab)\n","    return vocab"]},{"cell_type":"markdown","metadata":{"id":"rlOl92pKsbbw"},"source":["We create the vocabulary from the training data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFG7wzqpsbbw"},"outputs":[],"source":["vocab = make_vocab(train_data)"]},{"cell_type":"markdown","metadata":{"id":"P5jO6laPYD2Q"},"source":["Running the cell below prints the size of the vocabulary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rn5jIXQHYD2Q"},"outputs":[],"source":["print(len(vocab))"]},{"cell_type":"markdown","metadata":{"id":"2vlUhBqzYD2Q"},"source":["Next, we map each sentence to a vector that holds the counts in that sentence for all the words in the vocabulary. We collect all sentence vectors into a PyTorch tensor. We do the same thing for all labels, which results in a vector of integers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3bn_Q77YD2Q"},"outputs":[],"source":["import torch\n","\n","def vectorize(vocab, data):\n","    xs = []\n","    ys = []\n","    for sentence, label in data:\n","        x = [0] * len(vocab)\n","        for w in sentence:\n","            if w in vocab:\n","                x[vocab[w]] += 1\n","        xs.append(x)\n","        ys.append(label)\n","    return torch.FloatTensor(xs), torch.LongTensor(ys)"]},{"cell_type":"markdown","metadata":{"id":"OL51CWX9YD2R"},"source":["We vectorize the training data and the development data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwk6lc0KYD2R"},"outputs":[],"source":["train_x, train_y = vectorize(vocab, train_data)\n","dev_x, dev_y = vectorize(vocab, dev_data)"]},{"cell_type":"markdown","metadata":{"id":"H1vZmcwsYD2R"},"source":["The next cell prints the shapes of the resulting tensors:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNo2sN-tYD2R"},"outputs":[],"source":["print('Training data:', train_x.shape, train_y.shape)\n","print('Development data:', dev_x.shape, dev_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"cySq3NnfYD2R"},"source":["## Evaluation"]},{"cell_type":"markdown","metadata":{"id":"N9bGN4vmYD2R"},"source":["The standard evaluation measure for our dataset is **accuracy** – the percentage of examples for which the classifier predicts the correct label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRR1iXBnYD2S"},"outputs":[],"source":["def accuracy(y_pred, y):\n","    return torch.mean(torch.eq(y_pred, y).float()).item()"]},{"cell_type":"markdown","metadata":{"id":"2-fntYQuYD2S"},"source":["By always predicting the majority class in the training data (rating&nbsp;3), we can get an accuracy on the development data of slightly above 25%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MswmcwqhYD2S"},"outputs":[],"source":["accuracy(torch.full_like(dev_y, 3), dev_y)"]},{"cell_type":"markdown","metadata":{"id":"tCz4oUPXYD2S"},"source":["## Training the model"]},{"cell_type":"markdown","metadata":{"id":"uX1ArQIMYD2S"},"source":["We are now ready to set up a softmax regression model and train it using the categorical cross-entropy loss function. Recall that a softmax regression model consists of a linear layer followed by the softmax function. In PyTorch, linear layers are implemented by the class [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), and cross-entropy loss is implemented by the function [`cross_entropy()`](https://pytorch.org/docs/stable/nn.functional.html#cross-entropy). The softmax function will be computed inside the loss function; see the note below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m0PW7JyWYD2T"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"kl-pJtlhYD2T"},"source":["We will train our model using minibatch gradient descent. The following function splits the data into randomly sampled minibatches of the specified size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ElrAbs1YD2T"},"outputs":[],"source":["def minibatches(x, y, batch_size):\n","    random_indices = torch.randperm(x.size(0))\n","    for i in range(0, x.size(0) - batch_size + 1, batch_size):\n","        batch_indices = random_indices[i:i+batch_size]\n","        yield x[batch_indices], y[batch_indices]"]},{"cell_type":"markdown","metadata":{"id":"mkmd0bvwYD2T"},"source":["With this we can now write our training loop:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kyam_7dsYD2T"},"outputs":[],"source":["def train(n_epochs=20, batch_size=24, lr=1e-1):\n","    # Initialize the model\n","    model = nn.Linear(len(vocab), 5)\n","    \n","    # Initialize the optimizer\n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    \n","    # We train for several epochs\n","    for t in range(n_epochs):\n","        \n","        # In each epoch, we loop over all the minibatches\n","        for bx, by in minibatches(train_x, train_y, batch_size):\n","            \n","            # Reset the accumulated gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass\n","            output = model.forward(bx)\n","            \n","            # Compute the loss\n","            loss = F.cross_entropy(output, by)\n","            \n","            # Backward pass; propagates the loss and computes the gradients\n","            loss.backward()\n","            \n","            # Update the parameters of the model\n","            optimizer.step()\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"zhmALcLRYD2T"},"source":["**⚠️ The softmax is implicit!**\n","\n","One thing that you will note when you go through the code of the training loop is that there is no explicit call to the softmax function. The outputs of the network are not normalized probabilities but just scores (logits). The softmax is computed inside `cross_entropy()`."]},{"cell_type":"markdown","metadata":{"id":"g508BX-SYD2U"},"source":["Here is an embellished version of the training loop that plots the per-epoch losses and the per-epoch accuracies on the development data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-5DmSr9YD2U"},"outputs":[],"source":["# Same training loop with evaluation and plotting\n","\n","import matplotlib.pyplot as plt\n","import tqdm\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","def train(n_epochs=20, batch_size=24, lr=1e-1):\n","    model = nn.Linear(len(vocab), 5)\n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    losses = []\n","    dev_losses = []\n","    dev_accuracies = []\n","    info = {'dev loss': 0, 'dev acc': 0}\n","    with tqdm.tqdm(total=n_epochs) as pbar:\n","        for t in range(n_epochs):\n","            model.train()\n","            running_loss = 0\n","            for bx, by in minibatches(train_x, train_y, batch_size):\n","                optimizer.zero_grad()\n","                output = model.forward(bx)\n","                loss = F.cross_entropy(output, by)\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item() * len(bx)\n","            losses.append(running_loss / len(train_x))\n","            model.eval()\n","            with torch.no_grad():\n","                dev_output = model.forward(dev_x)\n","                dev_loss = F.cross_entropy(dev_output, dev_y)\n","                dev_losses.append(dev_loss)\n","                dev_y_pred = torch.argmax(dev_output, axis=1)\n","                dev_acc = accuracy(dev_y_pred, dev_y)\n","                dev_accuracies.append(dev_acc)\n","                info['dev loss'] = f'{dev_loss:.4f}'\n","                info['dev acc'] = f'{dev_acc:.4f}'\n","                pbar.set_postfix(info)\n","            pbar.update()\n","    plt.figure(figsize=(15, 6))\n","    plt.subplot(121)\n","    plt.plot(losses)\n","    plt.plot(dev_losses)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Average loss')\n","    plt.subplot(122)\n","    plt.plot(dev_accuracies)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Development set accuracy')\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"qsaNtnhFYD2U"},"source":["We are ready to train:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsG5whhzsbb0"},"outputs":[],"source":["train()"]},{"cell_type":"markdown","metadata":{"id":"W4bQ1ypKYD2U"},"source":["## Using a GPU"]},{"cell_type":"markdown","metadata":{"id":"tqJdZeDYYD2U"},"source":["The data set that we used in this notebook is very small, and the model is very simple. For larger datasets and/or models, you may want to use a GPU, say on a service such as [Colab](http://colab.research.google.com). Fortunately, making a model ready for GPU training is rather straightforward in PyTorch: You only need to ‘send’ the model and its input to the correct device.\n","\n","The next code cell contains a GPU-enabled version of the (embellished) training loop. Modified lines are marked as `CHANGED`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-KykUKT5YD2V"},"outputs":[],"source":["# GPU-enabled training loop\n","\n","import matplotlib.pyplot as plt\n","import tqdm\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","# CHANGED: Set the device variable\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train(n_epochs=20, batch_size=24, lr=1e-1):\n","    \n","    # CHANGED: Send the model to the available device\n","    model = nn.Linear(len(vocab), 5).to(device)\n","    \n","    optimizer = optim.SGD(model.parameters(), lr=lr)\n","    losses = []\n","    dev_losses = []\n","    dev_accuracies = []\n","    info = {'dev loss': 0, 'dev acc': 0}\n","    with tqdm.tqdm(total=n_epochs) as pbar:\n","        for t in range(n_epochs):\n","            model.train()\n","            running_loss = 0\n","            for bx, by in minibatches(train_x, train_y, batch_size):\n","\n","                # CHANGED: Send the minibatches to the available device\n","                bx = bx.to(device)\n","                by = by.to(device)\n","\n","                optimizer.zero_grad()\n","                output = model.forward(bx)\n","                loss = F.cross_entropy(output, by)\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item() * len(bx)\n","            losses.append(running_loss / len(train_x))\n","            model.eval()\n","            with torch.no_grad():\n","                dev_output = model.forward(dev_x.to(device))\n","                dev_loss = F.cross_entropy(dev_output, dev_y.to(device))\n","                dev_losses.append(dev_loss)\n","                dev_y_pred = torch.argmax(dev_output, axis=1)\n","                dev_acc = accuracy(dev_y, dev_y_pred)\n","                dev_accuracies.append(dev_acc)\n","                info['dev loss'] = f'{dev_loss:.4f}'\n","                info['dev acc'] = f'{dev_acc:.4f}'\n","                pbar.set_postfix(info)\n","            pbar.update()\n","    plt.figure(figsize=(15, 6))\n","    plt.subplot(121)\n","    plt.plot(losses)\n","    plt.plot(dev_losses)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Average loss')\n","    plt.subplot(122)\n","    plt.plot(dev_accuracies)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Development set accuracy')\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"YzRfOCgrYD2V"},"source":["We can now train on the GPU. (For this very simple model however, the training times are almost identical.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIJHknRSYD2V"},"outputs":[],"source":["train()"]},{"cell_type":"markdown","metadata":{"id":"eoatwTcIYD2V"},"source":["That’s all folks!"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"}},"nbformat":4,"nbformat_minor":0}