{"cells":[{"cell_type":"markdown","id":"8e7907e8","metadata":{"id":"8e7907e8"},"source":["# Sampling words by frequency"]},{"cell_type":"markdown","id":"67167697","metadata":{"id":"67167697"},"source":["This notebook illustrates the sampling recipe mentioned in lab L1."]},{"cell_type":"markdown","id":"4167876e","metadata":{"id":"4167876e"},"source":["## Goal\n","\n","We want to sample words from a vocabulary with a probability that is proportional to their counts (absolute frequencies) in some given text. That is, if we have two words $w_1$ and $w_2$, where $w_2$ appears $k$ times as often as $w_1$, then the expected number of times we sample $w_1$ should be $k$ times higher than the expected number of times we sample $w_2$."]},{"cell_type":"markdown","id":"81d0f6a3","metadata":{"id":"81d0f6a3"},"source":["## Sampling recipe\n","\n","Imagine all the words in the vocabulary covering a line marked with numbers between 0 and the sum of all word frequencies, where each word covers an interval corresponding to its frequency. To sample a word, we choose a random point on that line, and return that word whose interval includes this chosen point. In doing so, we will sample words with a probability that is proportional to its frequency."]},{"cell_type":"markdown","id":"1ded3597","metadata":{"id":"1ded3597"},"source":["## Example\n","\n","We illustrate the sampling recipe with a concrete example."]},{"cell_type":"code","execution_count":null,"id":"bf86a87a","metadata":{"id":"bf86a87a"},"outputs":[],"source":["import numpy as np\n","import torch"]},{"cell_type":"markdown","id":"cc9f8460","metadata":{"id":"cc9f8460"},"source":["Here is a list of counts for words in a ten-word vocabulary:"]},{"cell_type":"code","execution_count":null,"id":"864d01f4","metadata":{"id":"864d01f4"},"outputs":[],"source":["counts = np.array([14507, 5014, 4602, 4529, 4000, 3219, 3010, 2958, 2225, 1271])"]},{"cell_type":"markdown","id":"32bf71c5","metadata":{"id":"32bf71c5"},"source":["To implement the sampling recipe, we need the cumulative sums of these counts. We can get them with the function [`torch.cumsum()`](https://pytorch.org/docs/stable/generated/torch.cumsum.html)."]},{"cell_type":"code","execution_count":null,"id":"f73b493e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f73b493e","executionInfo":{"status":"ok","timestamp":1674812384627,"user_tz":-60,"elapsed":393,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"49ee42fd-5763-4c16-e3cf-36564dc16fa9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([14507, 19521, 24123, 28652, 32652, 35871, 38881, 41839, 44064, 45335])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([14507, 19521, 24123])"]},"metadata":{},"execution_count":11}],"source":["cumulative_sums = torch.cumsum(torch.from_numpy(counts), dim=0)\n","print(cumulative_sums)\n","cumulative_sums[0:3]"]},{"cell_type":"markdown","id":"8cf8c78e","metadata":{"id":"8cf8c78e"},"source":["To choose a random point on the counts line, we sample a random number between 0 and 1 and multiply it with the sum of all counts, which is the last entry in the list of cumulative sums. Here we choose $5$ such points."]},{"cell_type":"code","execution_count":14,"id":"8f0b6933","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f0b6933","executionInfo":{"status":"ok","timestamp":1674818389680,"user_tz":-60,"elapsed":278,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"946d1ce3-619e-4237-ff73-fdbe65ff6463"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([16304.9521,  5703.3110, 32852.5938, 43827.4414,  7527.5234])"]},"metadata":{},"execution_count":14}],"source":["random_points = torch.rand(5) * cumulative_sums[-1]\n","random_points"]},{"cell_type":"markdown","id":"d0838e73","metadata":{"id":"d0838e73"},"source":["To return the word whose interval on the counts line includes a chosen point, we use the function [`torch.searchsorted()`](https://pytorch.org/docs/stable/generated/torch.searchsorted.html). This function takes a sorted sequence and tensor of values and finds the indices from the sorted sequence such that, if the corresponding values were inserted before the indices, the order of the corresponding  dimension within the sorted sequence would be preserved."]},{"cell_type":"code","execution_count":13,"id":"d6035c26","metadata":{"id":"d6035c26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674818360017,"user_tz":-60,"elapsed":270,"user":{"displayName":"david angstrom","userId":"02213000046897981613"}},"outputId":"59fddf81-88d5-47ce-c675-6109c7f3f2d5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 2, 6, 0, 0])"]},"metadata":{},"execution_count":13}],"source":["torch.searchsorted(cumulative_sums, random_points)"]},{"cell_type":"markdown","id":"2e7f0b8c","metadata":{"id":"2e7f0b8c"},"source":["Good luck with the lab!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}