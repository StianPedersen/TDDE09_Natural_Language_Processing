{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). The code below defines an iterable-style dataset for parser data in the [CoNLL-U format](https://universaldependencies.org/format.html) that the project uses to distribute its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stian\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = [Dataset.ROOT]\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append((columns[1], columns[3], int(columns[6])))\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = [Dataset.ROOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('data/no_nynorsk-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('data/no_nynorsk-ud-dev.conllu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger evaluation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**accuracy** (*tagger*, *gold_data*)\n",
    "\n",
    "> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tagger, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        \n",
    "        nr_words += len(words)\n",
    "\n",
    "        correct_tags = [tokens[1] for tokens in sentence]\n",
    "        predicted_tags = tagger.predict(words)\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            if predicted_tags[i] == correct_tags[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the vocabularies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make_vocabs** (*gold_data*)\n",
    "\n",
    "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over tagged sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    tags = {PAD: 0}\n",
    "    for sentence in gold_data:\n",
    "        for pair in sentence:\n",
    "            word = pair[0]\n",
    "            tag = pair[1]\n",
    "            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            \n",
    "            if tag not in tags:\n",
    "                tags[tag] = len(tags)\n",
    "                    \n",
    "    return vocab, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29136\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "vocab, tags = make_vocabs(train_data)\n",
    "print(len(vocab))\n",
    "print(len(tags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n",
    "\n",
    "> A fixed-window model is initialized with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n",
    "\n",
    "**forward** (*self*, *features*)\n",
    "\n",
    "> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([\n",
    "                        ['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                        ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 1 * 10,\n",
    "\n",
    "        # Create RELU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags \n",
    "        words = features[:,:-1]\n",
    "        tags = features[:,-1]\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 1 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, tag_embs], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger interface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict** (*self*, *sentence*)\n",
    "\n",
    "> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *i*, *pred_tags*)\n",
    "\n",
    "> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n",
    "\n",
    "**predict** (*self*, *words*)\n",
    "\n",
    "> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowTaggerModel(embedding_specs, hidden_dim, len(vocab_tags)).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        feature = []\n",
    "        if len(words) == 1:\n",
    "            feature = [words[i], 0, 0, 0]\n",
    "\n",
    "        elif i == 0: # first word\n",
    "            # Wi, PAD, PAD, PAD\n",
    "            feature = [words[i], words[i+1], 0, 0]\n",
    "        elif i == len(words)-1: # last word\n",
    "            # Wi, Wi+1, PAD, PAD\n",
    "            feature = [words[i], 0, words[i-1], pred_tags[i-1]]\n",
    "        else:\n",
    "            # Wi, Wi+1, Wi-1, Ti-1\n",
    "            feature = [words[i], words[i+1], words[i-1], pred_tags[i-1]]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if not word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "\n",
    "        # predict tags\n",
    "        pred_tags_idxs = [0] * len(words)\n",
    "        for i in range(0, len(words_idxs)):\n",
    "            feature = self.featurize(words_idxs, i, pred_tags_idxs)\n",
    "            pred_tags = self.model.forward(feature)\n",
    "            # Find tag index with highest probability\n",
    "            pred_tags_idxs[i] = torch.argmax(pred_tags).item()\n",
    "        \n",
    "        # convert tag indexes\n",
    "        pred_tags = []\n",
    "        for tag_idx in pred_tags_idxs:\n",
    "            tag = [k for k, v in self.vocab_tags.items() if v == tag_idx][0]\n",
    "            pred_tags.append(tag)\n",
    "        \n",
    "        return pred_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training examples for the Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training_examples_tagger** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of tagged sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_tagger(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
    "    batch = []\n",
    "    gold_label = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "\n",
    "        for word, tag, _ in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "\n",
    "        for i in range(0, len(all_words_idx)):\n",
    "            batch.append(tagger.featurize(all_words_idx, i, all_tags_idx))\n",
    "            gold_label.append(all_tags_idx[i])\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 4).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(gold_label).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                gold_label = []\n",
    "\n",
    "        # Yield remaining batch\n",
    "        if sentence_idx == len(list(gold_data))-1:\n",
    "            remainder = len(batch)\n",
    "            batch_tensor = torch.Tensor(remainder, 4).long().to(device)\n",
    "            bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "            by = torch.Tensor(gold_label).long().to(device)\n",
    "            yield bx, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_fixed_window_tagger** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over tagged sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_tagger(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "    \n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        nr_iterations += len(words)\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                batch = 0\n",
    "                tagger.model.train()\n",
    "                for bx, by in training_examples_tagger(vocab_words, vocab_tags, train_data, tagger, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = tagger.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(loss.item()), batch=batch+1)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagger = train_fixed_window_tagger(train_data)\n",
    "#print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8951\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "tagger.model = torch.load('nynorsk_tagger_model', map_location=device)\n",
    "print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predicted part-of-speech tags dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tagger to create predicted part-of-speech tags dataset for parser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedDataset():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = []\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append(columns)\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/no_nynorsk-ud-train-projectivized-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "#     for sentence in TaggedDataset('data/no_nynorsk-ud-train-projectivized.conllu'):\n",
    "#         words = [columns[1] for columns in sentence]\n",
    "#         for i, t in enumerate(tagger.predict(words)):\n",
    "#             sentence[i][3] = t\n",
    "#         for columns in sentence:\n",
    "#             print('\\t'.join(c for c in columns), file=target)\n",
    "#         print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/no_nynorsk-ud-dev-retagged.conllu', 'wt', encoding='utf-8') as target:\n",
    "#     for sentence in TaggedDataset('data/no_nynorsk-ud-dev.conllu'):\n",
    "#         words = [columns[1] for columns in sentence]\n",
    "#         for i, t in enumerate(tagger.predict(words)):\n",
    "#             sentence[i][3] = t\n",
    "#         for columns in sentence:\n",
    "#             print('\\t'.join(c for c in columns), file=target)\n",
    "#         print(file=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_retagged = Dataset('data/no_nynorsk-ud-train-projectivized-retagged.conllu')\n",
    "dev_data_retagged = Dataset('data/no_nynorsk-ud-dev-retagged.conllu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser evaluation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**uas** (*parser*, *gold_data*)\n",
    "\n",
    "> Computes the unlabelled attachment score of the specified *parser* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. The unlabelled attachment score is the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). The calculation excludes the pseudo-roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_data):\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        tags = [tokens[1] for tokens in sentence]\n",
    "        # Do not include pseudo-root\n",
    "        nr_words += (len(words) - 1)\n",
    "\n",
    "        correct_head = [tokens[2] for tokens in sentence]\n",
    "        predicted_head = parser.predict(words, tags)\n",
    "\n",
    "        # skip pseudo-root\n",
    "        for i in range(1, len(words)):\n",
    "            if predicted_head[i] == correct_head[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser interface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Returns the list of predicted heads (a list of integers) for a single sentence, specified in terms of its *words* (a list of strings) and their corresponding *tags* (also a list of strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The arc-hybrid algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcHybridParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return (0, [], [0] * num_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        valid_moves = []\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            valid_moves.append(ArcHybridParser.SH)\n",
    "        if len(stack) > 0 and buffer < len(heads):\n",
    "            valid_moves.append(ArcHybridParser.LA)\n",
    "        if len(stack) > 1:\n",
    "            valid_moves.append(ArcHybridParser.RA)\n",
    "        return valid_moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        buffer, stack, heads = config\n",
    "        # SHIFT\n",
    "        if move == ArcHybridParser.SH:\n",
    "            stack.append(buffer)\n",
    "            buffer += 1\n",
    "        # LEFT ARC\n",
    "        elif move == ArcHybridParser.LA:\n",
    "            heads[stack[-1]] = buffer\n",
    "            stack = stack[:-1]\n",
    "        # RIGHT ARC\n",
    "        elif move == ArcHybridParser.RA:\n",
    "            heads[stack[-1]] = stack[-2]\n",
    "            stack = stack[:-1]\n",
    "            \n",
    "        return (buffer, stack, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        buffer, stack, heads = config\n",
    "        return buffer == len(heads) and len(stack) == 1 and stack[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<root>', '<root>', 0), ('Etter', 'ADP', 4), ('ei', 'DET', 4), ('lang', 'ADJ', 4), ('ferd', 'NOUN', 19), ('over', 'ADP', 6), ('fjell', 'NOUN', 4), ('og', 'CCONJ', 9), ('gjennom', 'ADP', 9), ('skogar', 'NOUN', 6), (',', 'PUNCT', 13), ('utan', 'ADP', 13), ('å', 'PART', 13), ('vite', 'VERB', 4), ('heilt', 'ADJ', 15), ('kor', 'ADV', 17), ('vi', 'PRON', 17), ('skulle', 'VERB', 13), ('-', 'PUNCT', 13), ('kom', 'VERB', 0), ('vi', 'PRON', 19), ('fram', 'ADP', 19), ('til', 'ADP', 25), ('den', 'DET', 25), ('store', 'ADJ', 25), ('garden', 'NOUN', 19), ('i', 'ADP', 27), ('vinterlandet', 'NOUN', 25), ('.', 'PUNCT', 19)]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39massert\u001b[39;00m move \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39mvalid_moves(config)\n\u001b[0;32m     12\u001b[0m     config \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mnext_config(config, move)\n\u001b[1;32m---> 13\u001b[0m \u001b[39massert\u001b[39;00m parser\u001b[39m.\u001b[39mis_final_config(config)\n\u001b[0;32m     14\u001b[0m \u001b[39massert\u001b[39;00m config \u001b[39m==\u001b[39m (\u001b[39m6\u001b[39m, [\u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLooks good!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example_sentence = list(train_data)[531]\n",
    "\n",
    "# print(example_sentence)\n",
    "\n",
    "# #moves = [0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]    # 0 = SH, 1 = LA, 2 = RA\n",
    "# moves = [0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 2]\n",
    "\n",
    "# parser = ArcHybridParser()\n",
    "# config = parser.initial_config(len(example_sentence))\n",
    "# for move in moves:\n",
    "#     assert move in parser.valid_moves(config)\n",
    "#     config = parser.next_config(config, move)\n",
    "# assert parser.is_final_config(config)\n",
    "# assert config == (6, [0], [0, 2, 0, 4, 2, 2])\n",
    "\n",
    "# print('Looks good!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_moves(gold_heads):\n",
    "    parser = ArcHybridParser()\n",
    "    config = parser.initial_config(len(gold_heads))\n",
    "    buffer, stack, heads = config\n",
    "    SH, LA, RA = parser.MOVES\n",
    "    dependants = {}\n",
    "\n",
    "    # For each word, count how many other words are dependant on it\n",
    "    for head in gold_heads:\n",
    "        if head not in dependants:\n",
    "            dependants[head] = 1    \n",
    "        else:\n",
    "            dependants[head] += 1\n",
    "    \n",
    "    # If we haven't reached our final configuration, keep looking\n",
    "    while not parser.is_final_config(config):\n",
    "        if len(stack) >= 1:\n",
    "            top = stack[-1]\n",
    "            buf = buffer\n",
    "            # LEFT ARC\n",
    "            # Does the buffer match the gold_head[top] and does the top of the stack not have any dependants left?\n",
    "            # Since top will be pushed off the stack, we need to have processed all of it's dependants\n",
    "            if buf == gold_heads[top] and dependants.get(top, 0) == 0:\n",
    "                yield config, LA\n",
    "                config = parser.next_config(config, LA)\n",
    "                buffer, stack, heads = config\n",
    "                dependants[buf] -= 1 # 1 dependant processed\n",
    "            \n",
    "            elif len(stack) >= 2:\n",
    "                second_top = stack[-2]\n",
    "                # RIGHT ARC\n",
    "                # Does the second_top of the stack match the gold_head[top] and does the top not have any dependants left?\n",
    "                # Since top will be pushed off the stack, we need to have processed all of it's dependants\n",
    "                if second_top == gold_heads[top] and dependants.get(top, 0) == 0:\n",
    "                    yield config, RA\n",
    "                    config = parser.next_config(config, RA)\n",
    "                    buffer, stack, heads = config\n",
    "                    dependants[second_top] -= 1 # 1 dependant processed\n",
    "                # SHIFT\n",
    "                # If neither LA or RA is the right move we have to keep shifting\n",
    "                elif buffer < len(heads):\n",
    "                    yield config, SH\n",
    "                    config = parser.next_config(config, SH)\n",
    "                    buffer, stack, heads = config\n",
    "            # SHIFT\n",
    "            # If neither LA or RA is the right move we have to keep shifting\n",
    "            else:\n",
    "                yield config, SH\n",
    "                config = parser.next_config(config, SH)\n",
    "                buffer, stack, heads = config\n",
    "\n",
    "        # If neither LA or RA is the right move we have to keep shifting\n",
    "        else:\n",
    "            yield config, SH\n",
    "            config = parser.next_config(config, SH)\n",
    "            buffer, stack, heads = config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed-window parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                                         ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 12 * 50 + 12 * 10,\n",
    "\n",
    "        # Create ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags\n",
    "        words, tags = torch.split(features, 12, dim=1)\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 12 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 12 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        concat_tags = tag_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, concat_tags], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcHybridParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        num_moves = len(ArcHybridParser.MOVES)\n",
    "        embedding_specs = [(12, len(vocab_words), word_dim), (12, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowParserModel(embedding_specs, hidden_dim, num_moves).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, tags, gold_heads, config):\n",
    "        buffer, stack, heads = config\n",
    "    \n",
    "        s0_w = self.vocab_words[PAD]\n",
    "        s0_t = self.vocab_tags[PAD]\n",
    "        s1_w = self.vocab_words[PAD]\n",
    "        s1_t = self.vocab_tags[PAD]\n",
    "        s2_w = self.vocab_words[PAD]\n",
    "        s2_t = self.vocab_tags[PAD]\n",
    "\n",
    "        b0_w = self.vocab_words[PAD]\n",
    "        b0_t = self.vocab_tags[PAD]\n",
    "        b1_w = self.vocab_words[PAD]\n",
    "        b1_t = self.vocab_tags[PAD]\n",
    "        b2_w = self.vocab_words[PAD]\n",
    "        b2_t = self.vocab_tags[PAD]\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            b0_w = words[buffer]\n",
    "            b0_t = tags[buffer]\n",
    "            if buffer + 1 < len(heads):\n",
    "                b1_w = words[buffer + 1]\n",
    "                b1_t = tags[buffer + 1]\n",
    "                if buffer + 2 < len(heads):\n",
    "                    b2_w = words[buffer + 2]\n",
    "                    b2_t = tags[buffer + 2]\n",
    "        \n",
    "        if len(stack) >= 1:\n",
    "            s0_w = words[stack[-1]]\n",
    "            s0_t = tags[stack[-1]]\n",
    "            if len(stack) >= 2:\n",
    "                s1_w = words[stack[-2]]\n",
    "                s1_t = tags[stack[-2]]\n",
    "                if len(stack) >= 3:\n",
    "                    s2_w = words[stack[-3]]\n",
    "                    s2_t = tags[stack[-3]]\n",
    "        \n",
    "        s0_b1_w = self.vocab_words[PAD]\n",
    "        s0_b2_w = self.vocab_words[PAD]\n",
    "        s0_b1_t = self.vocab_tags[PAD]\n",
    "        s0_b2_t = self.vocab_tags[PAD]\n",
    "        for idx, head in enumerate(gold_heads[0:s0_w]):\n",
    "            if head == s0_w and s0_b1_w == self.vocab_tags[PAD]:\n",
    "                s0_b1_w = words[idx]\n",
    "                s0_b1_t = tags[idx]\n",
    "            if head == s0_w and s0_b2_w == self.vocab_tags[PAD]:\n",
    "                s0_b2_w = words[idx]\n",
    "                s0_b2_t = tags[idx]\n",
    "\n",
    "\n",
    "        s0_f1_w = self.vocab_words[PAD]\n",
    "        s0_f2_w = self.vocab_words[PAD]\n",
    "        s0_f1_t = self.vocab_tags[PAD]\n",
    "        s0_f2_t = self.vocab_tags[PAD]\n",
    "        if len(stack) >= 1:\n",
    "            for idx, head in enumerate(gold_heads[s0_w:]):\n",
    "                if head == s0_w and s0_f1_w == self.vocab_tags[PAD]:\n",
    "                    s0_f1_w = words[idx]\n",
    "                    s0_f1_t = tags[idx]\n",
    "                if head == s0_w and s0_f2_w == self.vocab_tags[PAD]:\n",
    "                    s0_f2_w = tags[idx]\n",
    "                    s0_f2_t = tags[idx]\n",
    "\n",
    "\n",
    "        n0_b1_w = self.vocab_words[PAD]\n",
    "        n0_b2_w = self.vocab_words[PAD]\n",
    "        n0_b1_t = self.vocab_tags[PAD]\n",
    "        n0_b2_t = self.vocab_tags[PAD]\n",
    "        for idx, head in enumerate(gold_heads[0:b0_w]):\n",
    "            if head == b0_w and n0_b1_w == self.vocab_tags[PAD]:\n",
    "                n0_b1_w = words[idx]\n",
    "                n0_b1_t = tags[idx]\n",
    "            if head == b0_w and n0_b2_w == self.vocab_tags[PAD]:\n",
    "                n0_b2_w = words[idx]\n",
    "                n0_b2_t = tags[idx]\n",
    "\n",
    "\n",
    "        feature = [b0_w, b1_w, b2_w, s0_w, s1_w, s2_w,\n",
    "                   s0_b1_w, s0_b2_w, s0_f1_w, s0_f2_w, n0_b1_w, n0_b2_w,\n",
    "                   b0_t, b1_t, b2_t, s0_t, s1_t, s2_t,\n",
    "                   s0_b1_t, s0_b2_t, s0_f1_t, s0_f2_t, n0_b1_t, n0_b2_t]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "\n",
    "        # find tag indexes for given tags\n",
    "        tags_idxs = []\n",
    "        for tag in tags:\n",
    "            if tag in self.vocab_tags:\n",
    "                tags_idxs.append(self.vocab_tags[tag])\n",
    "            else:\n",
    "                tags_idxs.append(self.vocab_tags[PAD])\n",
    "\n",
    "        config = self.initial_config(len(words))\n",
    "\n",
    "        while not self.is_final_config(config):\n",
    "            valid_moves = self.valid_moves(config)\n",
    "            feature = self.featurize(words_idxs, tags_idxs, list(config[2]), config)\n",
    "            pred_moves = self.model.forward(feature)\n",
    "            _, sorted_indexes = torch.sort(pred_moves, descending=True)\n",
    "            # find valid move with highest score (SH, LA, RA)\n",
    "            if len(valid_moves) > 0:\n",
    "                sorted_move_list = sorted_indexes.tolist()[0]\n",
    "                # choose first valid move as default move\n",
    "                new_move = valid_moves[0]\n",
    "                for move in sorted_move_list:\n",
    "                    if move in valid_moves:\n",
    "                        new_move = move\n",
    "                        break\n",
    "                config = self.next_config(config, new_move)\n",
    "\n",
    "        return config[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training examples for the Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples_parser(vocab_words, vocab_tags, gold_data, parser, batch_size=100):\n",
    "    batch = []\n",
    "    moves = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "        all_heads = []\n",
    "\n",
    "        for word, tag, head in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "            all_heads.append(head)\n",
    "\n",
    "        for c, m in oracle_moves(all_heads):\n",
    "            batch.append(parser.featurize(all_words_idx, all_tags_idx, all_heads, c))\n",
    "            moves.append(m)\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 24).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(moves).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                moves = []\n",
    "\n",
    "    # Yield remaining batch\n",
    "    if sentence_idx == len(list(gold_data))-1:\n",
    "        remainder = len(batch)\n",
    "        batch_tensor = torch.Tensor(remainder, 24).long().to(device)\n",
    "        bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "        by = torch.Tensor(moves).long().to(device)\n",
    "        yield bx, by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop for the Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixed_window_parser(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
    "\n",
    "    nr_iterations = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        nr_iterations += 2 * len(sentence) - 1\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_iterations) as pbar:\n",
    "                batch = 1\n",
    "                train_loss = 0\n",
    "\n",
    "                parser.model.train()\n",
    "                for bx, by in training_examples_parser(vocab_words, vocab_tags, train_data, parser, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = parser.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    train_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(train_loss/batch), batch=batch)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 504800/504834 [00:50<00:00, 10004.49it/s, batch=5048, loss=0.252]\n"
     ]
    }
   ],
   "source": [
    "random.seed(12345)\n",
    "parser = train_fixed_window_parser(train_data, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7603\n"
     ]
    }
   ],
   "source": [
    "print('{:.4f}'.format(uas(parser, dev_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 504800/504834 [00:48<00:00, 10354.27it/s, batch=5048, loss=0.322]\n"
     ]
    }
   ],
   "source": [
    "random.seed(12345)\n",
    "parser_retagged = train_fixed_window_parser(train_data_retagged, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6612\n"
     ]
    }
   ],
   "source": [
    "print('{:.4f}'.format(uas(parser_retagged, dev_data_retagged)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsbb34",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42172f64df1a6ea7ffef16afee88427bf191e3efe91ff673607a65a00e26eb83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
