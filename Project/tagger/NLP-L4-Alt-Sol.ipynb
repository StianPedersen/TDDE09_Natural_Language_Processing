{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4: Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tagging is the task of labelling the words (tokens) of a sentence with parts-of-speech such as noun, adjective, and verb. In this lab you will implement the simple, autoregressive fixed-window tagger that was presented in Lecture&nbsp;4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for the lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org), a corpus containing more than 16,000 sentences (254,000&nbsp;tokens) annotated with, among other things, parts-of-speech. The Universal Dependencies Project distributes its data in the [CoNLL-U format](https://universaldependencies.org/format.html), but for this lab we have converted the data into a simpler format: words and their part-of-speech tags are separated by tabs, sentences are separated by empty lines. The code in the next cell defines a container class for data with this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        tmp = []\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            for line in lines:\n",
    "                line = line.rstrip()\n",
    "                if line:\n",
    "                    tmp.append(tuple(line.split('\\t')))\n",
    "                else:\n",
    "                    yield tmp\n",
    "                    tmp = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('train.txt')\n",
    "dev_data = Dataset('dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets consist of **tagged sentences**. On the Python side of things, a tagged sentence is represented as a list of string pairs, where the first component of each pair represents a word token and the second component represents the wordâ€™s tag. The possible tags are listed and exemplified in the [Annotation Guidelines](http://universaldependencies.org/u/pos/all.html) of the Universal Dependencies Project. Run the next code cell to see an example of a tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'PRON'),\n",
       " ('has', 'AUX'),\n",
       " ('been', 'VERB'),\n",
       " ('talk', 'NOUN'),\n",
       " ('that', 'SCONJ'),\n",
       " ('the', 'DET'),\n",
       " ('night', 'NOUN'),\n",
       " ('curfew', 'NOUN'),\n",
       " ('might', 'AUX'),\n",
       " ('be', 'AUX'),\n",
       " ('implemented', 'VERB'),\n",
       " ('again', 'ADV'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[x[0] for x in list(train_data)[42]]\n",
    "list(train_data)[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tagger that you will implement in this lab follows a simple interface with just one method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single method of this interface has the following specification:\n",
    "\n",
    "**predict** (*self*, *sentence*)\n",
    "\n",
    "> Returns the list of predicted tags (a list of strings) for a single *sentence* (a list of string tokens).\n",
    "\n",
    "One trivial implementation of this interface is a tagger that always predicts the same tag for every word, independently of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantTagger(Tagger):\n",
    "    \n",
    "    def __init__(self, the_tag):\n",
    "        self.the_tag = the_tag\n",
    "    \n",
    "    def predict(self, words):\n",
    "        return [self.the_tag] * len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Implement an evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to implement a function that computes the accuracy of a tagger on gold-standard data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tagger, gold_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        \n",
    "        nr_words += len(words)\n",
    "\n",
    "        correct_tags = [tokens[1] for tokens in sentence]\n",
    "        predicted_tags = tagger.predict(words)\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            if predicted_tags[i] == correct_tags[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "**accuracy** (*tagger*, *gold_data*)\n",
    "\n",
    "> Computes the accuracy of the *tagger* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. Recall that the accuracy is defined as the percentage of tokens to which the tagger assigns the correct tag (as per the gold standard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤ž Test your code\n",
    "\n",
    "Test your code by computing the accuracy on the development set of a trivial tagger that tags each word as a noun. The expected value is 16.69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1668919993637665"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = ConstantTagger('NOUN')\n",
    "\n",
    "accuracy(tagger, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Implement a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start working on the tagger as such, we ask you to first implement a simple baseline:\n",
    "\n",
    "> Tag each input word with the most frequent tag for that word in the training data. If an input word does not occur in the training data, tag it with the overall most frequent tag in the training data. Break ties by choosing that tag which comes first in the alphabetical order.\n",
    "\n",
    "To implement the baseline, you need to implement both a class `BaselineTagger` and a function `train_baseline`. A `BaselineTagger` has two fields: a dictionary mapping each word in the training data to the most frequent tag for that word, and a string representing the fallback tag (overall most frequent tag in the training data). Both of these fields are set in the `train_baseline` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineTagger(Tagger):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.most_frequent = {}\n",
    "        self.fallback = None\n",
    "\n",
    "    def predict(self, words):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        prediction = []\n",
    "        for word in words:\n",
    "            if word in self.most_frequent:\n",
    "                prediction.append(self.most_frequent[word])\n",
    "            else:\n",
    "                prediction.append(self.fallback)\n",
    "                \n",
    "        return prediction\n",
    "\n",
    "def train_baseline(train_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    temp_frequent = {}\n",
    "    total_tag_freq = {}\n",
    "\n",
    "    for sentence in train_data:\n",
    "        for word, tag in sentence:\n",
    "            if tag not in total_tag_freq:\n",
    "                total_tag_freq[tag] = 1\n",
    "            else:\n",
    "                total_tag_freq[tag] += 1\n",
    "\n",
    "            if word not in temp_frequent:\n",
    "                tag_freq = {}\n",
    "                tag_freq[tag] = 1\n",
    "                temp_frequent[word] = tag_freq\n",
    "            else:\n",
    "                if tag not in temp_frequent[word]:\n",
    "                    temp_frequent[word][tag] = 1\n",
    "                else:\n",
    "                    temp_frequent[word][tag] += 1\n",
    "    \n",
    "    for word in temp_frequent:\n",
    "        temp_frequent[word] = max(temp_frequent[word], key=temp_frequent[word].get)\n",
    "\n",
    "    tagger = BaselineTagger()\n",
    "    tagger.most_frequent = temp_frequent\n",
    "    tagger.fallback = max(total_tag_freq, key=total_tag_freq.get)\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤ž Test your code\n",
    "\n",
    "Test your implementation by computing the accuracy of the baseline tagger on the development data. The expected value is 85.61%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8564100524892636"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = train_baseline(train_data)\n",
    "\n",
    "accuracy(tagger, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Create the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in previous labs, you will need an explicit representation of your vocabulary. Here we actually have two vocabularies: one for the words and one for the tags. Both should be represented as dictionaries that map words/tags to a contiguous range of integers, starting at zero.\n",
    "\n",
    "The next cell contains skeleton code for a function `make_vocabs` that constructs the two vocabularies from gold-standard data. The code cell also defines a name for the â€˜unknown wordâ€™ (`UNK`) and for an additional pseudoword that you will use as a placeholder for undefined values (`PAD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    tags = {PAD: 0}\n",
    "    for sentence in gold_data:\n",
    "        for pair in sentence:\n",
    "            word = pair[0]\n",
    "            tag = pair[1]\n",
    "            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            \n",
    "            if tag not in tags:\n",
    "                tags[tag] = len(tags)\n",
    "                    \n",
    "    return vocab, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code according to the following specification:\n",
    "\n",
    "**make_vocabs** (*gold_data*)\n",
    "\n",
    "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over tagged sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤ž Test your code\n",
    "\n",
    "Test your implementation by computing the total number of unique words and tags in the training data (including the pseudowords). The expected values are 19,674&nbsp;words and 18&nbsp;tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19674\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "vocab, tags = make_vocabs(train_data)\n",
    "print(len(vocab))\n",
    "print(len(tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Fixed-window tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your main task in this lab is to implement a complete, autoregressive part-of-speech tagger based on the fixed-window architecture. This implementation has four parts: the fixed-window model; a tagger that uses the fixed-window model to make predictions; a function that generates training examples for the tagger; and the training function.\n",
    "\n",
    "**âš ï¸ We expect that solving this problem will take you the longest time in this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: Implement the fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the fixed-window model is presented in Lecture&nbsp;4.2. An input to the network takes the form of a $k$-dimensional vector of word ids and/or tag ids. Each integer $i$ is mapped to an $e_i$-dimensional embedding vector. These vectors are concatenated to form a vector of length $e_1 + \\cdots + e_k$, and sent through a feed-forward network with a single hidden layer and a rectified linear unit (ReLU).\n",
    "\n",
    "#### Default features\n",
    "\n",
    "We ask you to implement a fixed-window model with the following features ($k=4$):\n",
    "\n",
    "0. current word\n",
    "1. previous word\n",
    "2. next word\n",
    "3. tag predicted for the previous word\n",
    "\n",
    "Whenever the value of a feature is undefined, you should use the special value `PAD`.\n",
    "\n",
    "#### Embedding specifications\n",
    "\n",
    "To make your implementation of the fixed-window model useful for a range of different applications (including the parser that you will build in lab&nbsp;5), it should support other feature sets than the default model. To this end, the constructor of your model should accept a list of what we call *embedding specifications*. An embedding specification is a triple $(m, n, e)$ consisting of three integers. Such a triple specifies that the model should include $m$ instances of an embedding from $n$ items to vectors of size $e$. All of the $m$ instances are to share their weights. In this lab, the embeddings will be embeddings for words and tags. For example, to instantiate the default feature model, you would initialise the model with the following specifications:\n",
    "\n",
    "``\n",
    "[(3, num_words, word_dim), (1, num_tags, tag_dim)]\n",
    "``\n",
    "\n",
    "This specifies that the model should use 3 instances of an embedding from *num_words* words to vectors of length *word_dim*, and 1 instance of an embedding from *num_tags* tags to vectors of length *tag_dim*. All 3 instances of the word embedding would share their weights. If you rather wanted to have word embeddings with separate weights, you would initialise the model with the following specifications:\n",
    "\n",
    "``\n",
    "[(1, num_words, word_dim), (1, num_words, word_dim), (1, num_words, word_dim), (1, num_tags, tag_dim)]\n",
    "``\n",
    "\n",
    "We recommend that you initialize the weights of each embedding with values drawn from $\\mathcal{N}(0, 10^{-2})$.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The network architecture introduces a number of hyperparameters. The following choices are reasonable defaults:\n",
    "\n",
    "* width of each word embedding: 50\n",
    "* width of each tag embedding: 10\n",
    "* size of the hidden layer: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains skeleton code for the implementation of the fixed-window model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Add your code here\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([\n",
    "                        ['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                        ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 1 * 10,\n",
    "\n",
    "        # Create RELU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags \n",
    "        words = features[:,:-1]\n",
    "        tags = features[:,-1]\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 1 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, tag_embs], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should meet the following specification:\n",
    "\n",
    "**__init__** (*self*, *embedding_specs*, *hidden_dim*, *output_dim*)\n",
    "\n",
    "> A fixed-window model is initialized with a list of specifications for the embeddings the network should use (*embedding_specs*), the size of the hidden layer (*hidden_dim*), and the size of the output layer (*output_dim*).\n",
    "\n",
    "**forward** (*self*, *features*)\n",
    "\n",
    "> Computes the network output for a given feature representation *features*. This is a tensor of shape $B \\times k$ where $B$ is the batch size (number of samples in the batch) and $k$ is the total number of embeddings specified upon initialisation. For example, for the default feature model, $k=4$, as this model includes 3 (weight-sharing) word embeddings and 1 tag embedding.\n",
    "\n",
    "#### ðŸ’¡ Hint on the implementation\n",
    "\n",
    "You will have to construct embeddings based on the embedding specifications. It is natural to store these embeddings in a list- or dictionary-valued attribute of the `FixedWindowModel` object. However, in order to expose the embeddings to the auto-differentiation magic of PyTorch (so that their weights are updated during training), you must instead store them in an [`nn.ModuleList`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleList) or [`nn.ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "word_dim=50 \n",
    "tag_dim=10 \n",
    "hidden_dim=100\n",
    "\n",
    "embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "\n",
    "fwm_model = FixedWindowModel(embedding_specs, hidden_dim, len(vocab_tags))\n",
    "\n",
    "features = torch.tensor([[200, 7, 17, 5],\n",
    "                      [234, 78, 98, 12]])\n",
    "                      \n",
    "fwm_model.forward(features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2: Implement the tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the tagger itself. The tagger will use the simple algorithm that was presented in Lecture&nbsp;4.2: It processes an input sentence from left to right, and at each position, predicts the tag for the current word based on the features extracted from the current feature window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowTagger(Tagger):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=100):\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (1, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowModel(embedding_specs, hidden_dim, len(vocab_tags)).to(device)\n",
    "        # TODO: Replace the next line with your own code\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, i, pred_tags):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        feature = []\n",
    "        if len(words) == 1:\n",
    "            feature = [words[i], 0, 0, 0]\n",
    "\n",
    "        elif i == 0: # first word\n",
    "            # Wi, PAD, PAD, PAD\n",
    "            feature = [words[i], words[i+1], 0, 0]\n",
    "        elif i == len(words)-1: # last word\n",
    "            # Wi, Wi+1, PAD, PAD\n",
    "            feature = [words[i], 0, words[i-1], pred_tags[i-1]]\n",
    "        else:\n",
    "            # Wi, Wi+1, Wi-1, Ti-1\n",
    "            feature = [words[i], words[i+1], words[i-1], pred_tags[i-1]]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words):\n",
    "        # TODO: Replace the next line with your own code\n",
    "\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if not word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "\n",
    "        # predict tags\n",
    "        pred_tags_idxs = [0] * len(words)\n",
    "        for i in range(0, len(words_idxs)):\n",
    "            feature = self.featurize(words_idxs, i, pred_tags_idxs)\n",
    "            pred_tags = self.model.forward(feature)\n",
    "            # Find tag index with highest probability\n",
    "            pred_tags_idxs[i] = torch.argmax(pred_tags).item()\n",
    "        \n",
    "        # convert tag indexes\n",
    "        pred_tags = []\n",
    "        for tag_idx in pred_tags_idxs:\n",
    "            tag = [k for k, v in self.vocab_tags.items() if v == tag_idx][0]\n",
    "            pred_tags.append(tag)\n",
    "        \n",
    "        return pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the skeleton code by implementing the methods of this interface:\n",
    "\n",
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *i*, *pred_tags*)\n",
    "\n",
    "> Extracts features from the specified tagger configuration according to the default feature model. The configuration is specified in terms of the words in the input sentence (*words*, a list of word ids), the position of the current word (*i*), and the list of already predicted tags (*pred_tags*, a list of tag ids). Returns a tensor that can be fed to the fixed-window model.\n",
    "\n",
    "**predict** (*self*, *words*)\n",
    "\n",
    "> Processes the input sentence *words* (a list of string tokens) and makes calls to the fixed-window model to predict the tag of each word. Returns the list of the predicted tags (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred ['INTJ', 'DET', 'X', 'PROPN', 'PRON', 'ADP', 'SCONJ', 'PRON', 'ADP', 'NOUN', 'PRON', 'X', 'PART']\n",
      "tags ['PRON', 'AUX', 'VERB', 'NOUN', 'SCONJ', 'DET', 'NOUN', 'NOUN', 'AUX', 'AUX', 'VERB', 'ADV', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "fwt = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "\n",
    "sentence = list(train_data)[42]\n",
    "words = []\n",
    "tags = []\n",
    "for word_tag_pair in sentence:\n",
    "    words.append(word_tag_pair[0])\n",
    "    tags.append(word_tag_pair[1])\n",
    "\n",
    "pred = fwt.predict(words)\n",
    "print('pred',pred)\n",
    "print('tags', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3: Generate the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement a function that generates the training examples for the tagger. You will train the tagger as usual, using minibatch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples(vocab_words, vocab_tags, gold_data, tagger, batch_size=100):\n",
    "    batch = []\n",
    "    gold_label = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "\n",
    "        for word, tag in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "\n",
    "        for i in range(0, len(all_words_idx)):\n",
    "            batch.append(tagger.featurize(all_words_idx, i, all_tags_idx))\n",
    "            gold_label.append(all_tags_idx[i])\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 4).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(gold_label).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                gold_label = []\n",
    "\n",
    "        # Yield remaining batch\n",
    "        if sentence_idx == len(list(gold_data))-1:\n",
    "            remainder = len(batch)\n",
    "            batch_tensor = torch.Tensor(remainder, 4).long().to(device)\n",
    "            bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "            by = torch.Tensor(gold_label).long().to(device)\n",
    "            yield bx, by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should comply with the following specification:\n",
    "\n",
    "**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of tagged sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *tagger*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "fwt = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "\n",
    "train = training_examples(vocab_words, vocab_tags, train_data, fwt, batch_size=100)\n",
    "#len(list(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.4: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What remains to be done is the implementation of the training loop. This should be a straightforward generalization of the training loops that you have seen so far. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_init(model, std=0.01):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data.normal_(mean=0.0, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_fixed_window(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    tagger = FixedWindowTagger(vocab_words, vocab_tags)\n",
    "    \n",
    "    # Initialize embedding weights\n",
    "    #var_init(tagger.model)\n",
    "\n",
    "    # Load word embeddings from GloVe\n",
    "    glove = torch.load('glove.pt').to(device)\n",
    "    tagger.model.embeddings['word_embs'] = nn.Embedding.from_pretrained(glove, freeze=False)\n",
    "\n",
    "    optimizer = optim.Adam(tagger.model.parameters(), lr=lr)\n",
    "\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in train_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        nr_words += len(words)\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_words) as pbar:\n",
    "                batch = 0\n",
    "                tagger.model.train()\n",
    "                for bx, by in training_examples(vocab_words, vocab_tags, train_data, fwt, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = tagger.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(loss.item()), batch=batch+1)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_fixed_window** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window tagger from a set of training data *train_data* (an iterable over tagged sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell trains a tagger and evaluates it on the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 204559/204585 [16:34<00:00, 205.74it/s, batch=2046, loss=0.289]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789\n"
     ]
    }
   ],
   "source": [
    "tagger = train_fixed_window(train_data)\n",
    "print('{:.4f}'.format(accuracy(tagger, dev_data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Embedding(19674, 50, padding_idx=0)\n",
      "Pre Embedding(19674, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 204559/204585 [15:29<00:00, 220.14it/s, batch=2046, loss=0.237] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8854\n",
      "Embedding(19674, 50, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "tagger_glove = train_fixed_window(train_data)\n",
    "print('{:.4f}'.format(accuracy(tagger_glove, dev_data)))\n",
    "print(tagger.model.embeddings['word_embs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âš ï¸ Your submitted notebook must contain output demonstrating at least 88% accuracy on the development set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Pre-trained embeddings (reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many neural systems for natural language processing use pre-trained word embeddings, either to augment or to replace randomly initialised task-based embeddings. In this problem, you will investigate whether pre-trained embeddings help your part-of-speech tagger.\n",
    "\n",
    "The file `glove.pt` contains a PyTorch tensor containing 50-dimensional pre-trained word embeddings from the [GloVe project](https://nlp.stanford.edu/projects/glove/). You can load this tensor using the command\n",
    "\n",
    "```\n",
    "glove = torch.load('glove.pt')\n",
    "```\n",
    "\n",
    "and should be able to use it as a drop-in replacement for your randomly initialized word embeddings, assuming that the words in your vocabulary are numbered in the order in which they are found in the training data. Have a look at the documentation of the class [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) to learn how to do this.\n",
    "\n",
    "Run experiments to assess the effect that pre-trained embeddings have on (a)&nbsp;the accuracy of the tagger, and (b)&nbsp;the speed of learning, i.e., the number of training examples it takes to reach a certain loss. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n",
    "\n",
    "* How did you integrate the pre-trained embeddings into your system? What did you measure? What results did you get?\n",
    "* Based on what you know about word embeddings and transfer learning, did you expect your results? How do you explain them?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "**How did you integrate the pre-trained embeddings into your system? What did you measure? What results did you get?**\n",
    "* We integrated the pre-trained embeddings upon initializing our model, where we replaced our word embeddings with the GloVe embeddings. We measured the accuracy and runtime of both taggers and received very similar results. \n",
    "\n",
    "* Our own embeddings:\n",
    "`[runtime: 16:34, 205.74it/s, batch=2046, loss=0.289, acc: 0.8789]`\n",
    "\n",
    "* GloVe embeddings:\n",
    "`[runtime: 15:29, 220.14it/s, batch=2046, loss=0.237, acc: 0.8854]`\n",
    "\n",
    "\n",
    "**Based on what you know about word embeddings and transfer learning, did you expect your results? How do you explain them?**\n",
    "\n",
    "* The results we're somewhat expected, since transfer learning is mainly used to compensate for insuffcient data or computing power. It's not really correleated with improved accuracy, though it might help with the learning speed of the model since it has better intial parameters. This is why our model trained from scratch performs almost identically to the model with the fine-tuned GloVe embeddings.\n",
    "\n",
    "**What did you learn? How, exactly, did you learn it? Why does this learning matter?**\n",
    "* Transfer learning: how to use pre-trained embeddings and fine-tuning them according to our needs\n",
    "* Using vectorisation and avoiding unnecessary looping are important key points during training.\n",
    "* When creating a list with generator function, it will take much longer time since it will yield all dataset. Instead, use generator function directly. In our training_examples() function, we tried to create list of gold_data and access each sentence by index. But this approach resulted in a very slow runtime, since the entire dataset was unnecessarily copied each time it was accessed.\n",
    "* The things we learned in this lab will be very important henceforth, especially since our project will utilize parts from both lab 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ¥³ Congratulations on finishing this lab! ðŸ¥³**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
