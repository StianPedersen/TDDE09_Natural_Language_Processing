{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L5: Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is the task of mapping a sentence to a formal representation of its syntactic structure in the form of a dependency tree, which consists of directed arcs between individual words (tokens). In the lab you will implement a dependency parser based on the arc-standard algorithm and the fixed-window model that you implemented in Lab&nbsp;L4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for this lab is the same as for Lab&nbsp;L4: the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org). The code below defines an iterable-style dataset for parser data in the [CoNLL-U format](https://universaldependencies.org/format.html) that the project uses to distribute its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "\n",
    "    ROOT = ('<root>', '<root>', 0)  # Pseudo-root\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'rt', encoding='utf-8') as lines:\n",
    "            tmp = [Dataset.ROOT]\n",
    "            for line in lines:\n",
    "                if not line.startswith('#'):  # Skip lines with comments\n",
    "                    line = line.rstrip()\n",
    "                    if line:\n",
    "                        columns = line.split('\\t')\n",
    "                        if columns[0].isdigit():  # Skip range tokens\n",
    "                            tmp.append((columns[1], columns[3], int(columns[6])))\n",
    "                    else:\n",
    "                        yield tmp\n",
    "                        tmp = [Dataset.ROOT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data and the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('en_ewt-ud-train-projectivized.conllu')\n",
    "dev_data = Dataset('en_ewt-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets consist of **parsed sentences**. A parsed sentence is represented as a list of triples, where the first component of each triple (a string) represents a word, and the second component (also a string) represents the word’s part-of-speech tag. The third component (an integer) specifies the position of the word’s syntactic head, i.e., its parent in the dependency tree. Run the following code cell to see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<root>', '<root>', 0),\n",
       " ('I', 'PRON', 2),\n",
       " ('like', 'VERB', 0),\n",
       " ('yuor', 'PRON', 4),\n",
       " ('blog', 'NOUN', 2),\n",
       " ('.', 'PUNCT', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = list(train_data)[531]\n",
    "\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the head of the pronoun *I* is the word at position&nbsp;2 – the verb *like*. The dependents of *like* are *I* (position&nbsp;1) and the noun *blog* (position&nbsp;4), as well as the final punctuation mark. Note that each sentence starts with the so-called **pseudo-root** (position&nbsp;0). This pseudo-root is a pseudo-word that is guaranteed to be the root of the dependency tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the tagger in the previous lab, the parser that you will implement in this lab follows a simple interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single method of this interface has the following specification:\n",
    "\n",
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Returns the list of predicted heads (a list of integers) for a single sentence, specified in terms of its *words* (a list of strings) and their corresponding *tags* (also a list of strings).\n",
    "\n",
    "One trivial implementation of this interface is a parser that attaches each (real) word to its preceding word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialParser(Parser):\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        return [0] + list(range(len(words)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Implement an evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to implement a function that computes the **unlabelled attachment score (UAS)** of a parser on gold-standard data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_data):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    nr_correct = 0\n",
    "    nr_words = 0\n",
    "\n",
    "    for sentence in gold_data:\n",
    "        words = [tokens[0] for tokens in sentence]\n",
    "        tags = [tokens[1] for tokens in sentence]\n",
    "        # Do not include pseudo-root\n",
    "        nr_words += (len(words) - 1)\n",
    "\n",
    "        correct_head = [tokens[2] for tokens in sentence]\n",
    "        predicted_head = parser.predict(words, tags)\n",
    "\n",
    "        # skip pseudo-root\n",
    "        for i in range(1, len(words)):\n",
    "            if predicted_head[i] == correct_head[i]:\n",
    "                nr_correct += 1\n",
    "\n",
    "    acc = nr_correct / nr_words\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "**uas** (*parser*, *gold_data*)\n",
    "\n",
    "> Computes the unlabelled attachment score of the specified *parser* on the gold-standard data *gold_data* (an iterable of tagged sentences) and returns it as a float. The unlabelled attachment score is the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). The calculation excludes the pseudo-roots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "Test your code by computing the unlabelled attachment score for the trivial parser that attaches every word to its preceding word. The expected score on the development set is 9.76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09757843254204938"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uas(TrivialParser(), dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Create the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains skeleton code for a function `make_vocabs` that constructs the two vocabularies of the parser: one for the words and one for the tags. You should be quite familiar with this task by now. You will be able to re-use your code from lab&nbsp;L4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "def make_vocabs(gold_data):\n",
    "    vocab = {PAD: 0, UNK: 1}\n",
    "    tags = {PAD: 0}\n",
    "    for sentence in gold_data:\n",
    "        for pair in sentence:\n",
    "            word = pair[0]\n",
    "            tag = pair[1]\n",
    "            \n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "            \n",
    "            if tag not in tags:\n",
    "                tags[tag] = len(tags)\n",
    "                    \n",
    "    return vocab, tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code according to the following specification:\n",
    "\n",
    "**make_vocabs** (*gold_data*)\n",
    "\n",
    "> Returns a pair of dictionaries mapping the unique words and tags in the gold-standard data *gold_data* (an iterable over parsed sentences) to contiguous ranges of integers starting at zero. The word dictionary contains the pseudowords `PAD` (index&nbsp;0) and `UNK` (index&nbsp;1); the tag dictionary contains `PAD` (index&nbsp;0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "Test your implementation by computing the total number of unique words and part-of-speech tags in the training data (including the pseudowords and the part-of-speech tag for the pseudoroot). The expected values are 19,676&nbsp;words and 19&nbsp;tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19676\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "vocab, tags = make_vocabs(train_data)\n",
    "print(len(vocab))\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Implement the arc-standard algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser that you will implement in this lab consists of two parts: a static part that implements the logic of the arc-standard algorithm (presented in Lecture&nbsp;5.2), and a non-static part that contains the learning component – the fixed-window model that you implemented in Lab&nbsp;L4. In this problem you will implement the static part; the learning component is covered in Problem&nbsp;5.\n",
    "\n",
    "Recall that, in the arc-standard algorithm, the next move (also called ‘transition’) of the parser is predicted based on features extracted from the current parser configuration, with references to the words and part-of-speech tags of the input sentence. On the Python side of things, the words and part-of-speech tags are represented as lists of strings, and a configuration is represented as a triple\n",
    "\n",
    "$$\n",
    "(i, \\mathit{stack}, \\mathit{heads})\n",
    "$$\n",
    "\n",
    "where $i$ is an integer specifying the position of the next word in the buffer, $\\mathit{stack}$ is a list of integers specifying the positions of the words currently on the stack (with the topmost element last in the list), and $\\mathit{heads}$ is a list of integers specifying the positions of the head words. If a word has not yet been assigned a head, its head value is&nbsp;0. To illustrate this representation, the initial configuration for the example sentence above is"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(3, [0, 2], [0, 2, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a possible final configuration is"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(6, [0], [0, 2, 0, 4, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In Lecture&nbsp;5.2, both the buffer and the stack were presented as list of words. Here we only represent the *stack* as a list of words. To represent the *buffer*, we simply record the position of the next word that has not been processed yet (the integer $i$). This acknowledges the fact that the buffer (in contrast to the stack) can never grow, but will be processed from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a complete skeleton for the logic of the arc-standard algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandardParser(Parser):\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES  # Parser moves are specified as integers.\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        return (0, [], [0] * num_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        valid_moves = []\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            valid_moves.append(ArcStandardParser.SH)\n",
    "        if len(stack) > 2:\n",
    "            valid_moves.append(ArcStandardParser.LA)\n",
    "        if len(stack) > 1:\n",
    "            valid_moves.append(ArcStandardParser.RA)\n",
    "        return valid_moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        buffer, stack, heads = config\n",
    "        # SHIFT\n",
    "        if move == ArcStandardParser.SH:\n",
    "            stack.append(buffer)\n",
    "            buffer += 1\n",
    "        # LEFT ARC\n",
    "        elif move == ArcStandardParser.LA:\n",
    "            heads[stack[-2]] = stack[-1]\n",
    "            top = stack[-1]\n",
    "            stack = stack[:-2] \n",
    "            stack.append(top)\n",
    "        # RIGHT ARC\n",
    "        elif move == ArcStandardParser.RA:\n",
    "            heads[stack[-1]] = stack[-2]\n",
    "            stack = stack[:-1]\n",
    "            \n",
    "        return (buffer, stack, heads)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        buffer, stack, heads = config\n",
    "        return buffer == len(heads) and len(stack) == 1 and stack[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "**initial_config** (*num_words*)\n",
    "\n",
    "> Returns the initial configuration for a sentence with the specified number of words (*num_words*).\n",
    "\n",
    "**valid_moves** (*config*)\n",
    "\n",
    "> Returns the list of valid moves for the specified configuration (*config*).\n",
    "\n",
    "**next_config** (*config*, *move*)\n",
    "\n",
    "> Applies the *move* in the specified configuration *config* and returns the new configuration. This must not modify the input configuration.\n",
    "\n",
    "**is_final_config** (*config*)\n",
    "\n",
    "> Tests whether *config* is a final configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test your implementation, you can run the code below. The code in this cell creates the initial configuration for the example sentence, simulates a sequence of moves, and then tests that the resulting configuration is the expected final configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "moves = [0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]    # 0 = SH, 1 = LA, 2 = RA\n",
    "\n",
    "parser = ArcStandardParser()\n",
    "config = parser.initial_config(len(example_sentence))\n",
    "for move in moves:\n",
    "    assert move in parser.valid_moves(config)\n",
    "    config = parser.next_config(config, move)\n",
    "assert parser.is_final_config(config)\n",
    "assert config == (6, [0], [0, 2, 0, 4, 2, 2])\n",
    "\n",
    "print('Looks good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Implement the oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning component of the parser is the next move classifier. To train this classifier, we need training examples of the form $(\\mathbf{x}, m)$, where $\\mathbf{x}$ is a feature vector extracted from a given parser configuration $c$, and $m$ is the corresponding gold-standard move. To obtain $m$, we need an **oracle**.\n",
    "\n",
    "Recall that, in the context of transition-based dependency parsing, an oracle is a function that translates a gold-standard dependency tree (here represented as a list of head ids) into a sequence of moves such that, when the parser takes the moves starting from the initial configuration, then it recreates the original dependency tree. Here we ask you to implement the static oracle that was presented in Lecture&nbsp;5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_moves(gold_heads):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    parser = ArcStandardParser()\n",
    "    config = parser.initial_config(len(gold_heads))\n",
    "    buffer, stack, heads = config\n",
    "    SH, LA, RA = parser.MOVES\n",
    "    dependants = {}\n",
    "\n",
    "    # For each word, count how many other words are dependant on it\n",
    "    for head in gold_heads:\n",
    "        if head not in dependants:\n",
    "            dependants[head] = 1    \n",
    "        else:\n",
    "            dependants[head] += 1\n",
    "    \n",
    "    # If we haven't reached our final configuration, keep looking\n",
    "    while not parser.is_final_config(config):\n",
    "        if len(stack) >= 2:\n",
    "            top = stack[-1]\n",
    "            second_top = stack[-2]\n",
    "            \n",
    "            # LEFT ARC\n",
    "            # Does the top of the stack match the gold_head[second_top] and does the 2nd top not have any dependants left?\n",
    "            # Since second_top will be pushed off the stack, we need to have processed all of it's dependants\n",
    "            if top == gold_heads[second_top] and dependants.get(second_top, 0) == 0:\n",
    "                yield config, LA\n",
    "                config = parser.next_config(config, LA)\n",
    "                buffer, stack, heads = config\n",
    "                dependants[top] -= 1 # 1 dependant processed\n",
    "\n",
    "            # RIGHT ARC\n",
    "            # Does the second_top of the stack match the gold_head[top] and does the top not have any dependants left?\n",
    "            # Since top will be pushed off the stack, we need to have processed all of it's dependants\n",
    "            elif second_top == gold_heads[top] and dependants.get(top, 0) == 0:\n",
    "                yield config, RA\n",
    "                config = parser.next_config(config, RA)\n",
    "                buffer, stack, heads = config\n",
    "                dependants[second_top] -= 1 # 1 dependant processed\n",
    "            \n",
    "            # SHIFT\n",
    "            # If neither LA or RA is the right move we have to keep shifting\n",
    "            else:\n",
    "                yield config, SH\n",
    "                config = parser.next_config(config, SH)\n",
    "        \n",
    "        # SHIFT\n",
    "        # Shift more words from buffer onto the stack\n",
    "        else:\n",
    "            yield config, SH\n",
    "            config = parser.next_config(config, SH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "**oracle_moves** (*gold_heads*)\n",
    "\n",
    "> Translates a gold-standard head assignment for a single sentence (*gold_heads*) into the corresponding stream of oracle moves. More specifically, this yields pairs $(c, m)$ where $m$ is a move (an integer, as specified in the `ArcStandardParser` interface) and $c$ is the parser configuration in which $m$ was taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "Test your code by running the cell below. This uses your implementation of *oracle_moves* to extract the oracle move sequence from the example sentence and compares it to the gold-standard move sequence *gold_moves*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_heads = [h for w, t, h in example_sentence]\n",
    "gold_moves = [0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 2]\n",
    "\n",
    "assert list(m for _, m in oracle_moves(gold_heads)) == gold_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Fixed-window parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to put everything together. For the full implementation of the fixed-window parser, you will need the correspondents of the four parts of the fixed-window tagger from Lab&nbsp;L4: an implementation of the fixed-window model; a parser that uses the fixed-window model to make predictions; a function that generates the training examples for the parser; and the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5.1: Implement the fixed-window model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixed-window model for the parser is the same as the fixed-window model for the tagger in Lab&nbsp;L4. You can simply copy your code from that lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FixedWindowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_specs, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Add your code here\n",
    "        # Extract embedding_specs\n",
    "        emb_spec_words = embedding_specs[0]\n",
    "        emb_spec_tags = embedding_specs[1]\n",
    "\n",
    "        n_words = emb_spec_words[0]\n",
    "        vocab_size = emb_spec_words[1]\n",
    "        word_dim = emb_spec_words[2]\n",
    "\n",
    "        n_tags = emb_spec_tags[0]\n",
    "        tags_size = emb_spec_tags[1]\n",
    "        tag_dim = emb_spec_tags[2]\n",
    "\n",
    "        # Create embeddings\n",
    "        self.embeddings = nn.ModuleDict([['word_embs', nn.Embedding(vocab_size, word_dim, padding_idx=0)],\n",
    "                                         ['tag_embs', nn.Embedding(tags_size, tag_dim, padding_idx=0)]])\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden = nn.Linear(n_words * word_dim + n_tags * tag_dim, hidden_dim) # 3 * 50 + 3 * 10,\n",
    "\n",
    "        # Create ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Create output layers\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, features):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        batch_size = len(features)\n",
    "        \n",
    "        # Extract words and tags for buffer 1, stack 1, stack 2\n",
    "        words, tags = torch.split(features, 3, dim=1)\n",
    "\n",
    "        # Get the word and tag embeddings\n",
    "        word_embs = self.embeddings['word_embs'](words) # 3 * 50\n",
    "        tag_embs = self.embeddings['tag_embs'](tags) # 3 * 10\n",
    "        \n",
    "        concat_words = word_embs.view(batch_size, -1)\n",
    "        concat_tags = tag_embs.view(batch_size, -1)\n",
    "        \n",
    "        concat_embs = torch.cat([concat_words, concat_tags], dim=1)\n",
    "\n",
    "        hidden = self.hidden(concat_embs)\n",
    "\n",
    "        relu = self.activation(hidden)\n",
    "\n",
    "        output = self.output(relu)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5.2: Implement the parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the parser itself. This parser will use the fixed-window model to predict the next move for a given configuration in the arc-standard algorithm, based on the features extracted from the current feature window.\n",
    "\n",
    "#### Default feature model\n",
    "\n",
    "For the parser, we ask you to implement a fixed-window model with the following features ($k=6$):\n",
    "\n",
    "0. word form of the next word in the buffer\n",
    "1. word form of the topmost word on the stack\n",
    "2. word form of the second-topmost word on the stack\n",
    "3. part-of-speech tag of the next word in the buffer\n",
    "4. part-of-speech tag of the topmost word on the stack\n",
    "5. part-of-speech tag of the second-topmost word on the stack\n",
    "\n",
    "Whenever the value of a feature is undefined, you should use the special value `PAD`.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The following choices are reasonable defaults for the hyperparameters of the network architecture used by the parser:\n",
    "\n",
    "* width of the word embedding: 50\n",
    "* width of the tag embedding: 10\n",
    "* size of the hidden layer: 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowParser(ArcStandardParser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        # TODO: Add your own code\n",
    "        num_moves = len(ArcStandardParser.MOVES)\n",
    "        embedding_specs = [(3, len(vocab_words), word_dim), (3, len(vocab_tags), tag_dim)]\n",
    "        self.model = FixedWindowModel(embedding_specs, hidden_dim, num_moves).to(device)\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "\n",
    "    def featurize(self, words, tags, config):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        buffer, stack, heads = config\n",
    "\n",
    "        # stack might be empty or not have enough words, set words and tags to PAD\n",
    "        word_2 = self.vocab_words[PAD]\n",
    "        tag_2 = self.vocab_tags[PAD]\n",
    "        word_3 = self.vocab_words[PAD]\n",
    "        tag_3 = self.vocab_tags[PAD]\n",
    "\n",
    "        if buffer < len(heads):\n",
    "            word_1 = words[buffer]\n",
    "            tag_1 = tags[buffer]\n",
    "        else:\n",
    "            word_1 = self.vocab_words[PAD]\n",
    "            tag_1 = self.vocab_tags[PAD]\n",
    "        \n",
    "        if len(stack) >= 2 and len(stack) <= len(words):\n",
    "            word_2 = words[stack[-1]]\n",
    "            tag_2 = tags[stack[-1]]\n",
    "            word_3 = words[stack[-2]]\n",
    "            tag_3 = tags[stack[-2]]\n",
    "\n",
    "        elif len(stack) == 1:\n",
    "            word_2 = words[stack[-1]]\n",
    "            tag_2 = tags[stack[-1]]\n",
    "\n",
    "        # next word in buffer, topmost word on stack, 2nd topmost word on stack,\n",
    "        # tag of next word in buffer, tag of topmost word on stack, tag of 2nd topmost word on stack\n",
    "        feature = [word_1, word_2, word_3, tag_1, tag_2, tag_3]\n",
    "        return torch.tensor([feature]).to(device)\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        # find word indexes for given words\n",
    "        words_idxs = []\n",
    "        for word in words:\n",
    "            if word in self.vocab_words:\n",
    "                words_idxs.append(self.vocab_words[word])\n",
    "            else:\n",
    "                words_idxs.append(self.vocab_words[UNK])\n",
    "\n",
    "        # find tag indexes for given tags\n",
    "        tags_idxs = []\n",
    "        for tag in tags:\n",
    "            if tag in self.vocab_tags:\n",
    "                tags_idxs.append(self.vocab_tags[tag])\n",
    "            else:\n",
    "                tags_idxs.append(self.vocab_tags[PAD])\n",
    "\n",
    "        config = self.initial_config(len(words))\n",
    "\n",
    "        while not self.is_final_config(config):\n",
    "            valid_moves = self.valid_moves(config)\n",
    "            feature = self.featurize(words_idxs, tags_idxs, config)\n",
    "            pred_moves = self.model.forward(feature)\n",
    "            _, sorted_indexes = torch.sort(pred_moves, descending=True)\n",
    "            # find valid move with highest score (SH, LA, RA)\n",
    "            if len(valid_moves) > 0:\n",
    "                sorted_move_list = sorted_indexes.tolist()[0]\n",
    "                # choose first valid move as default move\n",
    "                new_move = valid_moves[0]\n",
    "                for move in sorted_move_list:\n",
    "                    if move in valid_moves:\n",
    "                        new_move = move\n",
    "                        break\n",
    "                config = self.next_config(config, new_move)\n",
    "\n",
    "        return config[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the skeleton code by implementing the methods of this interface:\n",
    "\n",
    "**__init__** (*self*, *vocab_words*, *vocab_tags*, *word_dim* = 50, *tag_dim* = 10, *hidden_dim* = 100)\n",
    "\n",
    "> Creates a new fixed-window model of appropriate dimensions and sets up any other data structures that you consider relevant. The parameters *vocab_words* and *vocab_tags* are the word vocabulary and tag vocabulary. The parameters *word_dim* and *tag_dim* specify the embedding width for the word embeddings and tag embeddings.\n",
    "\n",
    "**featurize** (*self*, *words*, *tags*, *config*)\n",
    "\n",
    "> Extracts features from the specified parser state according to the feature model given above. The state is specified in terms of the words in the input sentence (*words*, a list of word ids), their part-of-speech tags (*tags*, a list of tag ids), and the parser configuration proper (*config*, as specified in Problem&nbsp;3).\n",
    "\n",
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Predicts the list of all heads for the input sentence. This simulates the arc-standard algorithm, calling the move classifier whenever it needs to take a decision. The input sentence is specified in terms of the list of its words (strings) and the list of its tags (strings). Both of these should include the pseudoroot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 Hint on the implementation\n",
    "\n",
    "In the *predict* function, you must make sure to only execute valid moves. One simple way to do so is to let the fixed-window model predict scores for all moves, and to implement your own, customised argmax operation to find the *valid* move with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred [0, 0, 1, 4, 1, 1]\n",
      "heads [0, 2, 0, 4, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "fwp = FixedWindowParser(vocab_words, vocab_tags)\n",
    "\n",
    "words = []\n",
    "tags = []\n",
    "heads = []\n",
    "for word, tag, head in example_sentence:\n",
    "    words.append(word)\n",
    "    tags.append(tag)\n",
    "    heads.append(head)\n",
    "\n",
    "pred = fwp.predict(words, tags)\n",
    "print('pred',pred)\n",
    "print('heads', heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5.3: Generate the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement a function that generates the training examples for the parser. You will train as usual, using minibatch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_examples(vocab_words, vocab_tags, gold_data, parser, batch_size=100):\n",
    "    #list(m for _, m in oracle_moves(gold_heads))\n",
    "    batch = []\n",
    "    moves = []\n",
    "    sentence_idx = 0\n",
    "    for sentence in gold_data:\n",
    "        sentence_idx += 1\n",
    "        all_words_idx = []\n",
    "        all_tags_idx = []\n",
    "        all_heads = []\n",
    "\n",
    "        for word, tag, head in sentence:\n",
    "            all_words_idx.append(vocab_words[word])\n",
    "            all_tags_idx.append(vocab_tags[tag])\n",
    "            all_heads.append(head)\n",
    "\n",
    "        for c, m in oracle_moves(all_heads):\n",
    "            batch.append(parser.featurize(all_words_idx, all_tags_idx, c))\n",
    "            moves.append(m)\n",
    "\n",
    "            # Yield batch\n",
    "            if len(batch) == batch_size:\n",
    "                batch_tensor = torch.Tensor(batch_size, 6).long().to(device)\n",
    "                bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "                by = torch.Tensor(moves).long().to(device)\n",
    "                yield bx, by\n",
    "                batch = []\n",
    "                moves = []\n",
    "\n",
    "    # Yield remaining batch\n",
    "    if sentence_idx == len(list(gold_data))-1:\n",
    "        remainder = len(batch)\n",
    "        batch_tensor = torch.Tensor(remainder, 6).long().to(device)\n",
    "        bx = torch.cat(batch, out=batch_tensor).to(device)\n",
    "        by = torch.Tensor(moves).long().to(device)\n",
    "        yield bx, by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should comply with the following specification:\n",
    "\n",
    "**training_examples** (*vocab_words*, *vocab_tags*, *gold_data*, *tagger*, *batch_size* = 100)\n",
    "\n",
    "> Iterates through the given *gold_data* (an iterable of parsed sentences), encodes it into word ids and tag ids using the specified vocabularies *vocab_words* and *vocab_tags*, and then yields batches of training examples for gradient-based training. Each batch contains *batch_size* examples, except for the last batch, which may contain fewer examples. Each example in the batch is created by a call to the `featurize` function of the *parser*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 6])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "fwp = FixedWindowParser(vocab_words, vocab_tags)\n",
    "train = training_examples(vocab_words, vocab_tags, train_data, fwp, batch_size=100)\n",
    "\n",
    "x, y = next(train)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5.4: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece of the puzzle is the training loop. This should be straightforward by now. Complete the skeleton code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_fixed_window(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    vocab_words, vocab_tags =  make_vocabs(train_data)\n",
    "\n",
    "    parser = FixedWindowParser(vocab_words, vocab_tags)\n",
    "\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    nr_examples = 421700\n",
    "\n",
    "    try:    \n",
    "        for epoch in range(n_epochs):\n",
    "            # Begin training\n",
    "            with tqdm(total=nr_examples) as pbar:\n",
    "                batch = 1\n",
    "                train_loss = 0\n",
    "\n",
    "                parser.model.train()\n",
    "                for bx, by in training_examples(vocab_words, vocab_tags, train_data, parser, batch_size):\n",
    "                    curr_batch_size = len(bx)\n",
    "\n",
    "                    score = parser.model.forward(bx)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(score, by)\n",
    "                    train_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    pbar.set_postfix(loss=(train_loss/batch), batch=batch)\n",
    "                    pbar.update(curr_batch_size)\n",
    "                    batch += 1\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the specification of the training function:\n",
    "\n",
    "**train_fixed_window** (*train_data*, *n_epochs* = 1, *batch_size* = 100, *lr* = 1e-2)\n",
    "\n",
    "> Trains a fixed-window parser from a set of training data *train_data* (an iterable over parsed sentences) using minibatch gradient descent and returns it. The parameters *n_epochs* and *batch_size* specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) with learning rate *lr*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell trains a parser and evaluates it on the development data (gold-standard part-of-speech tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421700/421700 [00:18<00:00, 22619.12it/s, batch=4217, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7025\n"
     ]
    }
   ],
   "source": [
    "parser = train_fixed_window(train_data, n_epochs=1)\n",
    "print('{:.4f}'.format(uas(parser, dev_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠️ Your submitted notebook must contain output demonstrating at least 68% UAS on the development set.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell trains a parser and evaluates it on the development data (predicted part-of-speech tags):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421700/421700 [00:18<00:00, 22586.58it/s, batch=4217, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6548\n"
     ]
    }
   ],
   "source": [
    "train_data_retaged = Dataset('en_ewt-ud-train-projectivized-retagged.conllu')\n",
    "dev_data_retaged = Dataset('en_ewt-ud-dev-retagged.conllu')\n",
    "\n",
    "parser_retaged = train_fixed_window(train_data_retaged, n_epochs=1)\n",
    "print('{:.4f}'.format(uas(parser_retaged, dev_data_retaged)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Predicted part-of-speech tags (reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that you have used in this lab so far contains gold-standard part-of-speech tags, which makes the evaluation of your parser somewhat misleading: In a practical system (including the baseline for the standard project), one does not have access to gold-standard tags; instead one has to first tag the sentences with an automatic part-of-speech tagger.\n",
    "\n",
    "The lab directory contains the following alternative versions of the two data for this lab:\n",
    "\n",
    "* `en_ewt-ud-train-projectivized-retagged.conllu`\n",
    "* `en_ewt-ud-dev-retagged.conllu`\n",
    "\n",
    "In each of them, the gold-standard part-of-speech tags have been replaced by part-of-speech tags automatically predicted by the tagger from Lab&nbsp;L4.\n",
    "\n",
    "Run an experiment to assess the effect that using predicted part-of-speech tags instead of gold-standard tags has on the unlabelled attachment score of your parser. Document your exploration in a short reflection piece (ca. 150&nbsp;words). Respond to the following prompts:\n",
    "\n",
    "* How did you set up your experiment? What results did you get?\n",
    "* Based on what you know about machine learning, did you expect your results? How do you explain them?\n",
    "* What did you learn? How, exactly, did you learn it? Why does this learning matter?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "**How did you set up your experiment? What results did you get?**\n",
    "* We ran our first experiment with gold-standard part-of-speech tags dataset, where we applied L2-regularization through `weight_decay=1e-5` in the Adam Optimizer. This setup gave us much better UAS compared to running without. For the second experiment, we trained the parser with predicted part-of-speech tags dataset received a worse UAS value compared to our first experiment.\n",
    "\n",
    "* With gold-standard part-of-speech tags:\n",
    "`[runtime: 00:18, 22619.12it/s, batch=4217, loss=0.278, uas: 0.7025]`\n",
    "\n",
    "* With predicted part-of-speech tags:\n",
    "`[runtime: 00:18, 22586.58it/s, batch=4217, loss=0.303, uas: 0.6548]`\n",
    "\n",
    "**Based on what you know about machine learning, did you expect your results? How do you explain them?**\n",
    "* Yes, we expected that the result of the second experiment would be lower than the first, the reasoning behind this is the nature of training process. When the gold-standard dataset is used during the training process of a model, fabricated dependencies will be much better and accurate compared to using a predicted dataset. In our case, utilizing predicted part-of-speech tags instead of gold-standard tags, carries the already existing inaccuracy, which then reflects on the fabricated dependencies.\n",
    "\n",
    "**What did you learn? How, exactly, did you learn it? Why does this learning matter?**\n",
    "* In this lab, we have learnt how the arc-standard algorithm works to create a dependency tree, and how static oracle tells the gold-standard transition sequence for a tree.\n",
    "* The things we learned in this lab will be very important henceforth, especially since our project will utilize parts from both lab 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🥳 Congratulations on finishing the last lab in this course! 🥳**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "145b04a267d4e32ec1cb77aa0e2c16296a133873bd34953846b88d42e76b0009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
